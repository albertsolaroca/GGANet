{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-01T09:26:10.698229300Z",
     "start_time": "2024-02-01T09:26:10.617375800Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "import wandb\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "from utils.miscellaneous import read_config\n",
    "from utils.miscellaneous import create_folder_structure_MLPvsGNN\n",
    "from utils.miscellaneous import initalize_random_generators\n",
    "from utils.wandb_logger import save_response_graphs_in_ML_tracker\n",
    "from utils.normalization import *\n",
    "from utils.load import *\n",
    "\n",
    "from training.train import training\n",
    "from training.test import testing\n",
    "from training.models import * \n",
    "\n",
    "from utils.visualization import plot_R2, plot_loss\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse configuration file + initializations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-01T09:26:10.784319100Z",
     "start_time": "2024-02-01T09:26:10.713325400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating folder: ./experiments/unrolling_WDN0401\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# read config files\n",
    "cfg = read_config(\"config_unrolling.yaml\")\n",
    "# create folder for result\n",
    "exp_name = cfg['exp_name']\n",
    "data_folder = cfg['data_folder']\n",
    "results_folder = create_folder_structure_MLPvsGNN(cfg, parent_folder='./experiments')\n",
    "\n",
    "all_wdn_names = cfg['network']\n",
    "initalize_random_generators(cfg, count=0)\n",
    "\n",
    "# initialize pytorch device\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "#torch.set_num_threads(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-01T09:26:10.785390500Z",
     "start_time": "2024-02-01T09:26:10.753382400Z"
    }
   },
   "outputs": [],
   "source": [
    "# TO DO: at the moment I am not using the parsed values for batch size and num_epochs ;\n",
    "# I am not using alpha as well because the loss has no \"smoothness\" penalty (yet)\n",
    "batch_size = cfg['trainParams']['batch_size']\n",
    "num_epochs = cfg['trainParams']['num_epochs']\n",
    "res_columns = ['train_loss', 'valid_loss', 'test_loss', 'max_train_loss', 'max_valid_loss', 'max_test_loss',\n",
    "               'min_train_loss', 'min_valid_loss', 'min_test_loss', 'r2_train', 'r2_valid',\n",
    "               'r2_test', 'total_params', 'total_time', 'test_time']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "I will be Creating different models as follows:\n",
    "\n",
    "* A simple LSTM\n",
    "* An unrolled version of Heads and Flows, without static variables\n",
    "* An unrolled version with Heads, Flows and static variables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-01T09:27:30.623978200Z",
     "start_time": "2024-02-01T09:26:10.784319100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Working with FOS_pump_4, network 1 of 1\n",
      "UnrollingModel: training combination 1 of 1\n",
      "\n",
      "Total number of parameters is 22809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 50/500 [00:12<02:05,  3.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate is divided by 2 to: 0.005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 100/500 [00:25<01:43,  3.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate is divided by 2 to: 0.0025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 150/500 [00:40<01:30,  3.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate is divided by 2 to: 0.00125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███▉      | 198/500 [00:53<01:22,  3.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder where model was saved is ./experiments/unrolling_WDN0401\n"
     ]
    }
   ],
   "source": [
    "for ix_wdn, wdn in enumerate(all_wdn_names):\n",
    "    print(f'\\nWorking with {wdn}, network {ix_wdn + 1} of {len(all_wdn_names)}')\n",
    "\n",
    "    # retrieve wntr data\n",
    "    tra_database, val_database, tst_database = load_raw_dataset(wdn, data_folder)\n",
    "    # reduce training data\n",
    "    # tra_database = tra_database[:int(len(tra_database)*cfg['tra_prc'])]\n",
    "    if cfg['tra_num'] < len(tra_database):\n",
    "        tra_database = tra_database[:cfg['tra_num']]\n",
    "\n",
    "\n",
    "    # get GRAPH datasets     \n",
    "    # later on we should change this and use normal scalers from scikit (something is off here)\n",
    "    tra_dataset, A12_bar = create_dataset(tra_database)\n",
    "    # number of nodes\n",
    "    junctions = (tra_database[0].node_type == JUNCTION_TYPE).numpy().sum()\n",
    "    tanks = (tra_database[0].node_type == TANK_TYPE).numpy().sum()\n",
    "    output_nodes = len(tra_dataset[0].y[0]) # remove reservoirs\n",
    "    gn = GraphNormalizer(junctions + tanks, output=['pressure', 'pump_flow'])\n",
    "    gn = gn.fit(tra_dataset)\n",
    "    # The normalization messed with the 1H_type since we want unique IDs\n",
    "    tra_dataset, _ = create_dataset(tra_database, normalizer=gn)\n",
    "    val_dataset, _ = create_dataset(val_database, normalizer=gn)\n",
    "    tst_dataset, _ = create_dataset(tst_database, normalizer=gn)\n",
    "    node_size, edge_size = tra_dataset[0].x.size(-1), tra_dataset[0].edge_attr.size(-1)\n",
    "    \n",
    "    # dataloader\n",
    "    # transform dataset for MLP\n",
    "    # We begin with the MLP versions, when I want to add GNNs, check Riccardo's code\n",
    "    A10, A12 = create_incidence_matrices(tra_dataset, A12_bar)\n",
    "    tra_dataset_MLP, num_inputs, indices = create_dataset_MLP_from_graphs(tra_dataset)\n",
    "    val_dataset_MLP = create_dataset_MLP_from_graphs(val_dataset)[0]\n",
    "    tst_dataset_MLP = create_dataset_MLP_from_graphs(tst_dataset)[0]\n",
    "    tra_loader = torch.utils.data.DataLoader(tra_dataset_MLP,\n",
    "                                             batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset_MLP,\n",
    "                                             batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "    tst_loader = torch.utils.data.DataLoader(tst_dataset_MLP,\n",
    "                                             batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "    # loop through different algorithms\n",
    "    for algorithm in cfg['algorithms']:\n",
    "        # Importing of configuration parameters\n",
    "        hyperParams = cfg['hyperParams'][algorithm]\n",
    "        all_combinations = ParameterGrid(hyperParams)\n",
    "\n",
    "        # create results dataframe\n",
    "        results_df = pd.DataFrame(list(all_combinations))\n",
    "        results_df = pd.concat([results_df,\n",
    "                                pd.DataFrame(index=np.arange(len(all_combinations)),\n",
    "                                             columns=list(res_columns))], axis=1)\n",
    "\n",
    "        for i, combination in enumerate(all_combinations):\n",
    "            # wandb.init(project=\"unrolling-epanet\", entity=\"mertz\")\n",
    "            print(f'{algorithm}: training combination {i + 1} of {len(all_combinations)}\\n')\n",
    "            \n",
    "            combination['indices'] = indices\n",
    "            combination['junctions'] = junctions\n",
    "            combination['num_outputs'] = output_nodes\n",
    "\n",
    "            # model creation\n",
    "            model = getattr(sys.modules[__name__], algorithm)(**combination).float().to(device)\n",
    "\n",
    "            # get combination dictionary to determine how are indices made\n",
    "            total_parameters = sum(p.numel() for p in model.parameters())\n",
    "            print(\"Total number of parameters is\", total_parameters)\n",
    "\n",
    "            # model optimizer\n",
    "            optimizer = optim.Adam(params=model.parameters(), betas=(0.9, 0.999), **cfg['adamParams'])\n",
    "\n",
    "            # training\n",
    "            patience = cfg['earlyStopping']['patience']\n",
    "            lr_rate = cfg['earlyStopping']['divisor']\n",
    "            lr_epoch = cfg['earlyStopping']['epoch_frequency']\n",
    "            train_config = {\"Patience\": patience, \"Learning Rate Divisor\": lr_rate, \"LR Epoch Division\": lr_epoch}\n",
    "            model, tra_losses, val_losses, elapsed_time = training(model, optimizer, tra_loader, val_loader,\n",
    "                                                                   patience=patience, report_freq=0,\n",
    "                                                                   n_epochs=num_epochs,\n",
    "                                                                   alpha=0, lr_rate=lr_rate, lr_epoch=lr_epoch,\n",
    "                                                                   normalization=None, path=\"experiments\")\n",
    "            \n",
    "            loss_plot = plot_loss(tra_losses, val_losses, f'{results_folder}/{wdn}/{algorithm}/loss/{i}')\n",
    "            R2_plot = plot_R2(model, val_loader, f'{results_folder}/{wdn}/{algorithm}/R2/{i}', normalization=gn)[1]\n",
    "            # store training history and model\n",
    "            pd.DataFrame(data=np.array([tra_losses, val_losses]).T).to_csv(\n",
    "                f'{results_folder}/{wdn}/{algorithm}/hist/{i}.csv')\n",
    "            torch.save(model, f'{results_folder}/{wdn}/{algorithm}/models/{i}.csv')\n",
    "\n",
    "            # compute and store predictions, compute r2 scores\n",
    "            losses = {}\n",
    "            max_losses = {}\n",
    "            min_losses = {}\n",
    "            r2_scores = {}\n",
    "            for split, loader in zip(['training', 'validation', 'testing'], [tra_loader, val_loader, tst_loader]):\n",
    "                losses[split], max_losses[split], min_losses[split], pred, real, test_time = testing(model, loader, normalization=gn)\n",
    "                r2_scores[split] = r2_score(real, pred)\n",
    "                if i == 0:\n",
    "                    pd.DataFrame(data=real.reshape(-1, output_nodes)).to_csv(\n",
    "                        f'{results_folder}/{wdn}/{algorithm}/pred/{split}/real.csv')  # save real obs\n",
    "                pd.DataFrame(data=pred.reshape(-1, output_nodes)).to_csv(\n",
    "                    f'{results_folder}/{wdn}/{algorithm}/pred/{split}/{i}.csv')\n",
    "\n",
    "            # log_wandb_data(combination, wdn, algorithm, len(tra_database), len(val_database), len(tst_database), cfg, train_config, loss_plot, R2_plot)\n",
    "            # store results\n",
    "            results_df.loc[i, res_columns] = (losses['training'], losses['validation'], losses['testing'],\n",
    "                                              max_losses['training'], max_losses['validation'], max_losses['testing'],\n",
    "                                              min_losses['training'], min_losses['validation'], min_losses['testing'],\n",
    "                                              r2_scores['training'], r2_scores['validation'], r2_scores['testing'],\n",
    "                                              total_parameters, elapsed_time, test_time)\n",
    "            \n",
    "        # Calculate dummy model    \n",
    "        # wandb.finish()\n",
    "        # save graph normalizer\n",
    "        # with open(f'{results_folder}/{wdn}/{algorithm}/gn.pickle', 'wb') as handle:\n",
    "        #     pickle.dump(gn, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        # \n",
    "        with open(f'{results_folder}/{wdn}/{algorithm}/model.pickle', 'wb') as handle:\n",
    "            torch.save(model, handle)\n",
    "        results_df.to_csv(f'{results_folder}/{wdn}/{algorithm}/results_{algorithm}.csv')\n",
    "        \n",
    "        print('Folder where model was saved is', results_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-01T12:03:11.613097800Z",
     "start_time": "2024-02-01T12:03:08.757940300Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'testing' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mDashboard\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Dashboard\n\u001B[1;32m----> 2\u001B[0m _, _, _, pred, real, timed \u001B[38;5;241m=\u001B[39m \u001B[43mtesting\u001B[49m(model, tst_loader, normalization\u001B[38;5;241m=\u001B[39mgn)\n\u001B[0;32m      4\u001B[0m pred \u001B[38;5;241m=\u001B[39m gn\u001B[38;5;241m.\u001B[39mdenormalize_multiple(pred, output_nodes)\n\u001B[0;32m      5\u001B[0m real \u001B[38;5;241m=\u001B[39m gn\u001B[38;5;241m.\u001B[39mdenormalize_multiple(real, output_nodes)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'testing' is not defined"
     ]
    }
   ],
   "source": [
    "from utils.Dashboard import Dashboard\n",
    "_, _, _, pred, real, timed = testing(model, tst_loader, normalization=gn)\n",
    "\n",
    "pred = gn.denormalize_multiple(pred, output_nodes)\n",
    "real = gn.denormalize_multiple(real, output_nodes)\n",
    "\n",
    "dummy = Dummy(junctions + tanks).evaluate(real)\n",
    "# Array below is created to ensure proper indexing of the nodes when displaying\n",
    "type_array = (tst_database[0].node_type == 0) | (tst_database[0].node_type == 2)\n",
    "d = Dashboard(pd.DataFrame(real[0:24, :]), pd.DataFrame(pred[0:24, :]),\n",
    "              to_networkx(tst_dataset[0], node_attrs=['pos', 'ID']), type_array)\n",
    "f = d.display_results()\n",
    "\n",
    "# for i in range(0, len(real[:]), 5):\n",
    "# for i in [0, 1, 6, 26, 36, 37]:\n",
    "#     plt.plot(real[0:100, i], label=\"Real\")\n",
    "#     plt.plot(pred[0:100, i], label=\"Predicted\")\n",
    "#     plt.plot(dummy[0:100, i], label=\"Dummy\")\n",
    "    \n",
    "#     plt.ylabel('Head')\n",
    "#     plt.xlabel('Timestep')\n",
    "    \n",
    "#     plt.legend()\n",
    "#     names = {0: 'Next to Reservoir', 1: 'Random Node', 6: 'Next to Tank', 26: 'Random Node', 36: 'Tank', 37: 'Pump'}\n",
    "#     plt.title(names[i])\n",
    "#     # save_response_graphs_in_ML_tracker(real, pred, names[i], i)\n",
    "#     plt.show()\n",
    "#     plt.close()\n",
    "\n",
    "# plt.plot(real[0:100, 37], label=\"Real\")\n",
    "# plt.plot(pred[0:100, 37], label=\"Predicted\")\n",
    "# plt.plot(dummy[0:100, 37], label=\"Dummy\")\n",
    "# plt.ylabel('LPS')\n",
    "# plt.xlabel('Timestep')\n",
    "# \n",
    "# plt.legend()\n",
    "# plt.title(names[37])\n",
    "# plt.show()\n",
    "# plt.close()\n",
    "# save_response_graphs_in_ML_tracker(real, pred, names[i], i)\n",
    "# Create a table\n",
    "\n",
    "# Add Plotly figure as HTML file into Table\n",
    "table = wandb.Table(columns = [\"Figure\" + str(i)])\n",
    "# with open('./my_HTML_' + str(i) + '.html', 'r', encoding='utf-8') as file:\n",
    "#     html_content = file.read()\n",
    "# table.add_data(wandb.Html(html_content))\n",
    "display(f)\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-01T09:27:31.614499600Z",
     "start_time": "2024-02-01T09:27:31.581338700Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Uni\\Thesis\\Albert\\GGNet\\utils\\normalization.py:159: UserWarning:\n",
      "\n",
      "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 1.0000e+00,  2.0000e+00,  3.0000e+00,  2.0000e+00,  1.0000e+00,\n",
       "        -7.1054e-15], dtype=torch.float64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gn.transform_array([1,2,3,2,1,0], 'pressure')\n",
    "gn.inverse_transform_array(gn.transform_array([1,2,3,2,1,0], 'pressure'), 'pressure')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-01T12:03:11.607645600Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def nse(observed, simulated):\n",
    "    \"\"\"\n",
    "    Calculate Nash-Sutcliffe Efficiency (NSE) for 2D tensors.\n",
    "\n",
    "    Args:\n",
    "    observed (torch.Tensor): Tensor containing observed values.\n",
    "    simulated (torch.Tensor): Tensor containing simulated values.\n",
    "\n",
    "    Returns:\n",
    "    NSE (float): Nash-Sutcliffe Efficiency value.\n",
    "    \"\"\"\n",
    "    assert observed.shape == simulated.shape, \"Input tensors must have the same shape.\"\n",
    "    \n",
    "    numerator = torch.sum((observed - simulated) ** 2)\n",
    "    denominator = torch.sum((observed - torch.mean(observed)) ** 2)\n",
    "    \n",
    "    nse = 1 - (numerator / denominator)\n",
    "    return nse.item()\n",
    "\n",
    "dummy_score = r2_score(real, dummy, multioutput='variance_weighted')\n",
    "model_score = r2_score(real, pred, multioutput='variance_weighted')\n",
    "print(\"R2-values \\n\", \"Dummy:\", dummy_score, \"\\n Model\", model_score)\n",
    "\n",
    "dummy_score = mean_absolute_error(real, dummy)\n",
    "model_score = mean_absolute_error(real, pred)\n",
    "print(\"MAE-values \\n\", \"Dummy:\", dummy_score, \"\\n Model\", model_score)\n",
    "\n",
    "dummy_score = mean_squared_error(real, dummy)\n",
    "model_score = mean_squared_error(real, pred)\n",
    "print(\"MSE-values \\n\", \"Dummy:\", dummy_score, \"\\n Model\", model_score)\n",
    "\n",
    "\n",
    "dummy_score = mean_squared_error(real, dummy, squared=False)\n",
    "model_score = mean_squared_error(real, pred, squared=False)\n",
    "print(\"RMSE-values \\n\", \"Dummy:\", dummy_score, \"\\n Model\", model_score)\n",
    "\n",
    "dummy_score = nse(real, dummy)\n",
    "model_score = nse(real, pred)\n",
    "print(\"NSE-values \\n\", \"Dummy:\", dummy_score, \"\\n Model\", model_score)\n",
    "\n",
    "dummy_scores = []\n",
    "model_scores = []\n",
    "\n",
    "for i in range(38):\n",
    "    dummy_score = nse(real[:, i], dummy[:, i])\n",
    "    model_score = nse(real[:, i], pred[:, i])\n",
    "    dummy_scores.append(dummy_score)\n",
    "    model_scores.append(model_score)\n",
    "    \n",
    "    print(\"NSE-values\", i, dummy_score, model_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-01T09:27:31.674895100Z",
     "start_time": "2024-02-01T09:27:31.640962Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 \u001B[38;2;7;117;61m0.97\u001B[0m\n",
      "2 \u001B[38;2;10;122;64m0.96\u001B[0m\n",
      "3 \u001B[38;2;30;154;81m0.89\u001B[0m\n",
      "4 \u001B[38;2;132;202;102m0.75\u001B[0m\n",
      "5 \u001B[38;2;254;254;189m0.50\u001B[0m\n",
      "6 \u001B[38;2;78;177;93m0.83\u001B[0m\n",
      "7 \u001B[38;2;165;0;38m-29.81\u001B[0m\n",
      "8 \u001B[38;2;187;226;119m0.66\u001B[0m\n",
      "9 \u001B[38;2;93;184;96m0.81\u001B[0m\n",
      "10 \u001B[38;2;7;117;61m0.97\u001B[0m\n",
      "11 \u001B[38;2;17;136;71m0.93\u001B[0m\n",
      "12 \u001B[38;2;122;197;101m0.77\u001B[0m\n",
      "13 \u001B[38;2;199;231;127m0.64\u001B[0m\n",
      "14 \u001B[38;2;114;194;100m0.78\u001B[0m\n",
      "15 \u001B[38;2;20;141;74m0.92\u001B[0m\n",
      "16 \u001B[38;2;2;107;56m0.99\u001B[0m\n",
      "17 \u001B[38;2;232;85;56m0.16\u001B[0m\n",
      "18 \u001B[38;2;7;117;61m0.97\u001B[0m\n",
      "19 \u001B[38;2;93;184;96m0.81\u001B[0m\n",
      "20 \u001B[38;2;84;180;94m0.82\u001B[0m\n",
      "21 \u001B[38;2;75;175;92m0.83\u001B[0m\n",
      "22 \u001B[38;2;8;119;62m0.97\u001B[0m\n",
      "23 \u001B[38;2;3;109;57m0.99\u001B[0m\n",
      "24 \u001B[38;2;253;182;104m0.32\u001B[0m\n",
      "25 \u001B[38;2;1;105;55m0.99\u001B[0m\n",
      "26 \u001B[38;2;27;152;80m0.90\u001B[0m\n",
      "27 \u001B[38;2;11;124;65m0.96\u001B[0m\n",
      "28 \u001B[38;2;96;186;97m0.81\u001B[0m\n",
      "29 \u001B[38;2;8;119;62m0.97\u001B[0m\n",
      "30 \u001B[38;2;15;132;69m0.94\u001B[0m\n",
      "31 \u001B[38;2;5;113;59m0.98\u001B[0m\n",
      "32 \u001B[38;2;6;115;60m0.97\u001B[0m\n",
      "33 \u001B[38;2;3;109;57m0.99\u001B[0m\n",
      "34 \u001B[38;2;3;109;57m0.99\u001B[0m\n",
      "35 \u001B[38;2;20;141;74m0.92\u001B[0m\n",
      "36 \u001B[38;2;42;159;84m0.88\u001B[0m\n",
      "37 \u001B[38;2;165;0;38m-90.26\u001B[0m\n",
      "38 \u001B[38;2;165;0;38m0.00\u001B[0m\n",
      "\n",
      "\n",
      "1 \u001B[38;2;149;209;104m0.72\u001B[0m\n",
      "2 \u001B[38;2;181;223;115m0.67\u001B[0m\n",
      "3 \u001B[38;2;183;224;117m0.67\u001B[0m\n",
      "4 \u001B[38;2;167;217;106m0.70\u001B[0m\n",
      "5 \u001B[38;2;236;247;165m0.55\u001B[0m\n",
      "6 \u001B[38;2;195;229;124m0.64\u001B[0m\n",
      "7 \u001B[38;2;127;199;101m0.76\u001B[0m\n",
      "8 \u001B[38;2;189;226;120m0.65\u001B[0m\n",
      "9 \u001B[38;2;173;220;110m0.68\u001B[0m\n",
      "10 \u001B[38;2;154;212;104m0.72\u001B[0m\n",
      "11 \u001B[38;2;154;212;104m0.72\u001B[0m\n",
      "12 \u001B[38;2;159;214;105m0.71\u001B[0m\n",
      "13 \u001B[38;2;203;232;129m0.63\u001B[0m\n",
      "14 \u001B[38;2;162;215;105m0.70\u001B[0m\n",
      "15 \u001B[38;2;154;212;104m0.72\u001B[0m\n",
      "16 \u001B[38;2;152;210;104m0.72\u001B[0m\n",
      "17 \u001B[38;2;179;222;114m0.67\u001B[0m\n",
      "18 \u001B[38;2;154;212;104m0.72\u001B[0m\n",
      "19 \u001B[38;2;157;213;105m0.71\u001B[0m\n",
      "20 \u001B[38;2;159;214;105m0.71\u001B[0m\n",
      "21 \u001B[38;2;164;216;105m0.70\u001B[0m\n",
      "22 \u001B[38;2;154;212;104m0.72\u001B[0m\n",
      "23 \u001B[38;2;162;215;105m0.71\u001B[0m\n",
      "24 \u001B[38;2;185;225;118m0.66\u001B[0m\n",
      "25 \u001B[38;2;159;214;105m0.71\u001B[0m\n",
      "26 \u001B[38;2;154;212;104m0.72\u001B[0m\n",
      "27 \u001B[38;2;152;210;104m0.72\u001B[0m\n",
      "28 \u001B[38;2;197;230;126m0.64\u001B[0m\n",
      "29 \u001B[38;2;175;220;111m0.68\u001B[0m\n",
      "30 \u001B[38;2;191;227;122m0.65\u001B[0m\n",
      "31 \u001B[38;2;152;210;104m0.72\u001B[0m\n",
      "32 \u001B[38;2;152;210;104m0.72\u001B[0m\n",
      "33 \u001B[38;2;152;210;104m0.72\u001B[0m\n",
      "34 \u001B[38;2;154;212;104m0.72\u001B[0m\n",
      "35 \u001B[38;2;169;218;107m0.69\u001B[0m\n",
      "36 \u001B[38;2;154;212;104m0.71\u001B[0m\n",
      "37 \u001B[38;2;134;203;102m0.75\u001B[0m\n",
      "38 \u001B[38;2;251;162;91m0.28\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "# plt.plot(dummy_scores, label=\"Dummy\", color='green')\n",
    "# plt.xlabel('Node')\n",
    "# plt.ylabel('NSE')\n",
    "# plt.title('Dummy')\n",
    "# plt.show()\n",
    "# plt.close()\n",
    "# plt.plot(model_scores, label=\"Model\", color='orange')\n",
    "# plt.xlabel('Node')\n",
    "# plt.ylabel('NSE')\n",
    "# plt.title('Model (UM)')\n",
    "# plt.show()\n",
    "# plt.close()\n",
    "\n",
    "# Create a colormap\n",
    "cmap = plt.get_cmap('RdYlGn')\n",
    "\n",
    "switch_7 = dummy_scores[6]\n",
    "dummy_scores[6] = dummy_scores[16]\n",
    "dummy_scores[16] = switch_7\n",
    "# Get the rgba values from the colormap\n",
    "dummy_colors = cmap(dummy_scores)\n",
    "\n",
    "\n",
    "switch_7 = model_scores[6]\n",
    "model_scores[6] = model_scores[16]\n",
    "model_scores[16] = switch_7\n",
    "model_colors = cmap(model_scores)\n",
    "\n",
    "# Print the numbers with their corresponding colors\n",
    "\n",
    "zip_dummy = zip(dummy_scores, dummy_colors)\n",
    "zip_model = zip(model_scores, model_colors)\n",
    "\n",
    "index = 0\n",
    "for num, color in zip_dummy:\n",
    "    index += 1\n",
    "    r, g, b, _ = color\n",
    "    print(index, f\"\\033[38;2;{int(r * 255)};{int(g * 255)};{int(b * 255)}m{format(num, '.2f')}\\033[0m\")\n",
    "\n",
    "print('\\n')\n",
    "# Print the numbers with their corresponding colors\n",
    "index = 0\n",
    "for num, color in zip_model:\n",
    "    index += 1\n",
    "    r, g, b, _ = color\n",
    "    print(index, f\"\\033[38;2;{int(r * 255)};{int(g * 255)};{int(b * 255)}m{format(num, '.2f')}\\033[0m\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-01T09:27:31.731864300Z",
     "start_time": "2024-02-01T09:27:31.664145500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 950])\n",
      "1706783825.4427874\n",
      "Tra loader length 7\n",
      "2 128 950\n",
      "2 128 950\n",
      "2 128 950\n",
      "2 128 950\n",
      "2 128 950\n",
      "2 128 950\n",
      "2 32 950\n",
      "1706783825.4525552\n",
      "Simulation time: 0.009767770767211914\n",
      "tensor([27.8412, 28.3808, 29.4993,  ..., 26.9995, 19.9920, 31.9231])\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "with open(f'{results_folder}/{wdn}/{algorithm}/model.pickle', 'rb') as handle:\n",
    "    loaded_model = torch.load(handle)\n",
    "    loaded_model.eval()\n",
    "    \n",
    "\n",
    "# print(len(tra_dataset_MLP), len(tra_dataset_MLP[0]), len(tra_dataset_MLP[0][0]))\n",
    "input = tra_dataset_MLP[0][0].unsqueeze(0).to(device)\n",
    "print(input.shape)\n",
    "\n",
    "start_time = time.time()\n",
    "print(start_time)\n",
    "print('Tra loader length', len(tra_loader))\n",
    "for batch in tra_loader:\n",
    "    print(len(batch), len(batch[0]), len(batch[0][0])   )\n",
    "    input = batch[0].to(device)\n",
    "    output = model(input)\n",
    "\n",
    "end_time = time.time()\n",
    "print(end_time)\n",
    "\n",
    "print(f\"Simulation time: {end_time - start_time}\")\n",
    "\n",
    "output = output.detach().cpu().numpy()\n",
    "real = gn.inverse_transform_array(output, 'pressure')\n",
    "print(real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-01T09:27:31.860814300Z",
     "start_time": "2024-02-01T09:27:31.710711300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The latest WDN folder is: unrolling_WDN0401\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './experiments/unrolling_WDN0401/FOS_tank/LSTM/pred/testing/real.csv'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[10], line 27\u001B[0m\n\u001B[0;32m     24\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo WDN folders found in the specified directory.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     26\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m latest_wdn_folder \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m---> 27\u001B[0m     real \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_csv\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m./experiments/unrolling_WDN\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mlatest_wdn_number\u001B[49m\u001B[38;5;132;43;01m:\u001B[39;49;00m\u001B[38;5;124;43m04d\u001B[39;49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m/FOS_tank/LSTM/pred/testing/real.csv\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mdrop(\n\u001B[0;32m     28\u001B[0m         columns\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mUnnamed: 0\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[0;32m     29\u001B[0m     lstm_pred \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_csv(\n\u001B[0;32m     30\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m./experiments/unrolling_WDN\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlatest_wdn_number\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m04d\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/FOS_tank/LSTM/pred/testing/0.csv\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39mdrop(\n\u001B[0;32m     31\u001B[0m         columns\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mUnnamed: 0\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[0;32m     32\u001B[0m     unrolling_pred \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_csv(\n\u001B[0;32m     33\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m./experiments/unrolling_WDN\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlatest_wdn_number\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m04d\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/FOS_tank/BaselineUnrolling/pred/testing/0.csv\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39mdrop(\n\u001B[0;32m     34\u001B[0m         columns\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mUnnamed: 0\u001B[39m\u001B[38;5;124m'\u001B[39m])\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001B[0m, in \u001B[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    305\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(args) \u001B[38;5;241m>\u001B[39m num_allow_args:\n\u001B[0;32m    306\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[0;32m    307\u001B[0m         msg\u001B[38;5;241m.\u001B[39mformat(arguments\u001B[38;5;241m=\u001B[39marguments),\n\u001B[0;32m    308\u001B[0m         \u001B[38;5;167;01mFutureWarning\u001B[39;00m,\n\u001B[0;32m    309\u001B[0m         stacklevel\u001B[38;5;241m=\u001B[39mstacklevel,\n\u001B[0;32m    310\u001B[0m     )\n\u001B[1;32m--> 311\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:680\u001B[0m, in \u001B[0;36mread_csv\u001B[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001B[0m\n\u001B[0;32m    665\u001B[0m kwds_defaults \u001B[38;5;241m=\u001B[39m _refine_defaults_read(\n\u001B[0;32m    666\u001B[0m     dialect,\n\u001B[0;32m    667\u001B[0m     delimiter,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    676\u001B[0m     defaults\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdelimiter\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m,\u001B[39m\u001B[38;5;124m\"\u001B[39m},\n\u001B[0;32m    677\u001B[0m )\n\u001B[0;32m    678\u001B[0m kwds\u001B[38;5;241m.\u001B[39mupdate(kwds_defaults)\n\u001B[1;32m--> 680\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:575\u001B[0m, in \u001B[0;36m_read\u001B[1;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[0;32m    572\u001B[0m _validate_names(kwds\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnames\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m))\n\u001B[0;32m    574\u001B[0m \u001B[38;5;66;03m# Create the parser.\u001B[39;00m\n\u001B[1;32m--> 575\u001B[0m parser \u001B[38;5;241m=\u001B[39m TextFileReader(filepath_or_buffer, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n\u001B[0;32m    577\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m chunksize \u001B[38;5;129;01mor\u001B[39;00m iterator:\n\u001B[0;32m    578\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:934\u001B[0m, in \u001B[0;36mTextFileReader.__init__\u001B[1;34m(self, f, engine, **kwds)\u001B[0m\n\u001B[0;32m    931\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m kwds[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m    933\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles: IOHandles \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m--> 934\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_make_engine\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mengine\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1218\u001B[0m, in \u001B[0;36mTextFileReader._make_engine\u001B[1;34m(self, f, engine)\u001B[0m\n\u001B[0;32m   1214\u001B[0m     mode \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrb\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1215\u001B[0m \u001B[38;5;66;03m# error: No overload variant of \"get_handle\" matches argument types\u001B[39;00m\n\u001B[0;32m   1216\u001B[0m \u001B[38;5;66;03m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001B[39;00m\n\u001B[0;32m   1217\u001B[0m \u001B[38;5;66;03m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001B[39;00m\n\u001B[1;32m-> 1218\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;241m=\u001B[39m \u001B[43mget_handle\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore[call-overload]\u001B[39;49;00m\n\u001B[0;32m   1219\u001B[0m \u001B[43m    \u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1220\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1221\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mencoding\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1222\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcompression\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcompression\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1223\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmemory_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmemory_map\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1224\u001B[0m \u001B[43m    \u001B[49m\u001B[43mis_text\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_text\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1225\u001B[0m \u001B[43m    \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mencoding_errors\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstrict\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1226\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstorage_options\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1227\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1228\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1229\u001B[0m f \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles\u001B[38;5;241m.\u001B[39mhandle\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py:786\u001B[0m, in \u001B[0;36mget_handle\u001B[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[0;32m    781\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(handle, \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m    782\u001B[0m     \u001B[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001B[39;00m\n\u001B[0;32m    783\u001B[0m     \u001B[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001B[39;00m\n\u001B[0;32m    784\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mencoding \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mmode:\n\u001B[0;32m    785\u001B[0m         \u001B[38;5;66;03m# Encoding\u001B[39;00m\n\u001B[1;32m--> 786\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[0;32m    787\u001B[0m \u001B[43m            \u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    788\u001B[0m \u001B[43m            \u001B[49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    789\u001B[0m \u001B[43m            \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    790\u001B[0m \u001B[43m            \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    791\u001B[0m \u001B[43m            \u001B[49m\u001B[43mnewline\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    792\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    793\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    794\u001B[0m         \u001B[38;5;66;03m# Binary mode\u001B[39;00m\n\u001B[0;32m    795\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mopen\u001B[39m(handle, ioargs\u001B[38;5;241m.\u001B[39mmode)\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: './experiments/unrolling_WDN0401/FOS_tank/LSTM/pred/testing/real.csv'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import regex as re\n",
    "\n",
    "# Directory path where you want to search\n",
    "directory_path = \"./experiments\"\n",
    "\n",
    "# Get a list of all subdirectories in the specified directory\n",
    "subdirectories = [d for d in os.listdir(directory_path) if os.path.isdir(os.path.join(directory_path, d))]\n",
    "\n",
    "# Filter and extract the numbers from directory names\n",
    "wdn_numbers = []\n",
    "for subdir in subdirectories:\n",
    "    match = re.match(r'unrolling_WDN(\\d{4})', subdir)\n",
    "    if match:\n",
    "        wdn_numbers.append(int(match.group(1)))\n",
    "\n",
    "# Find the latest WDN number\n",
    "latest_wdn_number = None\n",
    "if wdn_numbers:\n",
    "    latest_wdn_number = max(wdn_numbers)\n",
    "    latest_wdn_folder = f'unrolling_WDN{latest_wdn_number:04d}'\n",
    "    print(f\"The latest WDN folder is: {latest_wdn_folder}\")\n",
    "else:\n",
    "    print(\"No WDN folders found in the specified directory.\")\n",
    "\n",
    "if latest_wdn_folder is not None:\n",
    "    real = pd.read_csv(f'./experiments/unrolling_WDN{latest_wdn_number:04d}/FOS_tank/LSTM/pred/testing/real.csv').drop(\n",
    "        columns=['Unnamed: 0'])\n",
    "    lstm_pred = pd.read_csv(\n",
    "        f'./experiments/unrolling_WDN{latest_wdn_number:04d}/FOS_tank/LSTM/pred/testing/0.csv').drop(\n",
    "        columns=['Unnamed: 0'])\n",
    "    unrolling_pred = pd.read_csv(\n",
    "        f'./experiments/unrolling_WDN{latest_wdn_number:04d}/FOS_tank/BaselineUnrolling/pred/testing/0.csv').drop(\n",
    "        columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-01T09:27:31.863941100Z",
     "start_time": "2024-02-01T09:27:31.862893100Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Not sure if below makes sense since we now have an extra dimension\n",
    "res = real.sub(lstm_pred).pow(2).sum(axis=0)\n",
    "tot = real.sub(lstm_pred.mean(axis=0)).pow(2).sum(axis=0)\n",
    "r2_lstm = 1 - res / tot\n",
    "res = real.sub(unrolling_pred).pow(2).sum(axis=0)\n",
    "tot = real.sub(unrolling_pred.mean(axis=0)).pow(2).sum(axis=0)\n",
    "r2_unrolling = 1 - res / tot\n",
    "r2s = pd.concat([r2_lstm, r2_unrolling], axis=1).rename(columns={0: 'LSTM', 1: 'Base-U'})\n",
    "fig, ax = plt.subplots()\n",
    "r2s.plot.box(ax=ax)\n",
    "ax.set_title(\"$R^2$ Scores Comparison for PES\")\n",
    "ax.set_ylabel('$R^2$ Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-01T09:27:31.866095200Z",
     "start_time": "2024-02-01T09:27:31.863941100Z"
    }
   },
   "outputs": [],
   "source": [
    "model = torch.load(f'{results_folder}/{wdn}/{algorithm}/model.pickle')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
