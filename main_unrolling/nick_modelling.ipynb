{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T09:23:28.780696300Z",
     "start_time": "2023-10-30T09:23:28.738782700Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "import wandb\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "from utils.miscellaneous import read_config\n",
    "from utils.miscellaneous import create_folder_structure_MLPvsGNN\n",
    "from utils.miscellaneous import initalize_random_generators\n",
    "from utils.wandb_logger import save_response_graphs_in_ML_tracker\n",
    "from utils.normalization import *\n",
    "from utils.load import *\n",
    "\n",
    "from training.train import training\n",
    "from training.test import testing\n",
    "from training.models import * \n",
    "\n",
    "from utils.visualization import plot_R2, plot_loss\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.18\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T09:23:28.860729200Z",
     "start_time": "2023-10-30T09:23:28.750763300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse configuration file + initializations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T09:23:28.875242300Z",
     "start_time": "2023-10-30T09:23:28.812195Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating folder: ./experiments/unrolling_WDN0302\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# read config files\n",
    "cfg = read_config(\"config_unrolling.yaml\")\n",
    "# create folder for result\n",
    "exp_name = cfg['exp_name']\n",
    "data_folder = cfg['data_folder']\n",
    "results_folder = create_folder_structure_MLPvsGNN(cfg, parent_folder='./experiments')\n",
    "\n",
    "all_wdn_names = cfg['networks']\n",
    "initalize_random_generators(cfg, count=0)\n",
    "\n",
    "# initialize pytorch device\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "#torch.set_num_threads(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T09:23:28.875242300Z",
     "start_time": "2023-10-30T09:23:28.843682800Z"
    }
   },
   "outputs": [],
   "source": [
    "# TO DO: at the moment I am not using the parsed values for batch size and num_epochs ;\n",
    "# I am not using alpha as well because the loss has no \"smoothness\" penalty (yet)\n",
    "batch_size = cfg['trainParams']['batch_size']\n",
    "num_epochs = cfg['trainParams']['num_epochs']\n",
    "alpha = cfg['lossParams']['alpha']\n",
    "res_columns = ['train_loss', 'valid_loss', 'test_loss', 'max_train_loss', 'max_valid_loss', 'max_test_loss',\n",
    "               'min_train_loss', 'min_valid_loss', 'min_test_loss', 'r2_train', 'r2_valid',\n",
    "               'r2_test', 'total_params', 'total_time', 'test_time']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "I will be Creating different models as follows:\n",
    "\n",
    "* A simple LSTM\n",
    "* An unrolled version of Heads and Flows, without static variables\n",
    "* An unrolled version with Heads, Flows and static variables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T09:48:20.295238Z",
     "start_time": "2023-10-30T09:46:13.588893100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Working with FOS_tank, network 1 of 1\n",
      "Dummy: training combination 1 of 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]C:\\Users\\nmert\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([10, 25, 37])) that is different to the input size (torch.Size([10, 37])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (10) must match the size of tensor b (25) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[26], line 82\u001B[0m\n\u001B[0;32m     80\u001B[0m lr_epoch \u001B[38;5;241m=\u001B[39m cfg[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mearlyStopping\u001B[39m\u001B[38;5;124m'\u001B[39m][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mepoch_frequency\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m     81\u001B[0m train_config \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPatience\u001B[39m\u001B[38;5;124m\"\u001B[39m: patience, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLearning Rate Divisor\u001B[39m\u001B[38;5;124m\"\u001B[39m: lr_rate, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLR Epoch Division\u001B[39m\u001B[38;5;124m\"\u001B[39m: lr_epoch}\n\u001B[1;32m---> 82\u001B[0m model, tra_losses, val_losses, elapsed_time \u001B[38;5;241m=\u001B[39m \u001B[43mtraining\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtra_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_loader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     83\u001B[0m \u001B[43m                                                       \u001B[49m\u001B[43mpatience\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpatience\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreport_freq\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     84\u001B[0m \u001B[43m                                                       \u001B[49m\u001B[43mn_epochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnum_epochs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     85\u001B[0m \u001B[43m                                                       \u001B[49m\u001B[43malpha\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43malpha\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlr_rate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlr_rate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlr_epoch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlr_epoch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     86\u001B[0m \u001B[43m                                                       \u001B[49m\u001B[43mnormalization\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m     87\u001B[0m \u001B[43m                                                       \u001B[49m\u001B[43mpath\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mresults_folder\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m/\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mwdn\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m/\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43malgorithm\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m/\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     88\u001B[0m loss_plot \u001B[38;5;241m=\u001B[39m plot_loss(tra_losses, val_losses, \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresults_folder\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mwdn\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00malgorithm\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/loss/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     89\u001B[0m R2_plot \u001B[38;5;241m=\u001B[39m plot_R2(model, val_loader, \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresults_folder\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mwdn\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00malgorithm\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/R2/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m, normalization\u001B[38;5;241m=\u001B[39mgn)[\u001B[38;5;241m1\u001B[39m]\n",
      "File \u001B[1;32mC:\\Uni\\Thesis\\Albert\\GGNet\\main_unrolling\\training\\train.py:190\u001B[0m, in \u001B[0;36mtraining\u001B[1;34m(model, optimizer, train_loader, val_loader, n_epochs, patience, report_freq, alpha, lr_rate, lr_epoch, normalization, device, path)\u001B[0m\n\u001B[0;32m    187\u001B[0m \u001B[38;5;66;03m# torch.autograd.set_detect_anomaly(True)\u001B[39;00m\n\u001B[0;32m    188\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m tqdm(\u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1\u001B[39m, n_epochs \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m)):\n\u001B[0;32m    189\u001B[0m     \u001B[38;5;66;03m# Model training\u001B[39;00m\n\u001B[1;32m--> 190\u001B[0m     train_loss \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43malpha\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43malpha\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnormalization\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnormalization\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    191\u001B[0m \u001B[43m                             \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    193\u001B[0m     \u001B[38;5;66;03m# Model validation\u001B[39;00m\n\u001B[0;32m    194\u001B[0m     val_loss, _, _, _, _, _ \u001B[38;5;241m=\u001B[39m testing(model, val_loader, alpha\u001B[38;5;241m=\u001B[39malpha, normalization\u001B[38;5;241m=\u001B[39mnormalization)\n",
      "File \u001B[1;32mC:\\Uni\\Thesis\\Albert\\GGNet\\main_unrolling\\training\\train.py:131\u001B[0m, in \u001B[0;36mtrain_epoch\u001B[1;34m(model, loader, optimizer, alpha, normalization, device)\u001B[0m\n\u001B[0;32m    128\u001B[0m         preds \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mfloat()(x)\n\u001B[0;32m    129\u001B[0m     \u001B[38;5;66;03m# MSE loss function\u001B[39;00m\n\u001B[0;32m    130\u001B[0m     \u001B[38;5;66;03m# TODO Check how error is calculated. Ensure every iteration is penalized.\u001B[39;00m\n\u001B[1;32m--> 131\u001B[0m     loss \u001B[38;5;241m=\u001B[39m \u001B[43mnn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mMSELoss\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpreds\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    133\u001B[0m \u001B[38;5;66;03m# Normalization to have more representative loss values\u001B[39;00m\n\u001B[0;32m    134\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m normalization \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536\u001B[0m, in \u001B[0;36mMSELoss.forward\u001B[1;34m(self, input, target)\u001B[0m\n\u001B[0;32m    535\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor, target: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 536\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmse_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreduction\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreduction\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:3294\u001B[0m, in \u001B[0;36mmse_loss\u001B[1;34m(input, target, size_average, reduce, reduction)\u001B[0m\n\u001B[0;32m   3291\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m size_average \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m reduce \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   3292\u001B[0m     reduction \u001B[38;5;241m=\u001B[39m _Reduction\u001B[38;5;241m.\u001B[39mlegacy_get_string(size_average, reduce)\n\u001B[1;32m-> 3294\u001B[0m expanded_input, expanded_target \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbroadcast_tensors\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3295\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_C\u001B[38;5;241m.\u001B[39m_nn\u001B[38;5;241m.\u001B[39mmse_loss(expanded_input, expanded_target, _Reduction\u001B[38;5;241m.\u001B[39mget_enum(reduction))\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\functional.py:74\u001B[0m, in \u001B[0;36mbroadcast_tensors\u001B[1;34m(*tensors)\u001B[0m\n\u001B[0;32m     72\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function(tensors):\n\u001B[0;32m     73\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(broadcast_tensors, tensors, \u001B[38;5;241m*\u001B[39mtensors)\n\u001B[1;32m---> 74\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_VF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbroadcast_tensors\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: The size of tensor a (10) must match the size of tensor b (25) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "g_normalizers = []\n",
    "for ix_wdn, wdn in enumerate(all_wdn_names):\n",
    "    print(f'\\nWorking with {wdn}, network {ix_wdn + 1} of {len(all_wdn_names)}')\n",
    "\n",
    "    # retrieve wntr data\n",
    "    tra_database, val_database, tst_database = load_raw_dataset(wdn, data_folder)\n",
    "    # reduce training data\n",
    "    # tra_database = tra_database[:int(len(tra_database)*cfg['tra_prc'])]\n",
    "    if cfg['tra_num'] < len(tra_database):\n",
    "        tra_database = tra_database[:cfg['tra_num']]\n",
    "\n",
    "    # remove PES anomaly\n",
    "    if wdn == 'PES':\n",
    "        if len(tra_database) > 4468:\n",
    "            del tra_database[4468]\n",
    "            print('Removed PES anomaly')\n",
    "            print('Check', tra_database[4468].pressure.mean())\n",
    "\n",
    "    # get GRAPH datasets     \n",
    "    # later on we should change this and use normal scalers from scikit (something is off here)\n",
    "    tra_dataset, A12_bar = create_dataset(tra_database)\n",
    "\n",
    "    gn = GraphNormalizer()\n",
    "    gn = gn.fit(tra_dataset)\n",
    "    g_normalizers.append(gn)\n",
    "    # The normalization messed with the 1H_type since we want unique IDs\n",
    "    tra_dataset, _ = create_dataset(tra_database, normalizer=gn)\n",
    "    val_dataset, _ = create_dataset(val_database, normalizer=gn)\n",
    "    tst_dataset, _ = create_dataset(tst_database, normalizer=gn)\n",
    "    node_size, edge_size = tra_dataset[0].x.size(-1), tra_dataset[0].edge_attr.size(-1)\n",
    "    # number of nodes\n",
    "    n_nodes = (tra_database[0].node_type == 0).numpy().sum() + (\n",
    "            tra_database[0].node_type == 2).numpy().sum()  # remove reservoirs\n",
    "    # dataloader\n",
    "    # transform dataset for MLP\n",
    "    # We begin with the MLP versions, when I want to add GNNs, check Riccardo's code\n",
    "    A10, A12 = create_incidence_matrices(tra_dataset, A12_bar)\n",
    "    tra_dataset_MLP, num_inputs, indices = create_dataset_MLP_from_graphs(tra_dataset)\n",
    "    val_dataset_MLP = create_dataset_MLP_from_graphs(val_dataset)[0]\n",
    "    tst_dataset_MLP = create_dataset_MLP_from_graphs(tst_dataset)[0]\n",
    "    tra_loader = torch.utils.data.DataLoader(tra_dataset_MLP,\n",
    "                                             batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset_MLP,\n",
    "                                             batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "    tst_loader = torch.utils.data.DataLoader(tst_dataset_MLP,\n",
    "                                             batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "    # loop through different zalgorithms\n",
    "    for algorithm in cfg['algorithms']:\n",
    "        # Importing of configuration parameters\n",
    "        hyperParams = cfg['hyperParams'][algorithm]\n",
    "        all_combinations = ParameterGrid(hyperParams)\n",
    "\n",
    "        # create results dataframe\n",
    "        results_df = pd.DataFrame(list(all_combinations))\n",
    "        results_df = pd.concat([results_df,\n",
    "                                pd.DataFrame(index=np.arange(len(all_combinations)),\n",
    "                                             columns=list(res_columns))], axis=1)\n",
    "\n",
    "        for i, combination in enumerate(all_combinations):\n",
    "            # wandb.init(project=\"unrolling-epanet\", entity=\"mertz\")\n",
    "            print(f'{algorithm}: training combination {i + 1} of {len(all_combinations)}\\n')\n",
    "            \n",
    "            combination['indices'] = indices\n",
    "            combination['num_outputs'] = n_nodes\n",
    "\n",
    "            # model creation\n",
    "            model = getattr(sys.modules[__name__], algorithm)(**combination).float().to(device)\n",
    "\n",
    "            # get combination dictionary to determine how are indices made\n",
    "            # print(\"Model\", model, combination) \n",
    "\n",
    "            total_parameters = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "            # model optimizer\n",
    "            optimizer = optim.Adam(params=model.parameters(), betas=(0.9, 0.999), **cfg['adamParams'])\n",
    "\n",
    "            # training\n",
    "            patience = cfg['earlyStopping']['patience']\n",
    "            lr_rate = cfg['earlyStopping']['divisor']\n",
    "            lr_epoch = cfg['earlyStopping']['epoch_frequency']\n",
    "            train_config = {\"Patience\": patience, \"Learning Rate Divisor\": lr_rate, \"LR Epoch Division\": lr_epoch}\n",
    "            model, tra_losses, val_losses, elapsed_time = training(model, optimizer, tra_loader, val_loader,\n",
    "                                                                   patience=patience, report_freq=0,\n",
    "                                                                   n_epochs=num_epochs,\n",
    "                                                                   alpha=alpha, lr_rate=lr_rate, lr_epoch=lr_epoch,\n",
    "                                                                   normalization=None,\n",
    "                                                                   path=f'{results_folder}/{wdn}/{algorithm}/')\n",
    "            loss_plot = plot_loss(tra_losses, val_losses, f'{results_folder}/{wdn}/{algorithm}/loss/{i}')\n",
    "            R2_plot = plot_R2(model, val_loader, f'{results_folder}/{wdn}/{algorithm}/R2/{i}', normalization=gn)[1]\n",
    "            # store training history and model\n",
    "            pd.DataFrame(data=np.array([tra_losses, val_losses]).T).to_csv(\n",
    "                f'{results_folder}/{wdn}/{algorithm}/hist/{i}.csv')\n",
    "            torch.save(model, f'{results_folder}/{wdn}/{algorithm}/models/{i}.csv')\n",
    "\n",
    "            # compute and store predictions, compute r2 scores\n",
    "            losses = {}\n",
    "            max_losses = {}\n",
    "            min_losses = {}\n",
    "            r2_scores = {}\n",
    "            for split, loader in zip(['training', 'validation', 'testing'], [tra_loader, val_loader, tst_loader]):\n",
    "                losses[split], max_losses[split], min_losses[split], pred, real, test_time = testing(model, loader,\n",
    "                                                                                                     normalization=gn)\n",
    "                r2_scores[split] = r2_score(real, pred)\n",
    "                if i == 0:\n",
    "                    pd.DataFrame(data=real.reshape(-1, n_nodes)).to_csv(\n",
    "                        f'{results_folder}/{wdn}/{algorithm}/pred/{split}/real.csv')  # save real obs\n",
    "                pd.DataFrame(data=pred.reshape(-1, n_nodes)).to_csv(\n",
    "                    f'{results_folder}/{wdn}/{algorithm}/pred/{split}/{i}.csv')\n",
    "\n",
    "            # log_wandb_data(combination, wdn, algorithm, len(tra_database), len(val_database), len(tst_database), cfg, train_config, loss_plot, R2_plot)\n",
    "            # store results\n",
    "            results_df.loc[i, res_columns] = (losses['training'], losses['validation'], losses['testing'],\n",
    "                                              max_losses['training'], max_losses['validation'], max_losses['testing'],\n",
    "                                              min_losses['training'], min_losses['validation'], min_losses['testing'],\n",
    "                                              r2_scores['training'], r2_scores['validation'], r2_scores['testing'],\n",
    "                                              total_parameters, elapsed_time, test_time)\n",
    "\n",
    "        \n",
    "        # Calculate dummy model\n",
    "        # tra_loader\n",
    "        \n",
    "        # wandb.finish()\n",
    "        # save graph normalizer\n",
    "        with open(f'{results_folder}/{wdn}/{algorithm}/gn.pickle', 'wb') as handle:\n",
    "            pickle.dump(gn, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        with open(f'{results_folder}/{wdn}/{algorithm}/model.pickle', 'wb') as handle:\n",
    "            torch.save(model, handle)\n",
    "        results_df.to_csv(f'{results_folder}/{wdn}/{algorithm}/results_{algorithm}.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T09:23:30.711123400Z",
     "start_time": "2023-10-30T09:23:30.704883100Z"
    }
   },
   "outputs": [],
   "source": [
    "from utils.Dashboard import Dashboard\n",
    "from IPython.display import display\n",
    "\n",
    "_, _, _, pred, real, time = testing(model, val_loader, normalization=gn)\n",
    "pred = gn.inverse_transform_array(pred, 'pressure')\n",
    "real = gn.inverse_transform_array(real, 'pressure')\n",
    "pred = pred.reshape(-1, n_nodes)\n",
    "real = real.reshape(-1, n_nodes)\n",
    "\n",
    "# Array below is created to ensure proper indexing of the nodes when displaying\n",
    "type_array = (val_database[0].node_type == 0) | (val_database[0].node_type == 2)\n",
    "d = Dashboard(pd.DataFrame(real[0:24, :]), pd.DataFrame(pred[0:24, :]),\n",
    "              to_networkx(val_dataset[0], node_attrs=['pos', 'ID']), type_array)\n",
    "# f = d.display_results()\n",
    "\n",
    "for i in [0, 1, 7, 36]:\n",
    "    plt.plot(real[0:100, i], label=\"Real\")\n",
    "    plt.plot(pred[0:100, i], label=\"Predicted\")\n",
    "    plt.ylabel('Head')\n",
    "    plt.xlabel('Timestep')\n",
    "    \n",
    "    plt.legend()\n",
    "    names = {0: 'Reservoir', 1: 'Next to Reservoir', 7: 'Random Node', 36: 'Tank'}\n",
    "    plt.title(names[i])\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "# Create a table\n",
    "\n",
    "# Add Plotly figure as HTML file into Table\n",
    "# table = wandb.Table(columns = [\"Figure\" + str(i)])\n",
    "# with open('./my_HTML_' + str(i) + '.html', 'r', encoding='utf-8') as file:\n",
    "# \thtml_content = file.read()\n",
    "# table.add_data(wandb.Html(html_content))\n",
    "# display(f)\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-30T09:23:30.711123400Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import regex as re\n",
    "\n",
    "# Directory path where you want to search\n",
    "directory_path = \"./experiments\"\n",
    "\n",
    "# Get a list of all subdirectories in the specified directory\n",
    "subdirectories = [d for d in os.listdir(directory_path) if os.path.isdir(os.path.join(directory_path, d))]\n",
    "\n",
    "# Filter and extract the numbers from directory names\n",
    "wdn_numbers = []\n",
    "for subdir in subdirectories:\n",
    "    match = re.match(r'unrolling_WDN(\\d{4})', subdir)\n",
    "    if match:\n",
    "        wdn_numbers.append(int(match.group(1)))\n",
    "\n",
    "# Find the latest WDN number\n",
    "latest_wdn_number = None\n",
    "if wdn_numbers:\n",
    "    latest_wdn_number = max(wdn_numbers)\n",
    "    latest_wdn_folder = f'unrolling_WDN{latest_wdn_number:04d}'\n",
    "    print(f\"The latest WDN folder is: {latest_wdn_folder}\")\n",
    "else:\n",
    "    print(\"No WDN folders found in the specified directory.\")\n",
    "\n",
    "if latest_wdn_folder is not None:\n",
    "    real = pd.read_csv(f'./experiments/unrolling_WDN{latest_wdn_number:04d}/FOS_tank/LSTM/pred/testing/real.csv').drop(\n",
    "        columns=['Unnamed: 0'])\n",
    "    lstm_pred = pd.read_csv(\n",
    "        f'./experiments/unrolling_WDN{latest_wdn_number:04d}/FOS_tank/LSTM/pred/testing/0.csv').drop(\n",
    "        columns=['Unnamed: 0'])\n",
    "    unrolling_pred = pd.read_csv(\n",
    "        f'./experiments/unrolling_WDN{latest_wdn_number:04d}/FOS_tank/BaselineUnrolling/pred/testing/0.csv').drop(\n",
    "        columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-30T09:23:30.711123400Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Not sure if below makes sense since we now have an extra dimension\n",
    "res = real.sub(lstm_pred).pow(2).sum(axis=0)\n",
    "tot = real.sub(lstm_pred.mean(axis=0)).pow(2).sum(axis=0)\n",
    "r2_lstm = 1 - res / tot\n",
    "res = real.sub(unrolling_pred).pow(2).sum(axis=0)\n",
    "tot = real.sub(unrolling_pred.mean(axis=0)).pow(2).sum(axis=0)\n",
    "r2_unrolling = 1 - res / tot\n",
    "r2s = pd.concat([r2_lstm, r2_unrolling], axis=1).rename(columns={0: 'LSTM', 1: 'Base-U'})\n",
    "fig, ax = plt.subplots()\n",
    "r2s.plot.box(ax=ax)\n",
    "ax.set_title(\"$R^2$ Scores Comparison for PES\")\n",
    "ax.set_ylabel('$R^2$ Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-30T09:23:30.711123400Z"
    }
   },
   "outputs": [],
   "source": [
    "model = torch.load(f'{results_folder}/{wdn}/{algorithm}/model.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T09:23:30.711123400Z",
     "start_time": "2023-10-30T09:23:30.711123400Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
