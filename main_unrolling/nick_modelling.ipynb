{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-16T12:40:16.655954700Z",
     "start_time": "2023-10-16T12:40:10.919254200Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "import wandb\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch_geometric\n",
    "from torch_geometric.utils import to_networkx\n",
    "import torch.nn as nn\n",
    "from torch.nn import Sequential, Linear\n",
    "import networkx as nx\n",
    "import wntr\n",
    "\n",
    "from utils.miscellaneous import read_config\n",
    "from utils.miscellaneous import create_folder_structure_MLPvsGNN\n",
    "from utils.miscellaneous import initalize_random_generators\n",
    "from utils.wandb_logger import log_wandb_data\n",
    "\n",
    "from training.train import training\n",
    "from training.test import testing\n",
    "\n",
    "from utils.visualization import plot_R2, plot_loss\n",
    "from matplotlib\timport pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse configuration file + initializations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-16T12:40:16.655954700Z",
     "start_time": "2023-10-16T12:40:16.624697800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating folder: ./experiments/unrolling_WDN0102\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# read config files\n",
    "cfg = read_config(\"config_unrolling.yaml\")\n",
    "# create folder for result\n",
    "exp_name = cfg['exp_name']\n",
    "data_folder = cfg['data_folder']\n",
    "results_folder = create_folder_structure_MLPvsGNN(cfg, parent_folder='./experiments')\n",
    "\n",
    "\n",
    "all_wdn_names = cfg['networks']\n",
    "initalize_random_generators(cfg, count=0)\n",
    "\n",
    "# initialize pytorch device\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "#torch.set_num_threads(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-16T12:40:16.671525300Z",
     "start_time": "2023-10-16T12:40:16.655954700Z"
    }
   },
   "outputs": [],
   "source": [
    "# TO DO: at the moment I am not using the parsed values for batch size and num_epochs ;\n",
    "# I am not using alpha as well because the loss has no \"smoothness\" penalty (yet)\n",
    "batch_size = cfg['trainParams']['batch_size']\n",
    "num_epochs = cfg['trainParams']['num_epochs']\n",
    "alpha = cfg['lossParams']['alpha']\n",
    "res_columns = ['train_loss', 'valid_loss','test_loss','max_train_loss', 'max_valid_loss','max_test_loss', 'min_train_loss', 'min_valid_loss','min_test_loss','r2_train', 'r2_valid',\n",
    "\t\t\t   'r2_test','total_params','total_time','test_time']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-16T12:40:16.718394700Z",
     "start_time": "2023-10-16T12:40:16.687150900Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.base import BaseEstimator,TransformerMixin\n",
    "\n",
    "class PowerLogTransformer(BaseEstimator,TransformerMixin):\n",
    "\tdef __init__(self,log_transform=False,power=4,reverse=True):\n",
    "\t\tif log_transform == True:\n",
    "\t\t\tself.log_transform = log_transform\n",
    "\t\t\tself.power = None\n",
    "\t\telse:\n",
    "\t\t\tself.power = power\n",
    "\t\t\tself.log_transform = None\n",
    "\t\tself.reverse=reverse\n",
    "\t\tself.max_ = None\n",
    "\t\tself.min_ = None\n",
    "\n",
    "\tdef fit(self,X,y=None):\n",
    "\t\tself.max_ = np.max(X)\n",
    "\t\tself.min_ = np.min(X)\n",
    "\t\treturn self\n",
    "\n",
    "\tdef transform(self,X):\n",
    "\t\tif self.log_transform==True:\n",
    "\t\t\tif self.reverse == True:\n",
    "\t\t\t\treturn np.log1p(self.max_-X)\n",
    "\t\t\telse:\n",
    "\t\t\t\treturn np.log1p(X-self.min_)\n",
    "\t\telse:\n",
    "\t\t\tif self.reverse == True:\n",
    "\t\t\t\treturn (self.max_-X)**(1/self.power )\n",
    "\t\t\telse:\n",
    "\t\t\t\treturn (X-self.min_)**(1/self.power )\n",
    "\n",
    "\tdef inverse_transform(self,X):\n",
    "\t\tif self.log_transform==True:\n",
    "\t\t\tif self.reverse == True:\n",
    "\t\t\t\treturn (self.max_ - np.exp(X))\n",
    "\t\t\telse:\n",
    "\t\t\t\treturn (np.exp(X) + self.min_)\n",
    "\t\telse:\n",
    "\t\t\tif self.reverse == True:\n",
    "\t\t\t\treturn (self.max_ - X**self.power )\n",
    "\t\t\telse:\n",
    "\t\t\t\treturn (X**self.power + self.min_)\n",
    "\n",
    "\n",
    "class GraphNormalizer:\n",
    "\tdef __init__(self, x_feat_names=['head', 'base_demand', 'node_diameter'],\n",
    "\t\t\t\t ea_feat_names=['diameter', 'length', 'roughness'], output='pressure'):\n",
    "\t\t# store\n",
    "\t\tself.x_feat_names = x_feat_names\n",
    "\t\tself.ea_feat_names = ea_feat_names\n",
    "\t\tself.output = output\n",
    "\n",
    "\t\t# create separate scaler for each feature (can be improved, e.g., you can fit a scaler for multiple columns)\n",
    "\t\tself.scalers = {}\n",
    "\t\tfor feat in self.x_feat_names:\n",
    "\t\t\tif feat == 'elevation':\n",
    "\t\t\t\tself.scalers[feat] = PowerLogTransformer(log_transform=True, reverse=False)\n",
    "\t\t\telse:\n",
    "\t\t\t\tself.scalers[feat] = MinMaxScaler()\n",
    "\t\tself.scalers[output] = PowerLogTransformer(log_transform=True, reverse=True)\n",
    "\t\tfor feat in self.ea_feat_names:\n",
    "\t\t\tif feat == 'length':\n",
    "\t\t\t\tself.scalers[feat] = PowerLogTransformer(log_transform=True, reverse=False)\n",
    "\t\t\telse:\n",
    "\t\t\t\tself.scalers[feat] = MinMaxScaler()\n",
    "\n",
    "\tdef fit(self, graphs):\n",
    "\t\t''' Fit the scalers on an array of x and ea features\n",
    "        '''\n",
    "\t\tx, y, ea = from_graphs_to_pandas(graphs)\n",
    "\t\t\n",
    "\t\tfor ix, feat in enumerate(self.x_feat_names):\n",
    "\t\t\tself.scalers[feat] = self.scalers[feat].fit(x[:, ix].reshape(-1, 1))\n",
    "\t\t\t\n",
    "\t\tself.scalers[self.output] = self.scalers[self.output].fit(y.reshape(-1, 1))\n",
    "\t\t\n",
    "\t\tfor ix, feat in enumerate(self.ea_feat_names):\n",
    "\t\t\tself.scalers[feat] = self.scalers[feat].fit(ea[:, ix].reshape(-1, 1))\n",
    "\t\t\t\n",
    "\t\treturn self\n",
    "\n",
    "\tdef transform(self, graph):\n",
    "\t\t''' Transform graph based on normalizer\n",
    "        '''\n",
    "\t\tgraph = graph.clone()\n",
    "\t\tfor ix, feat in enumerate(self.x_feat_names):\n",
    "\t\t\ttemp = graph.x[:, ix].numpy().reshape(-1, 1)\n",
    "\t\t\tgraph.x[:, ix] = torch.tensor(self.scalers[feat].transform(temp).reshape(-1))\n",
    "\t\tfor ix, feat in enumerate(self.ea_feat_names):\n",
    "\t\t\ttemp = graph.edge_attr[:, ix].numpy().reshape(-1, 1)\n",
    "\t\t\tgraph.edge_attr[:, ix] = torch.tensor(self.scalers[feat].transform(temp).reshape(-1))\n",
    "\t\t\n",
    "\t\tif isinstance(graph.y, list):\n",
    "\t\t\ttransformed_y = []\n",
    "\t\t\tfor i, el in enumerate(graph.y):\n",
    "\t\t\t\ttransformed_y.append(torch.tensor(self.scalers[self.output].transform(graph.y[i].numpy().reshape(-1, 1)).reshape(-1)))\n",
    "\t\t\tgraph.y = transformed_y\n",
    "\t\telse:\n",
    "\t\t\tgraph.y = torch.tensor(self.scalers[self.output].transform(graph.y.numpy().reshape(-1, 1)).reshape(-1))\n",
    "\t\treturn graph\n",
    "\n",
    "\tdef inverse_transform(self, graph):\n",
    "\t\t''' Perform inverse transformation to return original features\n",
    "        '''\n",
    "\t\tgraph = graph.clone()\n",
    "\t\tfor ix, feat in enumerate(self.x_feat_names):\n",
    "\t\t\ttemp = graph.x[:, ix].numpy().reshape(-1, 1)\n",
    "\t\t\tgraph.x[:, ix] = torch.tensor(self.scalers[feat].inverse_transform(temp).reshape(-1))\n",
    "\t\tfor ix, feat in enumerate(self.ea_feat_names):\n",
    "\t\t\ttemp = graph.edge_attr[:, ix].numpy().reshape(-1, 1)\n",
    "\t\t\tgraph.edge_attr[:, ix] = torch.tensor(self.scalers[feat].inverse_transform(temp).reshape(-1))\n",
    "\t\tgraph.y = torch.tensor(self.scalers[self.output].inverse_transform(graph.y.numpy().reshape(-1, 1)).reshape(-1))\n",
    "\t\treturn graph\n",
    "\n",
    "\tdef transform_array(self, z, feat_name):\n",
    "\t\t'''\n",
    "            This is for MLP dataset; it can be done better (the entire thing, from raw data to datasets)\n",
    "        '''\n",
    "\t\treturn torch.tensor(self.scalers[feat_name].transform(z).reshape(-1))\n",
    "\n",
    "\tdef inverse_transform_array(self, z, feat_name):\n",
    "\t\t'''\n",
    "            This is for MLP dataset; it can be done better (the entire thing, from raw data to datasets)\n",
    "        '''\n",
    "\t\treturn torch.tensor(self.scalers[feat_name].inverse_transform(z).reshape(-1))\n",
    "\n",
    "def from_graphs_to_pandas(graphs):\n",
    "\tx = []\n",
    "\ty = []\n",
    "\tea = []\n",
    "\tfor i, graph in enumerate(graphs):\n",
    "\t\tx.append(graph.x.numpy())\n",
    "\t\t\n",
    "\t\tif isinstance(graph.y, list):\n",
    "\t\t\t\n",
    "\t\t\ty.append(np.concatenate(graph.y, axis=0))\n",
    "\t\telse:\n",
    "\t\t\ty.append(graph.y.numpy())\n",
    "\n",
    "\t\tea.append(graph.edge_attr.numpy())\n",
    "\t\n",
    "\treturn np.concatenate(x, axis=0), np.concatenate(y, axis=0), np.concatenate(ea, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-16T12:42:07.994063300Z",
     "start_time": "2023-10-16T12:42:07.947022100Z"
    }
   },
   "outputs": [],
   "source": [
    "# Constant indexes that help reconstruct the graph structure\n",
    "# Below are the indexes of the node attributes in the x torch vector\n",
    "HEAD_INDEX = 0 # Elevation + base head + initial level\n",
    "BASEDEMAND_INDEX = 1 \n",
    "NODE_DIAMETER_INDEX = 2 # Needed for tanks\n",
    "TYPE_INDEX = 3\n",
    "# The following three indexes describe edges in the edge_attr torch vector\n",
    "DIAMETER_INDEX = 0 \n",
    "LENGTH_INDEX = 1 \n",
    "ROUGHNESS_INDEX = 2 \n",
    "FLOW_INDEX = 3 \n",
    "POWER_INDEX = 4\n",
    "# The following three indexes describe the node types inside the x torch vector -> TYPE_INDEX variable\n",
    "JUNCTION_TYPE = 0\n",
    "RESERVOIR_TYPE = 1\n",
    "TANK_TYPE = 2\n",
    "\n",
    "def load_raw_dataset(wdn_name, data_folder):\n",
    "\t'''\n",
    "\tLoad tra/val/data for a water distribution network datasets\n",
    "\t-------\n",
    "\twdn_name : string\n",
    "\t\tprefix of pickle files to open\n",
    "\tdata_folder : string\n",
    "\t\tpath to datasets\n",
    "\t'''\n",
    "\n",
    "\tdata_tra = pickle.load(open(f'{data_folder}/train/{wdn_name}.p', \"rb\"))\n",
    "\tdata_val = pickle.load(open(f'{data_folder}/valid/{wdn_name}.p', \"rb\"))\n",
    "\tdata_tst = pickle.load(open(f'{data_folder}/test/{wdn_name}.p', \"rb\"))\n",
    "\n",
    "\treturn data_tra, data_val, data_tst\n",
    "\n",
    "def create_dataset(database, normalizer=None, output='pressure'):\n",
    "\t'''\n",
    "\tCreates working datasets dataset from the pickle databases\n",
    "\t------\n",
    "\tdatabase : list\n",
    "\t\teach element in the list is a pickle file containing Data objects\n",
    "\tnormalization: dict\n",
    "\t\tnormalize the dataset using mean and std\n",
    "\t'''\n",
    "\n",
    "\tgraphs = []\n",
    "\n",
    "\tfor i in database:\n",
    "\t\t\n",
    "\t\tgraph = torch_geometric.data.Data()\n",
    "\n",
    "\t\t# Node attributes\n",
    "\t\t# min_elevation = min(i.elevation[i.type_1H == 0])\n",
    "\t\t\n",
    "\t\t# The head below is junctions plus reservoirs (plus tanks when implemented)\n",
    "\t\thead = i.pressure + i.base_head + i.elevation + i.initial_level\n",
    "\t\t# type_1H is equal to 1 when the node is a reservoir and 2 when it's a tank\n",
    "\t\t\n",
    "\t\t# We want to make the tuple that constructs a node of any type\n",
    "\t\tgraph.x = torch.stack((i.elevation + i.base_head + i.initial_level, i.base_demand, i.node_diameter, i.node_type), dim=1).float()\n",
    "\t\t# graph.x = torch.stack((i.elevation + i.base_head, i.base_demand, i.type_1H), dim=1).float()\n",
    "\n",
    "\t\t# Position and ID\n",
    "\t\tgraph.pos = i.pos\n",
    "\t\tgraph.ID = i.ID\n",
    "\n",
    "\t\t# Edge index (Adjacency matrix)\n",
    "\t\tgraph.edge_index = i.edge_index\n",
    "\n",
    "\t\t# Edge attributes\n",
    "\t\tdiameter = i.diameter\n",
    "\t\tlength = i.length\n",
    "\t\troughness = i.roughness\n",
    "\t\tschedule = i.schedule\n",
    "\t\tgraph.edge_attr = torch.stack((diameter, length, roughness, schedule), dim=1).float()\n",
    "\n",
    "\t\t# If the length of the shape of pressure was 2 then it means that the simulation was continuous\n",
    "\t\tpress_shape = i.pressure.shape\n",
    "\t\tif len(press_shape) == 2:\n",
    "\t\t\tgraph.y = []\n",
    "\t\t\tfor time_step in range(press_shape[0]):\n",
    "\t\t\t\t# Appending the tanks to the output since their pressures also need to be predicted like any other node\n",
    "\t\t\t\tgraph.y.append(i.pressure[time_step][[(i.node_type == 0) | (i.node_type == 2)]].reshape(-1, 1))\n",
    "\t\telse:\n",
    "\t\t\t# Graph output (head)\n",
    "\t\t\tif output == 'head':\n",
    "\t\t\t\tgraph.y  = head[i.node_type == 0].reshape(-1, 1)\n",
    "\t\t\telse:\n",
    "\t\t\t\tgraph.y = i.pressure[i.node_type == 0].reshape(-1, 1)\n",
    "\t\t# normalization\n",
    "\t\tif normalizer is not None:\n",
    "\t\t\tgraph = normalizer.transform(graph)\n",
    "\n",
    "\t\tgraphs.append(graph)\n",
    "\t\t\n",
    "\tA12 = nx.incidence_matrix(to_networkx(graphs[0]), oriented=True).toarray().transpose()\n",
    "\treturn graphs, A12\n",
    "\n",
    "def create_dataset_MLP_from_graphs(graphs, features=['nodal_demands', 'base_heads','diameter','roughness','length', 'nodal_diameters'],no_res_out=True):\n",
    "\n",
    "\t# index edges to avoid duplicates: this considers all graphs to be UNDIRECTED!\n",
    "\tix_edge = graphs[0].edge_index.numpy().T\n",
    "\tix_edge = (ix_edge[:, 0] < ix_edge[:, 1])\n",
    "\n",
    "\t# position of reservoirs, and tanks\n",
    "\t# reservoir type is 1, tank is 2\n",
    "\tix_junct = graphs[0].x[:,TYPE_INDEX].numpy()==JUNCTION_TYPE\n",
    "\tix_res = graphs[0].x[:,TYPE_INDEX].numpy()==RESERVOIR_TYPE\n",
    "\tix_tank = graphs[0].x[:,TYPE_INDEX].numpy()==TANK_TYPE\n",
    "\tindices = {}\n",
    "\tprev_feature = None\n",
    "\tfor ix_feat, feature in enumerate(features):\n",
    "\t\tfor ix_item, item in enumerate(graphs):\n",
    "\t\t\tif feature == 'diameter':\n",
    "\t\t\t\tx_ = item.edge_attr[ix_edge,DIAMETER_INDEX]\n",
    "\t\t\telif feature == 'roughness':\n",
    "\t\t\t\t# remove reservoirs\n",
    "\t\t\t\tx_ = item.edge_attr[ix_edge,ROUGHNESS_INDEX]\n",
    "\t\t\telif feature == 'length':\n",
    "\t\t\t\t# remove reservoirs\n",
    "\t\t\t\tx_ = item.edge_attr[ix_edge,LENGTH_INDEX]\n",
    "\t\t\telif feature == 'nodal_demands':\n",
    "\t\t\t\t# remove reservoirs\n",
    "\t\t\t\tx_ = item.x[ix_junct,BASEDEMAND_INDEX]\n",
    "\t\t\telif feature == 'nodal_diameters':\n",
    "\t\t\t\t# Only get diameters for \n",
    "\t\t\t\tx_ = item.x[ix_tank,NODE_DIAMETER_INDEX]\n",
    "\t\t\telif feature == 'base_heads':\n",
    "\t\t\t\t# filter below on ix_res or ix_tank\n",
    "\t\t\t\tix_res_or_tank = np.logical_or(ix_res, ix_tank)\n",
    "\t\t\t\tx_ = item.x[ix_res_or_tank,HEAD_INDEX]\n",
    "\t\t\telse:\n",
    "\t\t\t\traise ValueError(f'Feature {feature} not supported.')\t\n",
    "\t\t\t\n",
    "\t\t\tif ix_item == 0:\n",
    "\t\t\t\tx = x_\n",
    "\t\t\telse:\n",
    "\t\t\t\tx = torch.cat((x, x_), dim=0)\n",
    "\t\t\n",
    "\t\n",
    "\t\tif ix_feat == 0:\n",
    "\t\t\tX = x.reshape(len(graphs), -1)\n",
    "\t\telse:\n",
    "\t\t\tX = torch.cat((X, x.reshape(len(graphs), -1)), dim=1)\n",
    "\t\t\n",
    "\t\tif prev_feature:\n",
    "\t\t\tindices[feature] = slice(indices[prev_feature].stop, X.shape[1], 1)\n",
    "\t\telse:\n",
    "\t\t\tindices[feature] = slice(0, X.shape[1], 1)\n",
    "\t\t\n",
    "\t\tprev_feature = feature\n",
    "\t\n",
    "\tfor ix_item, item in enumerate(graphs):\n",
    "\t\t# remove reservoirs from y as well\n",
    "\t\tif ix_item == 0:\n",
    "\t\t\tif no_res_out:\n",
    "\t\t\t\tif isinstance(item.y, list):\n",
    "\t\t\t\t\ty = torch.stack(item.y).unsqueeze(0).expand(1, -1, -1)\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\ty = item.y\n",
    "\t\t\telse:\n",
    "\t\t\t\ty = item.y[ix_junct]\n",
    "\t\t\t\n",
    "\t\telse:\n",
    "\t\t\tif no_res_out:\n",
    "\t\t\t\tif isinstance(item.y, list):\n",
    "\t\t\t\t\ty = torch.cat((y, torch.stack(item.y).unsqueeze(0).expand(1, -1, -1)), dim=0)\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\ty = torch.cat((y, item.y), dim=0)\n",
    "\t\t\telse:\n",
    "\t\t\t\ty = torch.cat((y, item.y[ix_junct]), dim=0)\n",
    "\t\t\t\t\n",
    "\t\n",
    "\t# If the shape of y is 1D then it means that the simulation was single period and should be turned to 2D according to the amount of graphs, if 3D then it was continuous\n",
    "\tif len(y.shape) == 1:\n",
    "\t\ty = y.reshape(len(graphs), -1)\n",
    "\t# SOMEWHERE HERE THE SEQUENCE LENGTH NEEDS TO BE ADJUSTED\n",
    "\treturn torch.utils.data.TensorDataset(X, y), X.shape[1], indices\n",
    "\n",
    "def create_incidence_matrices(graphs,incidence_matrix):\n",
    "\n",
    "\t# position of reservoirs\n",
    "\n",
    "\tix_junct = graphs[0].x[:,TYPE_INDEX].numpy()==JUNCTION_TYPE\n",
    "\tix_edge = graphs[0].edge_index.numpy().T\n",
    "\tix_edge = (ix_edge[:, 0] < ix_edge[:, 1])\n",
    "\tincidence_matrix = incidence_matrix[ix_edge,:]\n",
    "\t# The A12 incidence matrix is definitely only junctions, but should the A10 include tanks?\n",
    "\tA10 = incidence_matrix[:, ~ix_junct]\n",
    "\tA12 = incidence_matrix[:, ix_junct]\n",
    "\tA12[np.where(A10 == 1),:] *= -1\n",
    "\tA10[np.where(A10 == 1),:] *= -1\n",
    "\treturn A10, A12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "I will be Creating different models as follows:\n",
    "\n",
    "* A simple LSTM\n",
    "* An unrolled version of Heads and Flows, without static variables\n",
    "* An unrolled version with Heads, Flows and static variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-16T12:42:08.692753800Z",
     "start_time": "2023-10-16T12:42:08.676954Z"
    }
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "\tdef __init__(self, num_outputs, hid_channels, indices, num_layers=6):\n",
    "\t\tsuper(MLP, self).__init__()\n",
    "\t\ttorch.manual_seed(42)\n",
    "\t\tself.hid_channels = hid_channels\n",
    "\t\tself.indices = indices\n",
    "\t\t\n",
    "\t\tself.total_input_length = indices[list(indices.keys())[-1]].stop\n",
    "\t\t\n",
    "\t\tlayers = [Linear(self.total_input_length, hid_channels),\n",
    "\t\t\t\t  nn.ReLU()]\n",
    "\n",
    "\t\tfor l in range(num_layers-1):\n",
    "\t\t\tlayers += [Linear(hid_channels, hid_channels),\n",
    "\t\t\t\t\t   nn.ReLU()]\n",
    "\n",
    "\t\tlayers += [Linear(hid_channels, num_outputs)]\n",
    "\n",
    "\t\tself.main = nn.Sequential(*layers)\n",
    "\n",
    "\tdef forward(self, x, num_steps=1):\n",
    "\n",
    "\t\tx = self.main(x)\n",
    "\n",
    "\t\treturn x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-16T12:42:08.973883500Z",
     "start_time": "2023-10-16T12:42:08.958571Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define your LSTM model class\n",
    "class LSTM(nn.Module):\n",
    "\tdef __init__(self, num_outputs, hid_channels, indices, num_layers=2):\n",
    "\t\tsuper(LSTM, self).__init__()\n",
    "\t\tself.hidden_size = hid_channels\n",
    "\t\tself.num_layers = num_layers\n",
    "\t\tself.output_size = num_outputs\n",
    "\t\tself.input_size = indices[list(indices.keys())[-1]].stop\n",
    "\t\t\n",
    "\t\t# Define the LSTM layer\n",
    "\t\tself.lstm = nn.LSTM(self.input_size + self.output_size, self.hidden_size, num_layers, batch_first=True)\n",
    "\t\t\n",
    "\t\t# Define the output layer\n",
    "\t\tself.linear = nn.Linear(self.hidden_size, self.output_size)\n",
    "\t\t\t\n",
    "\n",
    "\tdef forward(self, static_input, num_steps=25):\n",
    "\t\t# Initialize hidden state with zeros\n",
    "\t\t# x = x.unsqueeze(1)\n",
    "\t\tbatch_size = static_input.size(0)\n",
    "\t\t\n",
    "\t\t# Initial hidden state and cell state\n",
    "\t\th0 = torch.zeros(self.num_layers, batch_size, self.hidden_size, dtype=torch.float32).to(static_input.device)\n",
    "\t\tc0 = torch.zeros(self.num_layers, batch_size, self.hidden_size, dtype=torch.float32).to(static_input.device)\n",
    "\t\t\n",
    "\t\t# Initialize the output sequence with zeros\n",
    "\t\toutput_seq = torch.zeros(batch_size, num_steps, self.output_size, dtype=torch.float32).to(static_input.device)\n",
    "\t\t\n",
    "\t\trepeated_static_input = static_input.unsqueeze(1).repeat(1, 1, 1)\n",
    "\t\t# print(repeated_static_input.shape, static_input.shape, output_seq.shape)\n",
    "\t\t# Iterate through time steps\n",
    "\t\tfor t in range(num_steps):\n",
    "\t\t\t# Concatenate static_input with the previous output (if available)\n",
    "\t\t\tif t == 0:\n",
    "\t\t\t\tlstm_input = torch.cat((repeated_static_input, output_seq[:, t:t+1, :]), dim=-1)\n",
    "\t\t\telse:\n",
    "\t\t\t\tlstm_input = torch.cat((repeated_static_input, output_seq[:, t-1:t, :]), dim=-1)\n",
    "\t\t\t\t\n",
    "\t\t\th0 = h0.to(torch.float32)\n",
    "\t\t\tc0 = c0.to(torch.float32)\n",
    "\t\t\t# Forward pass through the LSTM\n",
    "\t\t\tlstm_input = lstm_input.to(torch.float32)\n",
    "\t\t\tlstm_output, (h0, c0) = self.lstm(lstm_input, (h0, c0))\n",
    "\t\t\n",
    "\t\t\t# Predict the output for the current time step\n",
    "\t\t\toutput_seq[:, t:t+1, :] = self.linear(lstm_output)\n",
    "\t\t\n",
    "\t\treturn output_seq.to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-16T12:42:09.291267600Z",
     "start_time": "2023-10-16T12:42:09.267809200Z"
    }
   },
   "outputs": [],
   "source": [
    "class BaselineUnrolling(nn.Module):\n",
    "\tdef __init__(self,num_outputs, indices, num_blocks):\n",
    "\t\tsuper(BaselineUnrolling, self).__init__()\n",
    "\t\ttorch.manual_seed(42)\n",
    "\t\tself.indices = indices\n",
    "\t\tself.num_heads = indices['nodal_demands'].stop\n",
    "\t\tself.num_flows = indices['diameter'].stop-indices['diameter'].start\n",
    "\t\tself.num_base_heads = indices['base_heads'].stop-indices['base_heads'].start\n",
    "\t\tself.num_blocks = num_blocks\n",
    "\t\tself.n = 1.852\n",
    "\n",
    "\t\tself.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\t\t\n",
    "\t\t\n",
    "\t\tself.hid_HF = nn.ModuleList()\n",
    "\t\tself.hid_FH = nn.ModuleList()\n",
    "\n",
    "\t\tfor i in range(num_blocks):\n",
    "\t\t\tself.hid_HF.append(Sequential(Linear(self.num_heads, self.num_flows), nn.ReLU()))\n",
    "\t\t\tself.hid_FH.append(Sequential(Linear(self.num_flows, self.num_heads), nn.ReLU()))\n",
    "\n",
    "\t\tself.out = Linear(self.num_heads, num_outputs)\n",
    "\n",
    "\tdef forward(self, x, num_steps=1):\n",
    "\t\t\n",
    "\t\ts, h0, d, c, l = torch.unsqueeze(x[:,self.indices['nodal_demands']],dim=2), \\\n",
    "\t\t\t\t\t\t\t   torch.unsqueeze(x[:,self.indices['base_heads']],dim=2), \\\n",
    "\t\t\t\t\t\t\t   x[:,self.indices['diameter']].float().view(-1,self.num_flows,1),\\\n",
    "\t\t\t\t\t\t\t\tx[:,self.indices['roughness']].float().view(-1,self.num_flows,1),\\\n",
    "\t\t\t\t\t\t\t\tx[:,self.indices['length']].float().view(-1,self.num_flows,1)\n",
    "\n",
    "\t\tq =  torch.mul(math.pi/4, torch.pow(d,2)).view(-1,self.num_flows).float()\n",
    "\n",
    "\t\t\n",
    "\t\tpredictions = []\n",
    "\t\tfor step in range(num_steps):\n",
    "\t\t\tfor j in range(self.num_blocks):\n",
    "\t\t\t\th = self.hid_FH[j](q)\n",
    "\t\t\t\tq = q - self.hid_HF[j](h)\n",
    "\n",
    "\n",
    "\t\t\t# Append the prediction for the current time step\n",
    "\t\t\tprediction = self.out(h)\n",
    "\t\t\tpredictions.append(prediction)\n",
    "\t\t\n",
    "\t\tif num_steps == 1:\n",
    "\t\t\treturn predictions[0]\n",
    "\t\t# Convert the list of predictions to a tensor\n",
    "\t\tpredictions = torch.stack(predictions, dim=1)\t\n",
    "\t\t\n",
    "\t\treturn predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-16T12:42:09.542087100Z",
     "start_time": "2023-10-16T12:42:09.526568500Z"
    }
   },
   "outputs": [],
   "source": [
    "class UnrollingModel(nn.Module):\n",
    "\tdef __init__(self, num_outputs, indices, num_blocks):\n",
    "\t\tsuper(UnrollingModel, self).__init__()\n",
    "\t\ttorch.manual_seed(42)\n",
    "\t\tself.indices = indices\t\t\n",
    "\t\tself.num_heads = indices['nodal_demands'].stop\n",
    "\t\tself.num_flows = indices['diameter'].stop-indices['diameter'].start\n",
    "\t\tself.num_base_heads = indices['base_heads'].stop-indices['base_heads'].start\n",
    "\t\tself.num_blocks = num_blocks\n",
    "\n",
    "\t\tself.hidq0_h = Linear(self.num_flows, self.num_heads) # 4.14 \n",
    "\t\tself.hids_q =  Linear(self.num_heads, self.num_flows) #4.6/4.10\n",
    "\t\tself.hidh0_h = Linear(self.num_base_heads, self.num_heads) #4.7/4.11\n",
    "\t\tself.hidh0_q = Linear(self.num_base_heads, self.num_flows) #4.8/4.12\n",
    "\t\tself.hid_S = Sequential(Linear(indices['length'].stop - indices['diameter'].start, self.num_flows), nn.ReLU()) #4.9/4.13\n",
    "\t\t\n",
    "\t\t# init.xavier_uniform_(self.hid_S[0].weight)\n",
    "\t\t# init.xavier_uniform_(self.hidq0_h.weight)\n",
    "\t\t# init.xavier_uniform_(self.hids_q.weight)\n",
    "\t\t# init.xavier_uniform_(self.hidh0_h.weight)\n",
    "\t\t# init.xavier_uniform_(self.hidh0_q.weight)\n",
    "\n",
    "\t\tself.hid_hf = nn.ModuleList()\n",
    "\t\tself.hid_fh = nn.ModuleList()\n",
    "\t\tself.resq = nn.ModuleList()\n",
    "\t\tself.hidD_h = nn.ModuleList()\n",
    "\t\tself.hidA_q = nn.ModuleList()\n",
    "\n",
    "\t\tfor i in range(num_blocks):\n",
    "\t\t\tself.hid_hf.append(Sequential(Linear(self.num_heads,self.num_flows), nn.PReLU()))\n",
    "\t\t\tself.hid_fh.append(Sequential(Linear(self.num_flows, self.num_heads), nn.ReLU()))\n",
    "\t\t\tself.resq.append(Sequential(Linear(self.num_flows,self.num_heads), nn.ReLU()))\n",
    "\t\t\tself.hidD_h.append(Sequential(Linear(self.num_flows,self.num_heads)))\n",
    "\t\t\tself.hidA_q.append(Sequential(Linear(self.num_flows,self.num_flows), nn.ReLU()))\n",
    "\t\t\t# init.xavier_uniform_(self.hid_hf[i][0].weight)\n",
    "\t\t\t# init.xavier_uniform_(self.hid_fh[i][0].weight)\n",
    "\t\t\t# init.xavier_uniform_(self.resq[i][0].weight)\n",
    "\t\t\t# init.xavier_uniform_(self.hidA_q[i][0].weight)\n",
    "\t\t\t# init.xavier_uniform_(self.hidD_h[i][0].weight)\n",
    "\t\t\t\n",
    "\t\tself.out = Linear(self.num_flows, num_outputs)\n",
    "\t\t# init.xavier_uniform_(self.out.weight)\n",
    "\n",
    "\tdef forward(self, x, num_steps=1):\n",
    "\t\t# s is the demand and h0 is the heads (perhaps different when tanks are added)\n",
    "\t\ts, h0, d, edge_features = x[:,self.indices['nodal_demands']].float(), x[:,self.indices['base_heads']].float(), x[:,self.indices['diameter']].float(), x[:,self.indices['diameter'].start:self.indices['length'].stop].float()\n",
    "\t\t\n",
    "\t\tres_h0_q, res_s_q, res_h0_h, res_S_q = self.hidh0_q(h0), self.hids_q(s), self.hidh0_h(h0), self.hid_S(edge_features)\n",
    "\t\t\n",
    "\t\tq =  torch.mul(math.pi/4, torch.pow(d,2)).float() # This is the educated \"guess\" of the flow\n",
    "\t\tres_q_h = self.hidq0_h(q) # 4.14\n",
    "\t\t\n",
    "\t\tpredictions = []\n",
    "\t\tfor step in range(num_steps):\n",
    "\t\t\tfor i in range(self.num_blocks):\n",
    "\t\n",
    "\t\t\t\tD_h = self.hidD_h[i](torch.mul(q, res_S_q)) # 4.16\n",
    "\t\t\t\tA_q = self.hidA_q[i](D_h) # 4.17\n",
    "\t\t\t\thid_x = torch.mul(A_q,torch.sum(torch.stack([q, res_s_q, res_h0_q]),dim=0)) # 4.18 (inside parentheses)\n",
    "\t\t\t\th = self.hid_fh[i](hid_x) # 4.18\n",
    "\t\t\t\thid_x = self.hid_hf[i](torch.mul(torch.sum(torch.stack([h,res_h0_h,res_q_h]),dim=0), D_h)) # 4.19 (inside parentheses)\n",
    "\t\t\t\tq = torch.sub(q,hid_x) # 4.19\n",
    "\t\t\t\tres_q_h = self.resq[i](q) # 4.14\n",
    "\n",
    "\n",
    "\t\t\t# Append the prediction for the current time step\n",
    "\t\t\tprediction = self.out(q)\n",
    "\t\t\tpredictions.append(prediction)\n",
    "\n",
    "\t\tif num_steps == 1:\n",
    "\t\t\treturn predictions[0]\n",
    "\t\t# Convert the list of predictions to a tensor\n",
    "\t\tpredictions = torch.stack(predictions, dim=1)\t\n",
    "\t\treturn predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-16T12:42:10.534744600Z",
     "start_time": "2023-10-16T12:42:10.315936900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Working with FOS_pump, network 1 of 1\n",
      "schedule tensor([], size=(118, 0))\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [118] at entry 0 and [118, 0] at entry 3",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[19], line 20\u001B[0m\n\u001B[0;32m     16\u001B[0m \t\t\u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mCheck\u001B[39m\u001B[38;5;124m'\u001B[39m,tra_database[\u001B[38;5;241m4468\u001B[39m]\u001B[38;5;241m.\u001B[39mpressure\u001B[38;5;241m.\u001B[39mmean())\n\u001B[0;32m     18\u001B[0m \u001B[38;5;66;03m# get GRAPH datasets    \u001B[39;00m\n\u001B[0;32m     19\u001B[0m \u001B[38;5;66;03m# later on we should change this and use normal scalers from scikit (something is off here)\u001B[39;00m\n\u001B[1;32m---> 20\u001B[0m tra_dataset, A12_bar \u001B[38;5;241m=\u001B[39m \u001B[43mcreate_dataset\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtra_database\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     22\u001B[0m gn \u001B[38;5;241m=\u001B[39m GraphNormalizer()\n\u001B[0;32m     23\u001B[0m gn \u001B[38;5;241m=\u001B[39m gn\u001B[38;5;241m.\u001B[39mfit(tra_dataset)\n",
      "Cell \u001B[1;32mIn[14], line 74\u001B[0m, in \u001B[0;36mcreate_dataset\u001B[1;34m(database, normalizer, output)\u001B[0m\n\u001B[0;32m     72\u001B[0m schedule \u001B[38;5;241m=\u001B[39m i\u001B[38;5;241m.\u001B[39mschedule\n\u001B[0;32m     73\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mschedule\u001B[39m\u001B[38;5;124m\"\u001B[39m, schedule)\n\u001B[1;32m---> 74\u001B[0m graph\u001B[38;5;241m.\u001B[39medge_attr \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstack\u001B[49m\u001B[43m(\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdiameter\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlength\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mroughness\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mschedule\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdim\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mfloat()\n\u001B[0;32m     76\u001B[0m \u001B[38;5;66;03m# If the length of the shape of pressure was 2 then it means that the simulation was continuous\u001B[39;00m\n\u001B[0;32m     77\u001B[0m press_shape \u001B[38;5;241m=\u001B[39m i\u001B[38;5;241m.\u001B[39mpressure\u001B[38;5;241m.\u001B[39mshape\n",
      "\u001B[1;31mRuntimeError\u001B[0m: stack expects each tensor to be equal size, but got [118] at entry 0 and [118, 0] at entry 3"
     ]
    }
   ],
   "source": [
    "for ix_wdn, wdn in enumerate(all_wdn_names):\n",
    "\tprint(f'\\nWorking with {wdn}, network {ix_wdn+1} of {len(all_wdn_names)}')\n",
    "\n",
    "\t# retrieve wntr data\n",
    "\ttra_database, val_database, tst_database = load_raw_dataset(wdn, data_folder)\n",
    "\t# reduce  ning data\n",
    "\t# tra_database = tra_database[:int(len(tra_database)*cfg['tra_prc'])]\n",
    "\tif cfg['tra_num'] < len(tra_database):\n",
    "\t\ttra_database = tra_database[:cfg['tra_num']]\n",
    "\n",
    "\t# remove PES anomaly\n",
    "\tif wdn == 'PES':\n",
    "\t\tif len(tra_database)>4468:\n",
    "\t\t\tdel tra_database[4468]\n",
    "\t\t\tprint('Removed PES anomaly')\n",
    "\t\t\tprint('Check',tra_database[4468].pressure.mean())\n",
    "\n",
    "\t# get GRAPH datasets     \n",
    "\t# later on we should change this and use normal scalers from scikit (something is off here)\n",
    "\ttra_dataset, A12_bar = create_dataset(tra_database)\n",
    "\t\n",
    "\tgn = GraphNormalizer()\n",
    "\tgn = gn.fit(tra_dataset)\n",
    "\t# The normalization messed with the 1H_type since we want unique IDs\n",
    "\ttra_dataset, _ = create_dataset(tra_database,normalizer=gn)\n",
    "\tval_dataset,_ = create_dataset(val_database,normalizer=gn)\n",
    "\ttst_dataset,_ = create_dataset(tst_database,normalizer=gn)\n",
    "\tnode_size, edge_size = tra_dataset[0].x.size(-1), tra_dataset[0].edge_attr.size(-1)\n",
    "\t# number of nodes\n",
    "\tn_nodes = (tra_database[0].node_type == 0).numpy().sum() + (tra_database[0].node_type == 2).numpy().sum() # remove reservoirs\n",
    "\t# dataloader\n",
    "\t# transform dataset for MLP\n",
    "\t# We begin with the MLP versions, when I want to add GNNs, check Riccardo's code\n",
    "\tA10,A12 = create_incidence_matrices(tra_dataset, A12_bar)\n",
    "\ttra_dataset_MLP, num_inputs, indices = create_dataset_MLP_from_graphs(tra_dataset)\n",
    "\tval_dataset_MLP = create_dataset_MLP_from_graphs(val_dataset)[0]\n",
    "\ttst_dataset_MLP = create_dataset_MLP_from_graphs(tst_dataset)[0]\n",
    "\ttra_loader = torch.utils.data.DataLoader(tra_dataset_MLP,\n",
    "\t\t\t\t\t\t\t\t\t\t\t batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "\tval_loader = torch.utils.data.DataLoader(val_dataset_MLP,\n",
    "\t\t\t\t\t\t\t\t\t\t\t batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "\ttst_loader = torch.utils.data.DataLoader(tst_dataset_MLP,\n",
    "\t\t\t\t\t\t\t\t\t\t\t batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "\t# loop through different algorithms\n",
    "\tfor algorithm in cfg['algorithms']:\n",
    "\t\t# Importing of configuration parameters\n",
    "\t\thyperParams = cfg['hyperParams'][algorithm]\n",
    "\t\tall_combinations = ParameterGrid(hyperParams)\n",
    "\n",
    "\n",
    "\t\t# create results dataframe\n",
    "\t\tresults_df = pd.DataFrame(list(all_combinations))\n",
    "\t\tresults_df = pd.concat([results_df,\n",
    "\t\t\t\t\t\t\t\tpd.DataFrame(index=np.arange(len(all_combinations)),\n",
    "\t\t\t\t\t\t\t\t\t\t  columns=list(res_columns))],axis=1)\n",
    "\n",
    "\t\tfor i, combination in enumerate(all_combinations):\n",
    "\t\t\t# wandb.init(project=\"unrolling-epanet\")\n",
    "\t\t\tprint(f'{algorithm}: training combination {i+1} of {len(all_combinations)}\\n')\n",
    "\t\t\tcombination['indices'] = indices\n",
    "\t\t\tcombination['num_outputs'] = n_nodes\n",
    "\t\t\t\n",
    "\t\t\t# model creation\n",
    "\t\t\tmodel = getattr(sys.modules[__name__], algorithm)(**combination).float().to(device)\n",
    "\t\t\t\n",
    "\t\t\t# get combination dictionary to determine how are indices made\n",
    "\t\t\t# print(\"Model\", model, combination) \n",
    "\t\t\t\n",
    "\t\t\ttotal_parameters = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "\t\t\t# model optimizer\n",
    "\t\t\toptimizer = optim.Adam(params=model.parameters(),betas=(0.9, 0.999), **cfg['adamParams'])\n",
    "\n",
    "\t\t\t# training\n",
    "\t\t\tpatience = 20\n",
    "\t\t\tlr_rate = 2\n",
    "\t\t\tlr_epoch = 100\n",
    "\t\t\ttrain_config = {\"Patience\": patience, \"Learning Rate Divisor\": lr_rate, \"LR Epoch Division\": lr_epoch}\n",
    "\t\t\tmodel, tra_losses, val_losses, elapsed_time = training(model, optimizer, tra_loader, val_loader,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tpatience=patience, report_freq=0, n_epochs=num_epochs,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   alpha=alpha, lr_rate=lr_rate, lr_epoch=lr_epoch,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   normalization=None, path = f'{results_folder}/{wdn}/{algorithm}/')\n",
    "\t\t\tloss_plot = plot_loss(tra_losses,val_losses,f'{results_folder}/{wdn}/{algorithm}/loss/{i}')\n",
    "\t\t\tR2_plot = plot_R2(model,val_loader,f'{results_folder}/{wdn}/{algorithm}/R2/{i}', normalization=gn)[1]\n",
    "\t\t\t# store training history and model\n",
    "\t\t\tpd.DataFrame(data = np.array([tra_losses, val_losses]).T).to_csv(\n",
    "\t\t\t\tf'{results_folder}/{wdn}/{algorithm}/hist/{i}.csv')\n",
    "\t\t\ttorch.save(model, f'{results_folder}/{wdn}/{algorithm}/models/{i}.csv')\n",
    "\n",
    "\t\t\t# compute and store predictions, compute r2 scores\n",
    "\t\t\tlosses = {}\n",
    "\t\t\tmax_losses = {}\n",
    "\t\t\tmin_losses = {}\n",
    "\t\t\tr2_scores = {}\n",
    "\t\t\tfor split, loader in zip(['training','validation','testing'],[tra_loader,val_loader,tst_loader]):\n",
    "\t\t\t\tlosses[split], max_losses[split], min_losses[split], pred, real, test_time = testing(model, loader, normalization=gn)\n",
    "\t\t\t\tr2_scores[split] = r2_score(real, pred)\n",
    "\t\t\t\tif i == 0:\n",
    "\t\t\t\t\tpd.DataFrame(data=real.reshape(-1,n_nodes)).to_csv(\n",
    "\t\t\t\t\t\tf'{results_folder}/{wdn}/{algorithm}/pred/{split}/real.csv') # save real obs\n",
    "\t\t\t\tpd.DataFrame(data=pred.reshape(-1,n_nodes)).to_csv(\n",
    "\t\t\t\t\tf'{results_folder}/{wdn}/{algorithm}/pred/{split}/{i}.csv')\n",
    "\t\t\t\n",
    "\t\t\t\n",
    "\t\t\t# log_wandb_data(combination, wdn, algorithm, len(tra_database), len(val_database), len(tst_database), cfg, train_config, loss_plot, R2_plot)\n",
    "\t\t\t# store results\n",
    "\t\t\tresults_df.loc[i,res_columns] = (losses['training'], losses['validation'], losses['testing'],\n",
    "\t\t\t\t\t\t\t\t\t\t\t max_losses['training'], max_losses['validation'], max_losses['testing'],\n",
    "\t\t\t\t\t\t\t\t\t\t\t min_losses['training'], min_losses['validation'], min_losses['testing'],\n",
    "\t\t\t\t\t\t\t\t\t\t\t r2_scores['training'], r2_scores['validation'], r2_scores['testing'],\n",
    "\t\t\t\t\t\t\t\t\t\t\t total_parameters, elapsed_time, test_time)\n",
    "\t\t\t\n",
    "\t\t\t_,_,_, pred, real, time = testing(model, val_loader)\n",
    "\t\t\tpred = pred.reshape(-1,n_nodes)\n",
    "\t\t\treal = real.reshape(-1,n_nodes)\n",
    "\t\t\t\n",
    "\t\t\tfor i in [0, 1, 7, 36]:\n",
    "\t\t\t\tplt.plot(real[0:24, i], label=\"Real\")\n",
    "\t\t\t\tplt.plot(pred[0:24, i], label=\"Predicted\")\n",
    "\t\t\t\tplt.ylabel('Head')\n",
    "\t\t\t\tplt.xlabel('Timestep')\n",
    "\t\t\t\tplt.legend()\n",
    "\t\t\t\t# wandb.log({f'Node {i}': wandb.Image(plt)})\n",
    "\t\t\t\tplt.close()\n",
    "\t\t\t\n",
    "\t\t\t# wandb.finish()\n",
    "\t\t# save graph normalizer\n",
    "\t\twith open(f'{results_folder}/{wdn}/{algorithm}/gn.pickle', 'wb') as handle:\n",
    "\t\t     pickle.dump(gn, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\t\twith open(f'{results_folder}/{wdn}/{algorithm}/model.pickle', 'wb') as handle:\n",
    "\t\t\ttorch.save(model, handle)\n",
    "\t\tresults_df.to_csv(f'{results_folder}/{wdn}/{algorithm}/results_{algorithm}.csv')\n",
    "\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-16T12:40:17.789765400Z"
    }
   },
   "outputs": [],
   "source": [
    "from utils.Dashboard import Dashboard\n",
    "from IPython.display import display\n",
    "\n",
    "_,_,_, pred, real, time = testing(model, val_loader)\n",
    "pred = pred.reshape(-1,n_nodes)\n",
    "real = real.reshape(-1,n_nodes)\n",
    "\n",
    "# Array below is created to ensure proper indexing of the nodes when displaying\n",
    "type_array = (val_database[0].node_type == 0) | (val_database[0].node_type == 2)\n",
    "d = Dashboard(pd.DataFrame(real[0:24, :]),pd.DataFrame(pred[0:24, :]),to_networkx(val_dataset[0],node_attrs=['pos', 'ID']),type_array)\n",
    "f = d.display_results()\n",
    "\n",
    "for i in [0, 1, 7, 36]:\n",
    "\tplt.plot(real[0:24, i], label=\"Real\")\n",
    "\tplt.plot(pred[0:24, i], label=\"Predicted\")\n",
    "\tplt.ylabel('Head')\n",
    "\tplt.xlabel('Timestep')\n",
    "\tplt.legend()\n",
    "\t# wandb.log({f'Node {i}': wandb.Image(plt)})\n",
    "\tplt.close()\n",
    "\n",
    "# Create a table\n",
    "\n",
    "# Add Plotly figure as HTML file into Table\n",
    "# table = wandb.Table(columns = [\"Figure\" + str(i)])\n",
    "# with open('./my_HTML_' + str(i) + '.html', 'r', encoding='utf-8') as file:\n",
    "# \thtml_content = file.read()\n",
    "# table.add_data(wandb.Html(html_content))\n",
    "display(f)\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-16T12:40:17.789765400Z",
     "start_time": "2023-10-16T12:40:17.789765400Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import regex as re\n",
    "# Directory path where you want to search\n",
    "directory_path = \"./experiments\"\n",
    "\n",
    "# Get a list of all subdirectories in the specified directory\n",
    "subdirectories = [d for d in os.listdir(directory_path) if os.path.isdir(os.path.join(directory_path, d))]\n",
    "\n",
    "# Filter and extract the numbers from directory names\n",
    "wdn_numbers = []\n",
    "for subdir in subdirectories:\n",
    "    match = re.match(r'unrolling_WDN(\\d{4})', subdir)\n",
    "    if match:\n",
    "        wdn_numbers.append(int(match.group(1)))\n",
    "\n",
    "# Find the latest WDN number\n",
    "latest_wdn_number = None\n",
    "if wdn_numbers:\n",
    "    latest_wdn_number = max(wdn_numbers)\n",
    "    latest_wdn_folder = f'unrolling_WDN{latest_wdn_number:04d}'\n",
    "    print(f\"The latest WDN folder is: {latest_wdn_folder}\")\n",
    "else:\n",
    "    print(\"No WDN folders found in the specified directory.\")\n",
    "\n",
    "if latest_wdn_folder is not None:\n",
    "\treal = pd.read_csv(f'./experiments/unrolling_WDN{latest_wdn_number:04d}/FOS_tank/LSTM/pred/testing/real.csv').drop(columns=['Unnamed: 0'])\n",
    "\tlstm_pred = pd.read_csv(f'./experiments/unrolling_WDN{latest_wdn_number:04d}/FOS_tank/LSTM/pred/testing/0.csv').drop(columns=['Unnamed: 0'])\n",
    "\tunrolling_pred =  pd.read_csv(f'./experiments/unrolling_WDN{latest_wdn_number:04d}/FOS_tank/BaselineUnrolling/pred/testing/0.csv').drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-16T12:40:17.789765400Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Not sure if below makes sense since we now have an extra dimension\n",
    "res = real.sub(lstm_pred).pow(2).sum(axis=0)\n",
    "tot = real.sub(lstm_pred.mean(axis=0)).pow(2).sum(axis=0)\n",
    "r2_lstm = 1 - res/tot\n",
    "res = real.sub(unrolling_pred).pow(2).sum(axis=0)\n",
    "tot = real.sub(unrolling_pred.mean(axis=0)).pow(2).sum(axis=0)\n",
    "r2_unrolling = 1 - res/tot\n",
    "r2s = pd.concat([r2_lstm,r2_unrolling],axis=1).rename(columns={0:'LSTM',1:'Base-U'})\n",
    "fig, ax = plt.subplots()\n",
    "r2s.plot.box(ax=ax)\n",
    "ax.set_title(\"$R^2$ Scores Comparison for PES\")\n",
    "ax.set_ylabel('$R^2$ Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-16T12:40:17.789765400Z",
     "start_time": "2023-10-16T12:40:17.789765400Z"
    }
   },
   "outputs": [],
   "source": [
    "model = torch.load(f'{results_folder}/{wdn}/{algorithm}/model.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-16T12:40:17.789765400Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
