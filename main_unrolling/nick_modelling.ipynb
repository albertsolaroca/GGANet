{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "import wandb\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "from utils.miscellaneous import read_config\n",
    "from utils.miscellaneous import create_folder_structure_MLPvsGNN\n",
    "from utils.miscellaneous import initalize_random_generators\n",
    "from utils.wandb_logger import save_response_graphs_in_ML_tracker\n",
    "from utils.normalization import *\n",
    "from utils.load import *\n",
    "\n",
    "from training.train import training\n",
    "from training.test import testing\n",
    "from training.models import * \n",
    "\n",
    "from utils.visualization import plot_R2, plot_loss\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse configuration file + initializations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# read config files\n",
    "cfg = read_config(\"config_unrolling.yaml\")\n",
    "# create folder for result\n",
    "exp_name = cfg['exp_name']\n",
    "data_folder = cfg['data_folder']\n",
    "results_folder = create_folder_structure_MLPvsGNN(cfg, parent_folder='./experiments')\n",
    "\n",
    "all_wdn_names = cfg['network']\n",
    "initalize_random_generators(cfg, count=0)\n",
    "\n",
    "# initialize pytorch device\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "#torch.set_num_threads(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# TO DO: at the moment I am not using the parsed values for batch size and num_epochs ;\n",
    "# I am not using alpha as well because the loss has no \"smoothness\" penalty (yet)\n",
    "batch_size = cfg['trainParams']['batch_size']\n",
    "num_epochs = cfg['trainParams']['num_epochs']\n",
    "res_columns = ['train_loss', 'valid_loss', 'test_loss', 'max_train_loss', 'max_valid_loss', 'max_test_loss',\n",
    "               'min_train_loss', 'min_valid_loss', 'min_test_loss', 'r2_train', 'r2_valid',\n",
    "               'r2_test', 'total_params', 'total_time', 'test_time']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "I will be Creating different models as follows:\n",
    "\n",
    "* A simple LSTM\n",
    "* An unrolled version of Heads and Flows, without static variables\n",
    "* An unrolled version with Heads, Flows and static variables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for ix_wdn, wdn in enumerate(all_wdn_names):\n",
    "    print(f'\\nWorking with {wdn}, network {ix_wdn + 1} of {len(all_wdn_names)}')\n",
    "\n",
    "    # retrieve wntr data\n",
    "    tra_database, val_database, tst_database = load_raw_dataset(wdn, data_folder)\n",
    "    # reduce training data\n",
    "    # tra_database = tra_database[:int(len(tra_database)*cfg['tra_prc'])]\n",
    "    if cfg['tra_num'] < len(tra_database):\n",
    "        tra_database = tra_database[:cfg['tra_num']]\n",
    "\n",
    "    # remove PES anomaly\n",
    "    if wdn == 'PES':\n",
    "        if len(tra_database) > 4468:\n",
    "            del tra_database[4468]\n",
    "            print('Removed PES anomaly')\n",
    "            print('Check', tra_database[4468].pressure.mean())\n",
    "\n",
    "    # get GRAPH datasets     \n",
    "    # later on we should change this and use normal scalers from scikit (something is off here)\n",
    "    tra_dataset, A12_bar = create_dataset(tra_database)\n",
    "\n",
    "    gn = GraphNormalizer()\n",
    "    gn = gn.fit(tra_dataset)\n",
    "    # The normalization messed with the 1H_type since we want unique IDs\n",
    "    tra_dataset, _ = create_dataset(tra_database, normalizer=gn)\n",
    "    val_dataset, _ = create_dataset(val_database, normalizer=gn)\n",
    "    tst_dataset, _ = create_dataset(tst_database, normalizer=gn)\n",
    "    node_size, edge_size = tra_dataset[0].x.size(-1), tra_dataset[0].edge_attr.size(-1)\n",
    "    # number of nodes\n",
    "    junctions = (tra_database[0].node_type == JUNCTION_TYPE).numpy().sum()\n",
    "    tanks = (tra_database[0].node_type == TANK_TYPE).numpy().sum()\n",
    "    output_nodes = len(tra_dataset[0].y[0]) # remove reservoirs\n",
    "    # dataloader\n",
    "    # transform dataset for MLP\n",
    "    # We begin with the MLP versions, when I want to add GNNs, check Riccardo's code\n",
    "    A10, A12 = create_incidence_matrices(tra_dataset, A12_bar)\n",
    "    tra_dataset_MLP, num_inputs, indices = create_dataset_MLP_from_graphs(tra_dataset)\n",
    "    val_dataset_MLP = create_dataset_MLP_from_graphs(val_dataset)[0]\n",
    "    tst_dataset_MLP = create_dataset_MLP_from_graphs(tst_dataset)[0]\n",
    "    tra_loader = torch.utils.data.DataLoader(tra_dataset_MLP,\n",
    "                                             batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset_MLP,\n",
    "                                             batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "    tst_loader = torch.utils.data.DataLoader(tst_dataset_MLP,\n",
    "                                             batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "    # loop through different algorithms\n",
    "    for algorithm in cfg['algorithms']:\n",
    "        # Importing of configuration parameters\n",
    "        hyperParams = cfg['hyperParams'][algorithm]\n",
    "        all_combinations = ParameterGrid(hyperParams)\n",
    "\n",
    "        # create results dataframe\n",
    "        results_df = pd.DataFrame(list(all_combinations))\n",
    "        results_df = pd.concat([results_df,\n",
    "                                pd.DataFrame(index=np.arange(len(all_combinations)),\n",
    "                                             columns=list(res_columns))], axis=1)\n",
    "\n",
    "        for i, combination in enumerate(all_combinations):\n",
    "            # wandb.init(project=\"unrolling-epanet\", entity=\"mertz\")\n",
    "            print(f'{algorithm}: training combination {i + 1} of {len(all_combinations)}\\n')\n",
    "            \n",
    "            combination['indices'] = indices\n",
    "            combination['junctions'] = junctions\n",
    "            combination['num_outputs'] = output_nodes\n",
    "\n",
    "            # model creation\n",
    "            model = getattr(sys.modules[__name__], algorithm)(**combination).float().to(device)\n",
    "\n",
    "            # get combination dictionary to determine how are indices made\n",
    "\n",
    "            total_parameters = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "            # model optimizer\n",
    "            optimizer = optim.Adam(params=model.parameters(), betas=(0.9, 0.999), **cfg['adamParams'])\n",
    "\n",
    "            # training\n",
    "            patience = cfg['earlyStopping']['patience']\n",
    "            lr_rate = cfg['earlyStopping']['divisor']\n",
    "            lr_epoch = cfg['earlyStopping']['epoch_frequency']\n",
    "            train_config = {\"Patience\": patience, \"Learning Rate Divisor\": lr_rate, \"LR Epoch Division\": lr_epoch}\n",
    "            model, tra_losses, val_losses, elapsed_time = training(model, optimizer, tra_loader, val_loader,\n",
    "                                                                   patience=patience, report_freq=0,\n",
    "                                                                   n_epochs=num_epochs,\n",
    "                                                                   alpha=0, lr_rate=lr_rate, lr_epoch=lr_epoch,\n",
    "                                                                   normalization=None, path=\"experiments\")\n",
    "            \n",
    "            loss_plot = plot_loss(tra_losses, val_losses, f'{results_folder}/{wdn}/{algorithm}/loss/{i}')\n",
    "            R2_plot = plot_R2(model, val_loader, f'{results_folder}/{wdn}/{algorithm}/R2/{i}', normalization=gn)[1]\n",
    "            # store training history and model\n",
    "            pd.DataFrame(data=np.array([tra_losses, val_losses]).T).to_csv(\n",
    "                f'{results_folder}/{wdn}/{algorithm}/hist/{i}.csv')\n",
    "            torch.save(model, f'{results_folder}/{wdn}/{algorithm}/models/{i}.csv')\n",
    "\n",
    "            # compute and store predictions, compute r2 scores\n",
    "            losses = {}\n",
    "            max_losses = {}\n",
    "            min_losses = {}\n",
    "            r2_scores = {}\n",
    "            for split, loader in zip(['training', 'validation', 'testing'], [tra_loader, val_loader, tst_loader]):\n",
    "                losses[split], max_losses[split], min_losses[split], pred, real, test_time = testing(model, loader, normalization=gn)\n",
    "                r2_scores[split] = r2_score(real, pred)\n",
    "                if i == 0:\n",
    "                    pd.DataFrame(data=real.reshape(-1, output_nodes)).to_csv(\n",
    "                        f'{results_folder}/{wdn}/{algorithm}/pred/{split}/real.csv')  # save real obs\n",
    "                pd.DataFrame(data=pred.reshape(-1, output_nodes)).to_csv(\n",
    "                    f'{results_folder}/{wdn}/{algorithm}/pred/{split}/{i}.csv')\n",
    "\n",
    "            # log_wandb_data(combination, wdn, algorithm, len(tra_database), len(val_database), len(tst_database), cfg, train_config, loss_plot, R2_plot)\n",
    "            # store results\n",
    "            results_df.loc[i, res_columns] = (losses['training'], losses['validation'], losses['testing'],\n",
    "                                              max_losses['training'], max_losses['validation'], max_losses['testing'],\n",
    "                                              min_losses['training'], min_losses['validation'], min_losses['testing'],\n",
    "                                              r2_scores['training'], r2_scores['validation'], r2_scores['testing'],\n",
    "                                              total_parameters, elapsed_time, test_time)\n",
    "            \n",
    "        # Calculate dummy model    \n",
    "        # wandb.finish()\n",
    "        # save graph normalizer\n",
    "        # with open(f'{results_folder}/{wdn}/{algorithm}/gn.pickle', 'wb') as handle:\n",
    "        #     pickle.dump(gn, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        # \n",
    "        with open(f'{results_folder}/{wdn}/{algorithm}/model.pickle', 'wb') as handle:\n",
    "            torch.save(model, handle)\n",
    "        results_df.to_csv(f'{results_folder}/{wdn}/{algorithm}/results_{algorithm}.csv')\n",
    "        \n",
    "        print('Folder where model was saved is', results_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "from utils.Dashboard import Dashboard\n",
    "_, _, _, pred, real, timed = testing(model, tst_loader, normalization=gn)\n",
    "pred = gn.inverse_transform_array(pred, 'pressure')\n",
    "real = gn.inverse_transform_array(real, 'pressure')\n",
    "pred = pred.reshape(-1, output_nodes)\n",
    "real = real.reshape(-1, output_nodes)\n",
    "dummy = Dummy().evaluate(real)\n",
    "# Array below is created to ensure proper indexing of the nodes when displaying\n",
    "type_array = (tst_database[0].node_type == 0) | (tst_database[0].node_type == 2)\n",
    "d = Dashboard(pd.DataFrame(real[0:24, :]), pd.DataFrame(pred[0:24, :]),\n",
    "              to_networkx(tst_dataset[0], node_attrs=['pos', 'ID']), type_array)\n",
    "# f = d.display_results()\n",
    "\n",
    "for i in [0, 1, 7, 26, 36, 37]:\n",
    "    plt.plot(real[0:100, i], label=\"Real\")\n",
    "    plt.plot(pred[0:100, i], label=\"Predicted\")\n",
    "    plt.plot(dummy[0:100, i], label=\"Dummy\")\n",
    "    plt.ylabel('Head')\n",
    "    plt.xlabel('Timestep')\n",
    "    \n",
    "    plt.legend()\n",
    "    names = {0: 'Reservoir', 1: 'Next to Reservoir', 7: 'Next to Tank', 26: 'Random Node', 36: 'Tank', 37: 'Pump'}\n",
    "    plt.title(names[i])\n",
    "    # save_response_graphs_in_ML_tracker(real, pred, names[i], i)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "# Create a table\n",
    "\n",
    "# Add Plotly figure as HTML file into Table\n",
    "table = wandb.Table(columns = [\"Figure\" + str(i)])\n",
    "# with open('./my_HTML_' + str(i) + '.html', 'r', encoding='utf-8') as file:\n",
    "#     html_content = file.read()\n",
    "# table.add_data(wandb.Html(html_content))\n",
    "# display(f)\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "gn.transform_array([1,2,3,2,1,0], 'pressure')\n",
    "gn.inverse_transform_array(gn.transform_array([1,2,3,2,1,0], 'pressure'), 'pressure')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def nse(observed, simulated):\n",
    "    \"\"\"\n",
    "    Calculate Nash-Sutcliffe Efficiency (NSE) for 2D tensors.\n",
    "\n",
    "    Args:\n",
    "    observed (torch.Tensor): Tensor containing observed values.\n",
    "    simulated (torch.Tensor): Tensor containing simulated values.\n",
    "\n",
    "    Returns:\n",
    "    NSE (float): Nash-Sutcliffe Efficiency value.\n",
    "    \"\"\"\n",
    "    assert observed.shape == simulated.shape, \"Input tensors must have the same shape.\"\n",
    "    \n",
    "    numerator = torch.sum((observed - simulated) ** 2)\n",
    "    denominator = torch.sum((observed - torch.mean(observed)) ** 2)\n",
    "    \n",
    "    nse = 1 - (numerator / denominator)\n",
    "    return nse.item()\n",
    "\n",
    "dummy_score = r2_score(real, dummy, multioutput='variance_weighted')\n",
    "model_score = r2_score(real, pred, multioutput='variance_weighted')\n",
    "print(\"R2-values \\n\", \"Dummy:\", dummy_score, \"\\n Model\", model_score)\n",
    "\n",
    "dummy_score = mean_absolute_error(real, dummy)\n",
    "model_score = mean_absolute_error(real, pred)\n",
    "print(\"MAE-values \\n\", \"Dummy:\", dummy_score, \"\\n Model\", model_score)\n",
    "\n",
    "dummy_score = mean_squared_error(real, dummy)\n",
    "model_score = mean_squared_error(real, pred)\n",
    "print(\"MSE-values \\n\", \"Dummy:\", dummy_score, \"\\n Model\", model_score)\n",
    "\n",
    "\n",
    "dummy_score = mean_squared_error(real, dummy, squared=False)\n",
    "model_score = mean_squared_error(real, pred, squared=False)\n",
    "print(\"RMSE-values \\n\", \"Dummy:\", dummy_score, \"\\n Model\", model_score)\n",
    "\n",
    "dummy_score = nse(real, dummy)\n",
    "model_score = nse(real, pred)\n",
    "print(\"NSE-values \\n\", \"Dummy:\", dummy_score, \"\\n Model\", model_score)\n",
    "\n",
    "dummy_scores = []\n",
    "model_scores = []\n",
    "for i in range(36):\n",
    "    dummy_score = nse(real[:, i], dummy[:, i])\n",
    "    model_score = nse(real[:, i], pred[:, i])\n",
    "    dummy_scores.append(dummy_score)\n",
    "    model_scores.append(model_score)\n",
    "    \n",
    "    print(\"R2-values\", dummy_score, model_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "with open(f'{results_folder}/{wdn}/{algorithm}/model.pickle', 'rb') as handle:\n",
    "    loaded_model = torch.load(handle)\n",
    "    loaded_model.eval()\n",
    "    \n",
    "\n",
    "# print(len(tra_dataset_MLP), len(tra_dataset_MLP[0]), len(tra_dataset_MLP[0][0]))\n",
    "input = tra_dataset_MLP[0][0].unsqueeze(0).to(device)\n",
    "print(input.shape)\n",
    "\n",
    "start_time = time.time_ns()\n",
    "print(start_time)\n",
    "for batch in tra_loader:\n",
    "    input = batch[0].to(device)\n",
    "    output = model(input)\n",
    "\n",
    "end_time = time.time_ns()\n",
    "print(end_time)\n",
    "\n",
    "print(f\"Simulation time: {end_time - start_time}\")\n",
    "\n",
    "output = output.detach().cpu().numpy()\n",
    "real = gn.inverse_transform_array(output, 'pressure')\n",
    "print(real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import regex as re\n",
    "\n",
    "# Directory path where you want to search\n",
    "directory_path = \"./experiments\"\n",
    "\n",
    "# Get a list of all subdirectories in the specified directory\n",
    "subdirectories = [d for d in os.listdir(directory_path) if os.path.isdir(os.path.join(directory_path, d))]\n",
    "\n",
    "# Filter and extract the numbers from directory names\n",
    "wdn_numbers = []\n",
    "for subdir in subdirectories:\n",
    "    match = re.match(r'unrolling_WDN(\\d{4})', subdir)\n",
    "    if match:\n",
    "        wdn_numbers.append(int(match.group(1)))\n",
    "\n",
    "# Find the latest WDN number\n",
    "latest_wdn_number = None\n",
    "if wdn_numbers:\n",
    "    latest_wdn_number = max(wdn_numbers)\n",
    "    latest_wdn_folder = f'unrolling_WDN{latest_wdn_number:04d}'\n",
    "    print(f\"The latest WDN folder is: {latest_wdn_folder}\")\n",
    "else:\n",
    "    print(\"No WDN folders found in the specified directory.\")\n",
    "\n",
    "if latest_wdn_folder is not None:\n",
    "    real = pd.read_csv(f'./experiments/unrolling_WDN{latest_wdn_number:04d}/FOS_tank/LSTM/pred/testing/real.csv').drop(\n",
    "        columns=['Unnamed: 0'])\n",
    "    lstm_pred = pd.read_csv(\n",
    "        f'./experiments/unrolling_WDN{latest_wdn_number:04d}/FOS_tank/LSTM/pred/testing/0.csv').drop(\n",
    "        columns=['Unnamed: 0'])\n",
    "    unrolling_pred = pd.read_csv(\n",
    "        f'./experiments/unrolling_WDN{latest_wdn_number:04d}/FOS_tank/BaselineUnrolling/pred/testing/0.csv').drop(\n",
    "        columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Not sure if below makes sense since we now have an extra dimension\n",
    "res = real.sub(lstm_pred).pow(2).sum(axis=0)\n",
    "tot = real.sub(lstm_pred.mean(axis=0)).pow(2).sum(axis=0)\n",
    "r2_lstm = 1 - res / tot\n",
    "res = real.sub(unrolling_pred).pow(2).sum(axis=0)\n",
    "tot = real.sub(unrolling_pred.mean(axis=0)).pow(2).sum(axis=0)\n",
    "r2_unrolling = 1 - res / tot\n",
    "r2s = pd.concat([r2_lstm, r2_unrolling], axis=1).rename(columns={0: 'LSTM', 1: 'Base-U'})\n",
    "fig, ax = plt.subplots()\n",
    "r2s.plot.box(ax=ax)\n",
    "ax.set_title(\"$R^2$ Scores Comparison for PES\")\n",
    "ax.set_ylabel('$R^2$ Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "model = torch.load(f'{results_folder}/{wdn}/{algorithm}/model.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
