{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from utils.miscellaneous import read_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "def load_raw_dataset(wdn_name, data_folder):\n",
    "\t'''\n",
    "\tLoad tra/val/data for a water distribution network datasets\n",
    "\t-------\n",
    "\twdn_name : string\n",
    "\t\tprefix of pickle files to open\n",
    "\tdata_folder : string\n",
    "\t\tpath to datasets\n",
    "\t'''\n",
    "\n",
    "\tdata_tra = pickle.load(open(f'{data_folder}/train/{wdn_name}.p', \"rb\"))\n",
    "\tdata_val = pickle.load(open(f'{data_folder}/valid/{wdn_name}.p', \"rb\"))\n",
    "\tdata_tst = pickle.load(open(f'{data_folder}/test/{wdn_name}.p', \"rb\"))\n",
    "\n",
    "\treturn data_tra, data_val, data_tst"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "cfg = read_config(\"config_unrolling_GNN.yaml\")\n",
    "data_folder = cfg['data_folder']\n",
    "all_wdn_names = cfg['networks']\n",
    "\n",
    "wdn = 'FOS'\n",
    "tra_database, val_database, tst_database = load_raw_dataset(wdn, data_folder)\n",
    "real = pd.read_csv(f'./experiments/unrolling_WDN/{wdn}/MLP/pred/testing/real.csv').drop(columns=['Unnamed: 0'])\n",
    "mlp_pred = pd.read_csv(f'./experiments/unrolling_WDN/{wdn}/MLP/pred/testing/1.csv').drop(columns=['Unnamed: 0'])\n",
    "unrolling_pred =  pd.read_csv(f'./experiments/unrolling_WDN/{wdn}/UnrollingModel/pred/testing/1.csv').drop(columns=['Unnamed: 0'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "0     0.251456\n1     0.353602\n2     0.409106\n3     0.460752\n4     0.610530\n5     0.316764\n6     0.258404\n7     0.287890\n8     0.299889\n9     0.376149\n10    0.392461\n11    0.450088\n12    0.516008\n13    0.451185\n14    0.395904\n15    0.359801\n16    0.300460\n17    0.370162\n18    0.434107\n19    0.435455\n20    0.435713\n21    0.373942\n22    0.360706\n23    0.266151\n24    0.353200\n25    0.406003\n26    0.381682\n27    0.310348\n28    0.351628\n29    0.367804\n30    0.361672\n31    0.371528\n32    0.352839\n33    0.349539\n34    0.318355\n35    0.298426\ndtype: float64"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_error_per_node_mlp = np.sqrt(((mlp_pred - real) ** 2).mean())\n",
    "mean_error_per_node_mlp"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import *\n",
    "class MLP(nn.Module):\n",
    "\tdef __init__(self, num_outputs, hid_channels, indices, num_layers=6):\n",
    "\t\tsuper(MLP, self).__init__()\n",
    "\t\ttorch.manual_seed(42)\n",
    "\t\tself.hid_channels = hid_channels\n",
    "\t\tself.indices = indices\n",
    "\t\tself.num_flows = indices[2] - indices[1]\n",
    "\n",
    "\t\tlayers = [Linear(indices[4], hid_channels),\n",
    "\t\t\t\t  nn.ReLU()]\n",
    "\n",
    "\t\tfor l in range(num_layers-1):\n",
    "\t\t\tlayers += [Linear(hid_channels, hid_channels),\n",
    "\t\t\t\t\t   nn.ReLU()]\n",
    "\n",
    "\t\tlayers += [Linear(hid_channels, num_outputs)]\n",
    "\n",
    "\t\tself.main = nn.Sequential(*layers)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\n",
    "\t\tx = self.main(x)\n",
    "\n",
    "\t\treturn x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "class UnrollingModel(nn.Module):\n",
    "\tdef __init__(self, num_outputs, indices, num_blocks):\n",
    "\t\tsuper(UnrollingModel, self).__init__()\n",
    "\t\ttorch.manual_seed(42)\n",
    "\t\tself.indices = indices\n",
    "\t\tself.num_heads = indices[0]\n",
    "\t\tself.num_flows = indices[2]-indices[1]\n",
    "\t\tself.num_blocks = num_blocks\n",
    "\n",
    "\t\tself.hidQ0_H = Linear(indices[2]-indices[1], self.num_heads)\n",
    "\t\tself.hidH0_Q = Linear(indices[1]-indices[0], self.num_flows)\n",
    "\t\tself.hidH0_H = Linear(indices[1]-indices[0], self.num_heads)\n",
    "\t\tself.hidq_Q =  Linear(indices[0], self.num_flows)\n",
    "\t\tself.hid_S = Sequential(Linear(indices[4] - indices[2], self.num_flows),\n",
    "\t\t\t\t\t\t   nn.ReLU())\n",
    "\n",
    "\t\tself.hid_HF = nn.ModuleList()\n",
    "\t\tself.hid_FH = nn.ModuleList()\n",
    "\t\tself.resQ = nn.ModuleList()\n",
    "\t\tself.hidD_Q = nn.ModuleList()\n",
    "\t\tself.hidD_H = nn.ModuleList()\n",
    "\n",
    "\t\tfor i in range(num_blocks):\n",
    "\t\t\tself.hid_HF.append(Sequential(Linear(self.num_heads,self.num_flows), nn.ReLU()))\n",
    "\t\t\tself.hid_FH.append(Sequential(Linear(self.num_flows, self.num_heads),\n",
    "\t\t\t\t\t\t   nn.ReLU()))\n",
    "\t\t\tself.resQ.append(Sequential(Linear(self.num_flows,self.num_heads),\n",
    "\t\t\t\t\t\t   nn.ReLU()))\n",
    "\t\t\tself.hidD_Q.append(Sequential(Linear(self.num_flows,self.num_flows),\n",
    "\t\t\t\t\t\t   nn.ReLU()))\n",
    "\t\t\tself.hidD_H.append(Linear(self.num_flows,self.num_heads))\n",
    "\n",
    "\t\tself.out = Linear(self.num_flows, num_outputs)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\n",
    "\t\tq, H0, Q, hid_S = x[:,:self.indices[0]], x[:,self.indices[0]:self.indices[1]], x[:,self.indices[1]:self.indices[2]], x[:,self.indices[2]:]\n",
    "\t\tres_H0_Q, res_q_Q, res_Q_H, res_H0_H, res_S_Q = self.hidH0_Q(H0), self.hidq_Q(q), self.hidQ0_H(Q), self.hidH0_H(H0), self.hid_S(hid_S)\n",
    "\n",
    "\n",
    "\t\tfor i in range(self.num_blocks-1):\n",
    "\n",
    "\t\t\tD_Q = self.hidD_Q[i](torch.mul(Q, res_S_Q))\n",
    "\t\t\tD_H = self.hidD_H[i](D_Q)\n",
    "\t\t\thid_x = torch.mul(D_Q,torch.sum(torch.stack([Q, res_q_Q, res_H0_Q]),dim=0))\n",
    "\t\t\tH = self.hid_FH[i](hid_x)\n",
    "\t\t\thid_x = self.hid_HF[i](torch.mul(torch.sum(torch.stack([H,res_H0_H,res_Q_H]),dim=0), D_H))\n",
    "\t\t\tQ = torch.sub(Q,hid_x)\n",
    "\t\t\tres_Q_H = self.resQ[i](Q)\n",
    "\t\t\tif torch.any(H > 100).item():\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\t\treturn self.out(Q)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "from models.virtual_nodes import add_virtual_nodes\n",
    "import torch_geometric\n",
    "from torch_geometric.utils import to_networkx\n",
    "\n",
    "# constant indexes for node and edge features\n",
    "HEAD_INDEX = 0\n",
    "BASEDEMAND_INDEX = 1\n",
    "TYPE_INDEX = 2\n",
    "DIAMETER_INDEX = 0\n",
    "LENGTH_INDEX = 1\n",
    "ROUGHNESS_INDEX = 2\n",
    "FLOW_INDEX = 3\n",
    "\n",
    "def load_raw_dataset(wdn_name, data_folder):\n",
    "\t'''\n",
    "\tLoad tra/val/data for a water distribution network datasets\n",
    "\t-------\n",
    "\twdn_name : string\n",
    "\t\tprefix of pickle files to open\n",
    "\tdata_folder : string\n",
    "\t\tpath to datasets\n",
    "\t'''\n",
    "\n",
    "\tdata_tra = pickle.load(open(f'{data_folder}/train/{wdn_name}.p', \"rb\"))\n",
    "\tdata_val = pickle.load(open(f'{data_folder}/valid/{wdn_name}.p', \"rb\"))\n",
    "\tdata_tst = pickle.load(open(f'{data_folder}/test/{wdn_name}.p', \"rb\"))\n",
    "\n",
    "\treturn data_tra, data_val, data_tst\n",
    "\n",
    "def create_dataset(database, normalizer=None, HW_rough_minmax=[60, 150],add_virtual_reservoirs=False, output='pressure'):\n",
    "\t'''\n",
    "\tCreates working datasets dataset from the pickle databases\n",
    "\t------\n",
    "\tdatabase : list\n",
    "\t\teach element in the list is a pickle file containing Data objects\n",
    "\tnormalization: dict\n",
    "\t\tnormalize the dataset using mean and std\n",
    "\t'''\n",
    "\t# Roughness info (Hazen-Williams) / TODO: remove the hard_coding\n",
    "\tminR = HW_rough_minmax[0]\n",
    "\tmaxR = HW_rough_minmax[1]\n",
    "\n",
    "\tgraphs = []\n",
    "\n",
    "\tfor i in database:\n",
    "\t\tgraph = torch_geometric.data.Data()\n",
    "\n",
    "\t\t# Node attributes\n",
    "\t\t# elevation_head = i.elevation + i.base_head\n",
    "\t\t# elevation_head = i.elevation.clone()\n",
    "\t\t# elevation_head[elevation_head == 0] = elevation_head.mean()\n",
    "\n",
    "\t\tmin_elevation = min(i.elevation[i.type_1H == 0])\n",
    "\t\thead = i.pressure + i.base_head + i.elevation\n",
    "\t\t# elevation_head[i.type_1H == 1] = head[i.type_1H == 1]\n",
    "\t\t# elevation = elevation_head - min_elevation\n",
    "\n",
    "\t\t# base_demand = i.base_demand * 1000  # convert to l/s\n",
    "\t\t# graph.x = torch.stack((i.elevation, i.base_demand, i.type_1H*i.base_head), dim=1).float()\n",
    "\t\tgraph.x = torch.stack((i.elevation+i.base_head, i.base_demand, i.type_1H), dim=1).float()\n",
    "\t\t# graph.x = torch.stack((i.elevation+i.base_head, i.base_demand, i.type_1H), dim=1).float()\n",
    "\n",
    "\t\t# Position and ID\n",
    "\t\tgraph.pos = i.pos\n",
    "\t\tgraph.ID = i.ID\n",
    "\n",
    "\t\t# Edge index (Adjacency matrix)\n",
    "\t\tgraph.edge_index = i.edge_index\n",
    "\n",
    "\t\t# Edge attributes\n",
    "\t\tdiameter = i.diameter\n",
    "\t\tlength = i.length\n",
    "\t\troughness = i.roughness\n",
    "\t\tgraph.edge_attr = torch.stack((diameter, length, roughness), dim=1).float()\n",
    "\n",
    "\t\t# pressure = i.pressure\n",
    "\t\t# graph.y = pressure.reshape(-1,1)\n",
    "\n",
    "\t\t# Graph output (head)\n",
    "\t\tif output == 'head':\n",
    "\t\t\tgraph.y  = head[i.type_1H == 0].reshape(-1, 1)\n",
    "\t\telse:\n",
    "\t\t\tgraph.y = i.pressure[i.type_1H == 0].reshape(-1, 1)\n",
    "\t\t\t# pressure[i.type_1H == 1] = 0 # THIS HAS TO BE DONE BETTER\n",
    "\t\t\t# graph.y = pressure\n",
    "\n",
    "\n",
    "\t\t# normalization\n",
    "\t\tif normalizer is not None:\n",
    "\t\t\tgraph = normalizer.transform(graph)\n",
    "\n",
    "\t\tif add_virtual_reservoirs:\n",
    "\n",
    "\t\t\tgraph.x = torch.nn.functional.pad(graph.x, (0, 1))\n",
    "\t\t\tgraph.edge_attr = torch.nn.functional.pad(graph.edge_attr, (0, 1))\n",
    "\t\t\tadd_virtual_nodes(graph)\n",
    "\t\tgraphs.append(graph)\n",
    "\tA12 = nx.incidence_matrix(to_networkx(graphs[0]), oriented=True).toarray().transpose()\n",
    "\treturn graphs, A12\n",
    "\n",
    "def create_dataset_MLP_from_graphs(graphs, features=['nodal_demands', 'base_heads','diameter','roughness','length'],no_res_out=True):\n",
    "\n",
    "\t# index edges to avoid duplicates: this considers all graphs to be UNDIRECTED!\n",
    "\tix_edge = graphs[0].edge_index.numpy().T\n",
    "\tix_edge = (ix_edge[:, 0] < ix_edge[:, 1])\n",
    "\n",
    "\t# position of reservoirs\n",
    "\tix_res = graphs[0].x[:,TYPE_INDEX].numpy()>0\n",
    "\tindices = []\n",
    "\tfor ix_feat, feature in enumerate(features):\n",
    "\t\tfor ix_item, item in enumerate(graphs):\n",
    "\t\t\tif feature == 'diameter':\n",
    "\t\t\t\tx_ = item.edge_attr[ix_edge,DIAMETER_INDEX]\n",
    "\t\t\telif feature == 'roughness':\n",
    "\t\t\t\t# remove reservoirs\n",
    "\t\t\t\tx_ = item.edge_attr[ix_edge,ROUGHNESS_INDEX]\n",
    "\t\t\telif feature == 'length':\n",
    "\t\t\t\t# remove reservoirs\n",
    "\t\t\t\tx_ = item.edge_attr[ix_edge,LENGTH_INDEX]\n",
    "\t\t\telif feature == 'nodal_demands':\n",
    "\t\t\t\t# remove reservoirs\n",
    "\t\t\t\tx_ = item.x[~ix_res,BASEDEMAND_INDEX]\n",
    "\t\t\telif feature == 'base_heads':\n",
    "\t\t\t\tx_ = item.x[ix_res,HEAD_INDEX]\n",
    "\t\t\telse:\n",
    "\t\t\t\traise ValueError(f'Feature {feature} not supported.')\n",
    "\t\t\tif ix_item == 0:\n",
    "\t\t\t\tx = x_\n",
    "\t\t\telse:\n",
    "\t\t\t\tx = torch.cat((x, x_), dim=0)\n",
    "\t\tif ix_feat == 0:\n",
    "\t\t\tX = x.reshape(len(graphs), -1)\n",
    "\t\telse:\n",
    "\t\t\tX = torch.cat((X, x.reshape(len(graphs), -1)), dim=1)\n",
    "\t\tindices.append(X.shape[1])\n",
    "\tfor ix_item, item in enumerate(graphs):\n",
    "\t\t# remove reservoirs from y as well\n",
    "\t\tif ix_item == 0:\n",
    "\t\t\tif no_res_out == True:\n",
    "\t\t\t\ty = item.y\n",
    "\t\t\telse:\n",
    "\t\t\t\ty = item.y[~ix_res]\n",
    "\t\telse:\n",
    "\t\t\tif no_res_out == True:\n",
    "\t\t\t\ty = torch.cat((y, item.y), dim=0)\n",
    "\t\t\telse:\n",
    "\t\t\t\ty = torch.cat((y, item.y[~ix_res]), dim=0)\n",
    "\ty = y.reshape(len(graphs), -1)\n",
    "\n",
    "\treturn torch.utils.data.TensorDataset(X, y), X.shape[1], indices\n",
    "\n",
    "def create_incidence_matrices(graphs,incidence_matrix):\n",
    "\n",
    "\t# position of reservoirs\n",
    "\n",
    "\tix_res = graphs[0].x[:,TYPE_INDEX].numpy()>0\n",
    "\tix_edge = graphs[0].edge_index.numpy().T\n",
    "\tix_edge = (ix_edge[:, 0] < ix_edge[:, 1])\n",
    "\tincidence_matrix = incidence_matrix[ix_edge,:]\n",
    "\tA10 = incidence_matrix[:, ix_res]\n",
    "\tA12 = incidence_matrix[:, ~ix_res]\n",
    "\tA12[np.where(A10 == 1),:] *= -1\n",
    "\tA10[np.where(A10 == 1),:] *= -1\n",
    "\treturn A10, A12"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.base import BaseEstimator,TransformerMixin\n",
    "\n",
    "class PowerLogTransformer(BaseEstimator,TransformerMixin):\n",
    "\tdef __init__(self,log_transform=False,power=4,reverse=True):\n",
    "\t\tif log_transform == True:\n",
    "\t\t\tself.log_transform = log_transform\n",
    "\t\t\tself.power = None\n",
    "\t\telse:\n",
    "\t\t\tself.power = power\n",
    "\t\t\tself.log_transform = None\n",
    "\t\tself.reverse=reverse\n",
    "\t\tself.max_ = None\n",
    "\t\tself.min_ = None\n",
    "\n",
    "\tdef fit(self,X,y=None):\n",
    "\t\tself.max_ = np.max(X)\n",
    "\t\tself.min_ = np.min(X)\n",
    "\t\treturn self\n",
    "\n",
    "\tdef transform(self,X):\n",
    "\t\tif self.log_transform==True:\n",
    "\t\t\tif self.reverse == True:\n",
    "\t\t\t\treturn np.log1p(self.max_-X)\n",
    "\t\t\telse:\n",
    "\t\t\t\treturn np.log1p(X-self.min_)\n",
    "\t\telse:\n",
    "\t\t\tif self.reverse == True:\n",
    "\t\t\t\treturn (self.max_-X)**(1/self.power )\n",
    "\t\t\telse:\n",
    "\t\t\t\treturn (X-self.min_)**(1/self.power )\n",
    "\n",
    "\tdef inverse_transform(self,X):\n",
    "\t\tif self.log_transform==True:\n",
    "\t\t\tif self.reverse == True:\n",
    "\t\t\t\treturn (self.max_ - np.exp(X))\n",
    "\t\t\telse:\n",
    "\t\t\t\treturn (np.exp(X) + self.min_)\n",
    "\t\telse:\n",
    "\t\t\tif self.reverse == True:\n",
    "\t\t\t\treturn (self.max_ - X**self.power )\n",
    "\t\t\telse:\n",
    "\t\t\t\treturn (X**self.power + self.min_)\n",
    "\n",
    "\n",
    "class GraphNormalizer:\n",
    "\tdef __init__(self, x_feat_names=['elevation', 'base_demand', 'base_head'],\n",
    "\t\t\t\t ea_feat_names=['diameter', 'length', 'roughness'], output='pressure'):\n",
    "\t\t# store\n",
    "\t\tself.x_feat_names = x_feat_names\n",
    "\t\tself.ea_feat_names = ea_feat_names\n",
    "\t\tself.output = output\n",
    "\n",
    "\t\t# create separate scaler for each feature (can be improved, e.g., you can fit a scaler for multiple columns)\n",
    "\t\tself.scalers = {}\n",
    "\t\tfor feat in self.x_feat_names:\n",
    "\t\t\tif feat == 'elevation':\n",
    "\t\t\t\tself.scalers[feat] = PowerLogTransformer(log_transform=True, reverse=False)\n",
    "\t\t\telse:\n",
    "\t\t\t\tself.scalers[feat] = MinMaxScaler()\n",
    "\t\tself.scalers[output] = PowerLogTransformer(log_transform=True, reverse=True)\n",
    "\t\tfor feat in self.ea_feat_names:\n",
    "\t\t\tif feat == 'length':\n",
    "\t\t\t\tself.scalers[feat] = PowerLogTransformer(log_transform=True, reverse=False)\n",
    "\t\t\telse:\n",
    "\t\t\t\tself.scalers[feat] = MinMaxScaler()\n",
    "\n",
    "\tdef fit(self, graphs):\n",
    "\t\t''' Fit the scalers on an array of x and ea features\n",
    "        '''\n",
    "\t\tx, y, ea = from_graphs_to_pandas(graphs)\n",
    "\t\tfor ix, feat in enumerate(self.x_feat_names):\n",
    "\t\t\tself.scalers[feat] = self.scalers[feat].fit(x[:, ix].reshape(-1, 1))\n",
    "\t\tself.scalers[self.output] = self.scalers[self.output].fit(y.reshape(-1, 1))\n",
    "\t\tfor ix, feat in enumerate(self.ea_feat_names):\n",
    "\t\t\tself.scalers[feat] = self.scalers[feat].fit(ea[:, ix].reshape(-1, 1))\n",
    "\t\treturn self\n",
    "\n",
    "\tdef transform(self, graph):\n",
    "\t\t''' Transform graph based on normalizer\n",
    "        '''\n",
    "\t\tgraph = graph.clone()\n",
    "\t\tfor ix, feat in enumerate(self.x_feat_names):\n",
    "\t\t\ttemp = graph.x[:, ix].numpy().reshape(-1, 1)\n",
    "\t\t\tgraph.x[:, ix] = torch.tensor(self.scalers[feat].transform(temp).reshape(-1))\n",
    "\t\tfor ix, feat in enumerate(self.ea_feat_names):\n",
    "\t\t\ttemp = graph.edge_attr[:, ix].numpy().reshape(-1, 1)\n",
    "\t\t\tgraph.edge_attr[:, ix] = torch.tensor(self.scalers[feat].transform(temp).reshape(-1))\n",
    "\t\tgraph.y = torch.tensor(self.scalers[self.output].transform(graph.y.numpy().reshape(-1, 1)).reshape(-1))\n",
    "\t\treturn graph\n",
    "\n",
    "\tdef inverse_transform(self, graph):\n",
    "\t\t''' Perform inverse transformation to return original features\n",
    "        '''\n",
    "\t\tgraph = graph.clone()\n",
    "\t\tfor ix, feat in enumerate(self.x_feat_names):\n",
    "\t\t\ttemp = graph.x[:, ix].numpy().reshape(-1, 1)\n",
    "\t\t\tgraph.x[:, ix] = torch.tensor(self.scalers[feat].inverse_transform(temp).reshape(-1))\n",
    "\t\tfor ix, feat in enumerate(self.ea_feat_names):\n",
    "\t\t\ttemp = graph.edge_attr[:, ix].numpy().reshape(-1, 1)\n",
    "\t\t\tgraph.edge_attr[:, ix] = torch.tensor(self.scalers[feat].inverse_transform(temp).reshape(-1))\n",
    "\t\tgraph.y = torch.tensor(self.scalers[self.output].inverse_transform(graph.y.numpy().reshape(-1, 1)).reshape(-1))\n",
    "\t\treturn graph\n",
    "\n",
    "\tdef transform_array(self, z, feat_name):\n",
    "\t\t'''\n",
    "            This is for MLP dataset; it can be done better (the entire thing, from raw data to datasets)\n",
    "        '''\n",
    "\t\treturn torch.tensor(self.scalers[feat_name].transform(z).reshape(-1))\n",
    "\n",
    "\tdef inverse_transform_array(self, z, feat_name):\n",
    "\t\t'''\n",
    "            This is for MLP dataset; it can be done better (the entire thing, from raw data to datasets)\n",
    "        '''\n",
    "\t\treturn torch.tensor(self.scalers[feat_name].inverse_transform(z).reshape(-1))\n",
    "\n",
    "def from_graphs_to_pandas(graphs, l_x=3, l_ea=3):\n",
    "\tx = []\n",
    "\ty = []\n",
    "\tea = []\n",
    "\tfor i, graph in enumerate(graphs):\n",
    "\t\tx.append(graph.x.numpy())\n",
    "\t\ty.append(graph.y.reshape(-1, 1).numpy())\n",
    "\t\tea.append(graph.edge_attr.numpy())\n",
    "\treturn np.concatenate(x, axis=0), np.concatenate(y, axis=0), np.concatenate(ea, axis=0)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alber\\AppData\\Local\\Temp\\ipykernel_4972\\2221086024.py:98: FutureWarning: incidence_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n",
      "  A12 = nx.incidence_matrix(to_networkx(graphs[0]), oriented=True).toarray().transpose()\n",
      "C:\\Users\\alber\\AppData\\Local\\Temp\\ipykernel_4972\\2221086024.py:98: FutureWarning: incidence_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n",
      "  A12 = nx.incidence_matrix(to_networkx(graphs[0]), oriented=True).toarray().transpose()\n",
      "C:\\Users\\alber\\AppData\\Local\\Temp\\ipykernel_4972\\2221086024.py:98: FutureWarning: incidence_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n",
      "  A12 = nx.incidence_matrix(to_networkx(graphs[0]), oriented=True).toarray().transpose()\n",
      "C:\\Users\\alber\\AppData\\Local\\Temp\\ipykernel_4972\\2221086024.py:98: FutureWarning: incidence_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n",
      "  A12 = nx.incidence_matrix(to_networkx(graphs[0]), oriented=True).toarray().transpose()\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "model_name = 'MLP'\n",
    "wdn = 'FOS'\n",
    "PATH = f'./experiments/Results Unrolling/{wdn}/{model_name}/models/1.csv'\n",
    "\n",
    "model = torch.load(PATH)\n",
    "\n",
    "# remove PES anomaly\n",
    "if wdn == 'PES':\n",
    "\tif len(tra_database)>4468:\n",
    "\t\tdel tra_database[4468]\n",
    "\t\tprint('Removed PES anomaly')\n",
    "\t\tprint('Check',tra_database[4468].pressure.mean())\n",
    "\n",
    "# get GRAPH datasets    # later on we should change this and use normal scalers from scikit\n",
    "tra_dataset, A12_bar = create_dataset(tra_database)\n",
    "gn = GraphNormalizer()\n",
    "gn = gn.fit(tra_dataset)\n",
    "tra_dataset, _ = create_dataset(tra_database,normalizer=gn)\n",
    "val_dataset,_ = create_dataset(val_database,normalizer=gn)\n",
    "tst_dataset,_ = create_dataset(tst_database,normalizer=gn)\n",
    "node_size, edge_size = tra_dataset[0].x.size(-1), tra_dataset[0].edge_attr.size(-1)\n",
    "# number of nodes\n",
    "# n_nodes=tra_dataset[0].x.shape[0]\n",
    "n_nodes=(1-tra_database[0].type_1H).numpy().sum() # remove reservoirs\n",
    "# dataloader\n",
    "# transform dataset for MLP\n",
    "# We begin with the MLP versions, when I want to add GNNs, check Riccardo's code\n",
    "A10,A12 = create_incidence_matrices(tra_dataset, A12_bar)\n",
    "tra_dataset_MLP, num_inputs, indices = create_dataset_MLP_from_graphs(tra_dataset)\n",
    "val_dataset_MLP = create_dataset_MLP_from_graphs(val_dataset)[0]\n",
    "tst_dataset_MLP = create_dataset_MLP_from_graphs(tst_dataset)[0]\n",
    "tra_loader = torch.utils.data.DataLoader(tra_dataset_MLP,\n",
    "\t\t\t\t\t\t\t\t\t\t batch_size=256, shuffle=True, pin_memory=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset_MLP,\n",
    "\t\t\t\t\t\t\t\t\t\t batch_size=256, shuffle=False, pin_memory=True)\n",
    "tst_loader = torch.utils.data.DataLoader(tst_dataset_MLP,\n",
    "\t\t\t\t\t\t\t\t\t\t batch_size=256, shuffle=False, pin_memory=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "linear(): argument 'input' (position 1) must be Tensor, not TensorDataset",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [37], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtra_dataset_MLP\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Thesis\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Cell \u001B[1;32mIn [29], line 24\u001B[0m, in \u001B[0;36mMLP.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     22\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[1;32m---> 24\u001B[0m \tx \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     26\u001B[0m \t\u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Thesis\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Thesis\\lib\\site-packages\\torch\\nn\\modules\\container.py:139\u001B[0m, in \u001B[0;36mSequential.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    137\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[0;32m    138\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[1;32m--> 139\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    140\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Thesis\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Thesis\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001B[0m, in \u001B[0;36mLinear.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 114\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mTypeError\u001B[0m: linear(): argument 'input' (position 1) must be Tensor, not TensorDataset"
     ]
    }
   ],
   "source": [
    "model(tra_dataset_MLP)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
