{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33malbert-sola9\u001B[0m. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "wandb version 0.13.10 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.13.7"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>C:\\Code\\main_unrolling\\wandb\\run-20230222_113959-ozgd12a3</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href=\"https://wandb.ai/albert-sola9/Unrolling%20WDNs/runs/ozgd12a3\" target=\"_blank\">treasured-dawn-283</a></strong> to <a href=\"https://wandb.ai/albert-sola9/Unrolling%20WDNs\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/albert-sola9/Unrolling%20WDNs/runs/ozgd12a3?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>",
      "text/plain": "<wandb.sdk.wandb_run.Run at 0x1cab56f4cd0>"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "import wandb\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch_geometric\n",
    "from torch_geometric.utils import to_networkx\n",
    "import torch.nn as nn\n",
    "from torch.nn import Sequential, Linear\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "from utils.miscellaneous import read_config\n",
    "from utils.miscellaneous import create_folder_structure_MLPvsGNN\n",
    "from utils.miscellaneous import initalize_random_generators\n",
    "\n",
    "from training.train import training\n",
    "from training.test import testing\n",
    "\n",
    "from utils.visualization import plot_R2, plot_loss\n",
    "\n",
    "wandb.init(project=\"Unrolling WDNs\", entity=\"albert-sola9\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Parse configuration file + initializations\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating folder: ./experiments/unrolling_WDN0016\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# read config files\n",
    "cfg = read_config(\"config_unrolling.yaml\")\n",
    "# create folder for results\n",
    "exp_name = cfg['exp_name']\n",
    "data_folder = cfg['data_folder']\n",
    "results_folder = create_folder_structure_MLPvsGNN(cfg, parent_folder='./experiments')\n",
    "\n",
    "\n",
    "all_wdn_names = cfg['networks']\n",
    "initalize_random_generators(cfg, count=0)\n",
    "\n",
    "# initialize pytorch device\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# device ='cpu'\n",
    "print(device)\n",
    "#torch.set_num_threads(12)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# TO DO: at the moment I am not using the parsed values for batch size and num_epochs ;\n",
    "# I am not using alpha as well because the loss has no \"smoothness\" penalty (yet)\n",
    "batch_size = cfg['trainParams']['batch_size']\n",
    "alpha = cfg['lossParams']['alpha']\n",
    "res_columns = ['train_loss', 'valid_loss','test_loss', 'r2_train', 'r2_valid',\n",
    "               'r2_test','total_params','total_time','test_time','num_epochs']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Functions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.base import BaseEstimator,TransformerMixin\n",
    "\n",
    "class PowerLogTransformer(BaseEstimator,TransformerMixin):\n",
    "    def __init__(self,log_transform=False,power=4,reverse=True):\n",
    "        if log_transform == True:\n",
    "            self.log_transform = log_transform\n",
    "            self.power = None\n",
    "        else:\n",
    "            self.power = power\n",
    "            self.log_transform = None\n",
    "        self.reverse=reverse\n",
    "        self.max_ = None\n",
    "        self.min_ = None\n",
    "\n",
    "    def fit(self,X,y=None):\n",
    "        self.max_ = np.max(X)\n",
    "        self.min_ = np.min(X)\n",
    "        return self\n",
    "\n",
    "    def transform(self,X):\n",
    "        if self.log_transform==True:\n",
    "            if self.reverse == True:\n",
    "                return np.log1p(self.max_-X)\n",
    "            else:\n",
    "                return np.log1p(X-self.min_)\n",
    "        else:\n",
    "            if self.reverse == True:\n",
    "                return (self.max_-X)**(1/self.power )\n",
    "            else:\n",
    "                return (X-self.min_)**(1/self.power )\n",
    "\n",
    "    def inverse_transform(self,X):\n",
    "        if self.log_transform==True:\n",
    "            if self.reverse == True:\n",
    "                return (self.max_ - np.exp(X))\n",
    "            else:\n",
    "                return (np.exp(X) + self.min_)\n",
    "        else:\n",
    "            if self.reverse == True:\n",
    "                return (self.max_ - X**self.power )\n",
    "            else:\n",
    "                return (X**self.power + self.min_)\n",
    "\n",
    "class GraphNormalizer:\n",
    "    def __init__(self, x_feat_names=['elevation','base_demand','base_head'],\n",
    "                 ea_feat_names=['diameter','length','roughness'], output='pressure'):\n",
    "        # store\n",
    "        self.x_feat_names = x_feat_names\n",
    "        self.ea_feat_names = ea_feat_names\n",
    "        self.output = output\n",
    "\n",
    "        # create separate scaler for each feature (can be improved, e.g., you can fit a scaler for multiple columns)\n",
    "        self.scalers = {}\n",
    "        for feat in self.x_feat_names:\n",
    "            if feat == 'elevation':\n",
    "                self.scalers[feat] = PowerLogTransformer(log_transform=True,reverse=False)\n",
    "            else:\n",
    "                self.scalers[feat] = MinMaxScaler()\n",
    "        self.scalers[output] = PowerLogTransformer(log_transform=True,reverse=True)\n",
    "        for feat in self.ea_feat_names:\n",
    "            if feat == 'length':\n",
    "                self.scalers[feat] = PowerLogTransformer(log_transform=True,reverse=False)\n",
    "            else:\n",
    "                self.scalers[feat] = MinMaxScaler()\n",
    "\n",
    "    def fit(self, graphs):\n",
    "        ''' Fit the scalers on an array of x and ea features\n",
    "        '''\n",
    "        x, y, ea = from_graphs_to_pandas(graphs)\n",
    "        for ix, feat in enumerate(self.x_feat_names):\n",
    "            self.scalers[feat] = self.scalers[feat].fit(x[:,ix].reshape(-1,1))\n",
    "        self.scalers[self.output] = self.scalers[self.output].fit(y.reshape(-1,1))\n",
    "        for ix, feat in enumerate(self.ea_feat_names):\n",
    "            self.scalers[feat] = self.scalers[feat].fit(ea[:,ix].reshape(-1,1))\n",
    "        return self\n",
    "\n",
    "    def transform(self, graph):\n",
    "        ''' Transform graph based on normalizer\n",
    "        '''\n",
    "        graph = graph.clone()\n",
    "        for ix, feat in enumerate(self.x_feat_names):\n",
    "            temp = graph.x[:,ix].numpy().reshape(-1,1)\n",
    "            graph.x[:,ix] = torch.tensor(self.scalers[feat].transform(temp).reshape(-1))\n",
    "        for ix, feat in enumerate(self.ea_feat_names):\n",
    "            temp = graph.edge_attr[:,ix].numpy().reshape(-1,1)\n",
    "            graph.edge_attr[:,ix] = torch.tensor(self.scalers[feat].transform(temp).reshape(-1))\n",
    "        graph.y = torch.tensor(self.scalers[self.output].transform(graph.y.numpy().reshape(-1,1)).reshape(-1))\n",
    "        return graph\n",
    "\n",
    "    def inverse_transform(self, graph):\n",
    "        ''' Perform inverse transformation to return original features\n",
    "        '''\n",
    "        graph = graph.clone()\n",
    "        for ix, feat in enumerate(self.x_feat_names):\n",
    "            temp = graph.x[:,ix].numpy().reshape(-1,1)\n",
    "            graph.x[:,ix] = torch.tensor(self.scalers[feat].inverse_transform(temp).reshape(-1))\n",
    "        for ix, feat in enumerate(self.ea_feat_names):\n",
    "            temp = graph.edge_attr[:,ix].numpy().reshape(-1,1)\n",
    "            graph.edge_attr[:,ix] = torch.tensor(self.scalers[feat].inverse_transform(temp).reshape(-1))\n",
    "        graph.y = torch.tensor(self.scalers[self.output].inverse_transform(graph.y.numpy().reshape(-1,1)).reshape(-1))\n",
    "        return graph\n",
    "\n",
    "    def transform_array(self,z,feat_name):\n",
    "        '''\n",
    "            This is for MLP dataset; it can be done better (the entire thing, from raw data to datasets)\n",
    "        '''\n",
    "        return torch.tensor(self.scalers[feat_name].transform(z).reshape(-1))\n",
    "\n",
    "    def inverse_transform_array(self,z,feat_name):\n",
    "        '''\n",
    "            This is for MLP dataset; it can be done better (the entire thing, from raw data to datasets)\n",
    "        '''\n",
    "        return torch.tensor(self.scalers[feat_name].inverse_transform(z).reshape(-1))\n",
    "\n",
    "def from_graphs_to_pandas(graphs, l_x=3, l_ea=3):\n",
    "    x = []\n",
    "    y = []\n",
    "    ea = []\n",
    "    for i, graph in enumerate(graphs):\n",
    "        x.append(graph.x.numpy())\n",
    "        y.append(graph.y.reshape(-1,1).numpy())\n",
    "        ea.append(graph.edge_attr.numpy())\n",
    "    return np.concatenate(x,axis=0),np.concatenate(y,axis=0),np.concatenate(ea,axis=0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from models.virtual_nodes import add_virtual_nodes\n",
    "\n",
    "# constant indexes for node and edge features\n",
    "ELEVATION_INDEX = 0\n",
    "BASEDEMAND_INDEX = 1\n",
    "BASEHEAD_INDEX = 2\n",
    "DIAMETER_INDEX = 0\n",
    "LENGTH_INDEX = 1\n",
    "ROUGHNESS_INDEX = 2\n",
    "FLOW_INDEX = 3\n",
    "\n",
    "def load_raw_dataset(wdn_name, data_folder):\n",
    "    '''\n",
    "    Load tra/val/data for a water distribution network datasets\n",
    "    -------\n",
    "    wdn_name : string\n",
    "        prefix of pickle files to open\n",
    "    data_folder : string\n",
    "        path to datasets\n",
    "    '''\n",
    "\n",
    "    data_tra = pickle.load(open(f'{data_folder}/train/{wdn_name}.p', \"rb\"))\n",
    "    data_val = pickle.load(open(f'{data_folder}/valid/{wdn_name}.p', \"rb\"))\n",
    "    data_tst = pickle.load(open(f'{data_folder}/test/{wdn_name}.p', \"rb\"))\n",
    "\n",
    "    return data_tra, data_val, data_tst\n",
    "\n",
    "def create_dataset(database, normalizer=None, HW_rough_minmax=[60, 150],add_virtual_reservoirs=False, output='pressure'):\n",
    "    '''\n",
    "    Creates working datasets dataset from the pickle databases\n",
    "    ------\n",
    "    database : list\n",
    "        each element in the list is a pickle file containing Data objects\n",
    "    normalization: dict\n",
    "        normalize the dataset using mean and std\n",
    "    '''\n",
    "    # Roughness info (Hazen-Williams) / TODO: remove the hard_coding\n",
    "    minR = HW_rough_minmax[0]\n",
    "    maxR = HW_rough_minmax[1]\n",
    "\n",
    "    graphs = []\n",
    "\n",
    "    for i in database:\n",
    "        graph = torch_geometric.data.Data()\n",
    "\n",
    "        # Node attributes\n",
    "        # elevation_head = i.elevation + i.base_head\n",
    "        # elevation_head = i.elevation.clone()\n",
    "        # elevation_head[elevation_head == 0] = elevation_head.mean()\n",
    "\n",
    "        min_elevation = min(i.elevation[i.type_1H == 0])\n",
    "        head = i.pressure + i.base_head + i.elevation\n",
    "        # elevation_head[i.type_1H == 1] = head[i.type_1H == 1]\n",
    "        # elevation = elevation_head - min_elevation\n",
    "\n",
    "        # base_demand = i.base_demand * 1000  # convert to l/s\n",
    "        # graph.x = torch.stack((i.elevation, i.base_demand, i.type_1H*i.base_head), dim=1).float()\n",
    "        graph.x = torch.stack((i.elevation+i.base_head, i.base_demand, i.type_1H), dim=1).float()\n",
    "        # graph.x = torch.stack((i.elevation+i.base_head, i.base_demand, i.type_1H), dim=1).float()\n",
    "\n",
    "        # Position and ID\n",
    "        # graph.pos = i.pos\n",
    "        graph.ID = i.ID\n",
    "\n",
    "        # Edge index (Adjacency matrix)\n",
    "        graph.edge_index = i.edge_index\n",
    "\n",
    "        # Edge attributes\n",
    "        diameter = i.diameter\n",
    "        length = i.length\n",
    "        roughness = i.roughness\n",
    "        graph.edge_attr = torch.stack((diameter, length, roughness), dim=1).float()\n",
    "\n",
    "        # pressure = i.pressure\n",
    "        # graph.y = pressure.reshape(-1,1)\n",
    "\n",
    "        # Graph output (head)\n",
    "        if output == 'head':\n",
    "            graph.y  = head[i.type_1H == 0].reshape(-1, 1)\n",
    "        else:\n",
    "            graph.y = i.pressure[i.type_1H == 0].reshape(-1, 1)\n",
    "            # pressure[i.type_1H == 1] = 0 # THIS HAS TO BE DONE BETTER\n",
    "            # graph.y = pressure\n",
    "\n",
    "\n",
    "        # normalization\n",
    "        if normalizer is not None:\n",
    "            graph = normalizer.transform(graph)\n",
    "\n",
    "        if add_virtual_reservoirs:\n",
    "\n",
    "            graph.x = torch.nn.functional.pad(graph.x, (0, 1))\n",
    "            graph.edge_attr = torch.nn.functional.pad(graph.edge_attr, (0, 1))\n",
    "            add_virtual_nodes(graph)\n",
    "        graphs.append(graph)\n",
    "    A12 = nx.incidence_matrix(to_networkx(graphs[0]), oriented=True).toarray().transpose()\n",
    "    return graphs, A12\n",
    "\n",
    "def create_dataset_MLP_from_graphs(graphs, features=['nodal_demands', 'base_heads','diameter','roughness','length'],no_res_out=True):\n",
    "\n",
    "    # index edges to avoid duplicates: this considers all graphs to be UNDIRECTED!\n",
    "    ix_edge = graphs[0].edge_index.numpy().T\n",
    "    ix_edge = (ix_edge[:, 0] < ix_edge[:, 1])\n",
    "\n",
    "    # position of reservoirs\n",
    "    ix_res = graphs[0].x[:,BASEHEAD_INDEX].numpy()>0\n",
    "    indices = []\n",
    "    for ix_feat, feature in enumerate(features):\n",
    "        for ix_item, item in enumerate(graphs):\n",
    "            if feature == 'diameter':\n",
    "                x_ = item.edge_attr[ix_edge,DIAMETER_INDEX]\n",
    "            elif feature == 'roughness':\n",
    "                # remove reservoirs\n",
    "                x_ = item.edge_attr[ix_edge,ROUGHNESS_INDEX]\n",
    "            elif feature == 'length':\n",
    "                # remove reservoirs\n",
    "                x_ = item.edge_attr[ix_edge,LENGTH_INDEX]\n",
    "            elif feature == 'nodal_demands':\n",
    "                # remove reservoirs\n",
    "                x_ = item.x[~ix_res,BASEDEMAND_INDEX]\n",
    "            elif feature == 'base_heads':\n",
    "                x_ = item.x[ix_res,BASEHEAD_INDEX]\n",
    "            else:\n",
    "                raise ValueError(f'Feature {feature} not supported.')\n",
    "            if ix_item == 0:\n",
    "                x = x_\n",
    "            else:\n",
    "                x = torch.cat((x, x_), dim=0)\n",
    "        if ix_feat == 0:\n",
    "            X = x.reshape(len(graphs), -1)\n",
    "        else:\n",
    "            X = torch.cat((X, x.reshape(len(graphs), -1)), dim=1)\n",
    "        indices.append(X.shape[1])\n",
    "    for ix_item, item in enumerate(graphs):\n",
    "        # remove reservoirs from y as well\n",
    "        if ix_item == 0:\n",
    "            if no_res_out == True:\n",
    "                y = item.y\n",
    "            else:\n",
    "                y = item.y[~ix_res]\n",
    "        else:\n",
    "            if no_res_out == True:\n",
    "                y = torch.cat((y, item.y), dim=0)\n",
    "            else:\n",
    "                y = torch.cat((y, item.y[~ix_res]), dim=0)\n",
    "    y = y.reshape(len(graphs), -1)\n",
    "\n",
    "    return torch.utils.data.TensorDataset(X, y), X.shape[1], indices\n",
    "\n",
    "def create_incidence_matrices(graphs,incidence_matrix):\n",
    "\n",
    "    # position of reservoirs\n",
    "\n",
    "    ix_res = graphs[0].x[:,BASEHEAD_INDEX].numpy()>0\n",
    "    ix_edge = graphs[0].edge_index.numpy().T\n",
    "    ix_edge = (ix_edge[:, 0] < ix_edge[:, 1])\n",
    "    incidence_matrix = incidence_matrix[ix_edge,:]\n",
    "    A10 = incidence_matrix[:, ix_res]\n",
    "    A12 = incidence_matrix[:, ~ix_res]\n",
    "    return A10, A12"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Models\n",
    "I will be Creating different models as follows:\n",
    "\n",
    "* A simple MLP\n",
    "* An unrolled version of Heads and Flows\n",
    "* An unrolled version of Heads, Flows and base_demands\n",
    "* An unrolled version of Heads, Flows, Initial Head and base_demands\n",
    "* An unrolled version of Heads, Flows, Initial Head, base_demands and static features\n",
    "* An unrolled version of Heads, Flows, Initial Head, base_demands, static features and calculating a D block"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, num_outputs, hid_channels, indices, num_layers=6):\n",
    "        super(MLP, self).__init__()\n",
    "        torch.manual_seed(42)\n",
    "        self.hid_channels = hid_channels\n",
    "        self.indices = indices\n",
    "        self.num_flows = indices[2] - indices[1]\n",
    "\n",
    "        layers = [Linear(indices[4], hid_channels),\n",
    "                  nn.ReLU()]\n",
    "\n",
    "        for l in range(num_layers-1):\n",
    "            layers += [Linear(hid_channels, hid_channels),\n",
    "                       nn.ReLU()]\n",
    "\n",
    "        layers += [Linear(hid_channels, num_outputs)]\n",
    "\n",
    "        self.main = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.main(x)\n",
    "\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "#Unrolling with flows, heads, H0 and q\n",
    "class UnrollingHFH0q(nn.Module):\n",
    "    def __init__(self, num_outputs, indices, dropout_rate=0):\n",
    "        super(UnrollingHFH0q, self).__init__()\n",
    "        torch.manual_seed(42)\n",
    "        self.indices = indices\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.num_heads = indices[1] - indices[0]\n",
    "        self.num_flows = indices[2]-indices[1]\n",
    "\n",
    "        self.hidQ0_H = Linear(indices[2]-indices[1], self.num_heads)\n",
    "        self.hidH0_Q = Linear(indices[1]-indices[0], self.num_flows)\n",
    "        self.hidH0_H = Linear(indices[1]-indices[0], self.num_heads)\n",
    "        self.hidq_Q =  Linear(indices[0], self.num_flows)\n",
    "\n",
    "        self.hid1 = Sequential(Linear(self.num_flows, self.num_heads),\n",
    "                           nn.ReLU(),\n",
    "                           nn.Dropout(self.dropout_rate))\n",
    "        self.hid2 = Sequential(Linear(self.num_heads,self.num_flows ),\n",
    "                           nn.ReLU(),\n",
    "                           nn.Dropout(self.dropout_rate))\n",
    "        self.hid3 = Sequential(Linear(self.num_flows, self.num_heads),\n",
    "                           nn.ReLU(),\n",
    "                           nn.Dropout(self.dropout_rate))\n",
    "        self.hid4 = Sequential(Linear(self.num_heads, self.num_flows),\n",
    "                           nn.ReLU(),\n",
    "                           nn.Dropout(self.dropout_rate))\n",
    "        self.hid5 = Sequential(Linear(self.num_flows, self.num_heads),\n",
    "                           nn.ReLU(),\n",
    "                           nn.Dropout(self.dropout_rate))\n",
    "        self.hid6 = Sequential(Linear(self.num_heads, self.num_flows),\n",
    "                           nn.ReLU(),\n",
    "                           nn.Dropout(self.dropout_rate))\n",
    "        self.hid7 = Sequential(Linear(self.num_flows,self.num_heads),\n",
    "                           nn.ReLU(),\n",
    "                           nn.Dropout(self.dropout_rate))\n",
    "        self.hid8 = Sequential(Linear(self.num_heads,self.num_flows),\n",
    "                           nn.ReLU(),\n",
    "                           nn.Dropout(self.dropout_rate))\n",
    "        self.hid9 = Sequential(Linear(self.num_flows,self.num_heads),\n",
    "                           nn.ReLU(),\n",
    "                           nn.Dropout(self.dropout_rate))\n",
    "        self.hid10 = Sequential(Linear(self.num_heads,self.num_flows),\n",
    "                           nn.ReLU(),\n",
    "                           nn.Dropout(self.dropout_rate))\n",
    "        self.hid11 = Sequential(Linear(self.num_flows, self.num_heads),\n",
    "                           nn.ReLU(),\n",
    "                           nn.Dropout(self.dropout_rate))\n",
    "        self.hid12 = Sequential(Linear(self.num_heads,self.num_flows),\n",
    "                           nn.ReLU(),\n",
    "                           nn.Dropout(self.dropout_rate))\n",
    "        self.resQ1 = Sequential(Linear(self.num_flows,self.num_heads),\n",
    "                           nn.ReLU(),\n",
    "                           nn.Dropout(self.dropout_rate))\n",
    "        self.resQ2 = Sequential(Linear(self.num_flows,self.num_heads),\n",
    "                           nn.ReLU(),\n",
    "                           nn.Dropout(self.dropout_rate))\n",
    "        self.resQ3 = Sequential(Linear(self.num_flows,self.num_heads),\n",
    "                           nn.ReLU(),\n",
    "                           nn.Dropout(self.dropout_rate))\n",
    "        self.resQ4 = Sequential(Linear(self.num_flows,self.num_heads),\n",
    "                           nn.ReLU(),\n",
    "                           nn.Dropout(self.dropout_rate))\n",
    "        self.resQ5 = Sequential(Linear(self.num_flows,self.num_heads),\n",
    "                           nn.ReLU(),\n",
    "                           nn.Dropout(self.dropout_rate))\n",
    "        self.res_D0 = Sequential(Linear(indices[4]-indices[2], self.num_flows),\n",
    "                           nn.ReLU(),\n",
    "                           nn.Dropout(self.dropout_rate))\n",
    "        self.res_D1 = Sequential(Linear(indices[4]-indices[2], self.num_flows),\n",
    "                           nn.ReLU(),\n",
    "                           nn.Dropout(self.dropout_rate))\n",
    "        self.res_D2 = Sequential(Linear(indices[4]-indices[2], self.num_flows),\n",
    "                           nn.ReLU(),\n",
    "                           nn.Dropout(self.dropout_rate))\n",
    "        self.res_D3 = Sequential(Linear(indices[4]-indices[2], self.num_flows),\n",
    "                           nn.ReLU(),\n",
    "                           nn.Dropout(self.dropout_rate))\n",
    "        self.res_D4 = Sequential(Linear(indices[4]-indices[2], self.num_flows),\n",
    "                           nn.ReLU(),\n",
    "                           nn.Dropout(self.dropout_rate))\n",
    "        self.res_D5 = Sequential(Linear(indices[4]-indices[2], self.num_flows),\n",
    "                           nn.ReLU(),\n",
    "                           nn.Dropout(self.dropout_rate))\n",
    "\n",
    "        self.out = Linear(self.num_flows, num_outputs)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        q, H0, Q0, S = x[:,:self.indices[0]], x[:,self.indices[0]:self.indices[1]], x[:,self.indices[1]:self.indices[2]], x[:,self.indices[2]:]\n",
    "        res_H0_Q, res_q_Q, res_Q0_H, res_H0_H = self.hidH0_Q(H0), self.hidq_Q(q), self.hidQ0_H(Q0), self.hidH0_H(H0)\n",
    "\n",
    "        H1 = self.hid1(Q0)\n",
    "        hid_x = torch.sum(torch.stack([H1,res_H0_H,res_Q0_H]),dim=0)\n",
    "        Q1 = self.hid2(hid_x)\n",
    "        res_Q1_H = self.resQ1(Q1)\n",
    "        hid_x = torch.sum(torch.stack([Q1,res_q_Q,res_H0_Q]),dim=0)\n",
    "\n",
    "        H2 = self.hid3(hid_x)\n",
    "        hid_x = torch.sum(torch.stack([H2,res_H0_H,res_Q1_H]),dim=0)\n",
    "        Q2 = self.hid4(hid_x)\n",
    "        res_Q2_H = self.resQ2(Q2)\n",
    "        hid_x = torch.sum(torch.stack([Q2,res_q_Q,res_H0_Q]),dim=0)\n",
    "\n",
    "        H3 = self.hid5(hid_x)\n",
    "        hid_x = torch.sum(torch.stack([H3,res_H0_H,res_Q2_H]),dim=0)\n",
    "        Q3 = self.hid6(hid_x)\n",
    "        res_Q3_H = self.resQ3(Q3)\n",
    "        hid_x = torch.sum(torch.stack([Q3,res_q_Q,res_H0_Q]),dim=0)\n",
    "\n",
    "        H4 = self.hid7(hid_x)\n",
    "        hid_x = torch.sum(torch.stack([H4,res_H0_H,res_Q3_H]),dim=0)\n",
    "        Q4 = self.hid8(hid_x)\n",
    "        res_Q4_H = self.resQ4(Q4)\n",
    "        hid_x = torch.sum(torch.stack([Q4,res_q_Q,res_H0_Q]),dim=0)\n",
    "\n",
    "        H5 = self.hid9(hid_x)\n",
    "        hid_x = torch.sum(torch.stack([H5,res_H0_H,res_Q4_H]),dim=0)\n",
    "        Q5 = self.hid10(hid_x)\n",
    "        res_Q5_H = self.resQ5(Q5)\n",
    "        hid_x = torch.sum(torch.stack([Q5,res_q_Q,res_H0_Q]),dim=0)\n",
    "\n",
    "        H6 = self.hid11(hid_x)\n",
    "        hid_x = torch.sum(torch.stack([H6,res_H0_H,res_Q5_H]),dim=0)\n",
    "        Q6 = self.hid12(hid_x)\n",
    "        hid_x = torch.sum(torch.stack([Q6,res_q_Q,res_H0_Q]),dim=0)\n",
    "\n",
    "        return self.out(hid_x)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "#Unrolling with flows, heads, H0, q and S\n",
    "class UnrollingD(nn.Module):\n",
    "    def __init__(self, num_outputs, indices, num_blocks):\n",
    "        super(UnrollingD, self).__init__()\n",
    "        torch.manual_seed(42)\n",
    "        self.indices = indices\n",
    "        self.num_blocks = num_blocks\n",
    "        self.num_heads = indices[0]\n",
    "        self.num_flows = indices[2]-indices[1]\n",
    "\n",
    "        self.hidQ0_H = Linear(indices[2]-indices[1], self.num_heads)\n",
    "        self.hidH0_Q = Linear(indices[1]-indices[0], self.num_flows)\n",
    "        self.hidH0_H = Linear(indices[1]-indices[0], self.num_heads)\n",
    "        self.hidq_Q =  Linear(indices[0], self.num_flows)\n",
    "        self.hid_S = Sequential(Linear(indices[4] - indices[2], self.num_flows),\n",
    "                           nn.ReLU())\n",
    "\n",
    "        self.hid_HF = nn.ModuleList()\n",
    "        self.hid_FH = nn.ModuleList()\n",
    "        self.resQ = nn.ModuleList()\n",
    "        self.hidD_Q = nn.ModuleList()\n",
    "        self.hidD_H = nn.ModuleList()\n",
    "\n",
    "        for i in range(num_blocks):\n",
    "            self.hid_HF.append(Sequential(Linear(self.num_heads,self.num_flows),\n",
    "                           nn.ReLU()))\n",
    "            self.hid_FH.append(Sequential(Linear(self.num_flows, self.num_heads),\n",
    "                           nn.ReLU()))\n",
    "            self.resQ.append( Sequential(Linear(self.num_flows,self.num_heads),\n",
    "                           nn.ReLU()))\n",
    "            self.hidD_Q.append(Sequential(Linear(self.num_flows,self.num_flows),\n",
    "                           nn.ReLU()))\n",
    "            self.hidD_H.append(Linear(self.num_flows,self.num_heads))\n",
    "\n",
    "        self.out = Linear(self.num_flows, num_outputs)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        q, H0, Q, S = x[:,:self.indices[0]], x[:,self.indices[0]:self.indices[1]], x[:,self.indices[1]:self.indices[2]], x[:,self.indices[2]:]\n",
    "        res_H0_Q, res_q_Q, res_Q_H, res_H0_H, res_S_Q = self.hidH0_Q(H0), self.hidq_Q(q), self.hidQ0_H(Q), self.hidH0_H(H0), self.hid_S(S)\n",
    "\n",
    "        for i in range(self.num_blocks):\n",
    "            D_Q = self.hidD_Q[i](torch.mul(Q, res_S_Q))\n",
    "            D_H = self.hidD_H[i](D_Q)\n",
    "            hid_x = torch.add(D_Q,Q)\n",
    "            H = self.hid_FH[i](hid_x)\n",
    "            hid_x = torch.sum(torch.stack([H,res_H0_H,res_Q_H, D_H]),dim=0)\n",
    "            Q = self.hid_HF[i](hid_x)\n",
    "            res_Q_H = self.resQ[i](Q)\n",
    "\n",
    "        return self.out(Q)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "#Unrolling with flows, heads, H0, q and S\n",
    "class UnrollingModel(nn.Module):\n",
    "    def __init__(self, num_outputs, indices, num_blocks):\n",
    "        super(UnrollingModel, self).__init__()\n",
    "        torch.manual_seed(42)\n",
    "        self.indices = indices\n",
    "        self.num_heads = indices[0]\n",
    "        self.num_flows = indices[2]-indices[1]\n",
    "        self.num_blocks = num_blocks\n",
    "\n",
    "        self.hidQ0_H = Linear(indices[2]-indices[1], self.num_heads)\n",
    "        self.hidH0_Q = Linear(indices[1]-indices[0], self.num_flows)\n",
    "        self.hidH0_H = Linear(indices[1]-indices[0], self.num_heads)\n",
    "        self.hidq_Q =  Linear(indices[0], self.num_flows)\n",
    "        self.hid_S = Sequential(Linear(indices[4] - indices[2], self.num_flows),\n",
    "                           nn.ReLU())\n",
    "\n",
    "        self.hid_HF = nn.ModuleList()\n",
    "        self.hid_FH = nn.ModuleList()\n",
    "        self.resQ = nn.ModuleList()\n",
    "        self.hidD_Q = nn.ModuleList()\n",
    "        self.hidD_H = nn.ModuleList()\n",
    "\n",
    "        for i in range(num_blocks):\n",
    "            self.hid_HF.append(Sequential(Linear(self.num_heads,self.num_flows), nn.ReLU()))\n",
    "            self.hid_FH.append(Sequential(Linear(self.num_flows, self.num_heads),\n",
    "                           nn.ReLU()))\n",
    "            self.resQ.append(Sequential(Linear(self.num_flows,self.num_heads),\n",
    "                           nn.ReLU()))\n",
    "            self.hidD_Q.append(Sequential(Linear(self.num_flows,self.num_flows),\n",
    "                           nn.ReLU()))\n",
    "            self.hidD_H.append(Linear(self.num_flows,self.num_heads))\n",
    "\n",
    "        self.out = Linear(self.num_flows, num_outputs)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        q, H0, Q, hid_S = x[:,:self.indices[0]], x[:,self.indices[0]:self.indices[1]], x[:,self.indices[1]:self.indices[2]], x[:,self.indices[2]:]\n",
    "        res_H0_Q, res_q_Q, res_Q_H, res_H0_H, res_S_Q = self.hidH0_Q(H0), self.hidq_Q(q), self.hidQ0_H(Q), self.hidH0_H(H0), self.hid_S(hid_S)\n",
    "\n",
    "\n",
    "        for i in range(self.num_blocks):\n",
    "\n",
    "            D_Q = self.hidD_Q[i](torch.mul(Q, res_S_Q))\n",
    "            D_H = self.hidD_H[i](D_Q)\n",
    "            hid_x = torch.mul(D_Q,torch.sum(torch.stack([Q, res_q_Q, res_H0_Q]),dim=0))\n",
    "            H = self.hid_FH[i](hid_x)\n",
    "            hid_x = self.hid_HF[i](torch.mul(torch.sum(torch.stack([H,res_H0_H,res_Q_H]),dim=0), D_H))\n",
    "            Q = torch.sub(Q,hid_x)\n",
    "            res_Q_H = self.resQ[i](Q)\n",
    "\n",
    "        return self.out(Q)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "#Unrolling with flows, heads, H0, q and S\n",
    "class UnrollingFinal(nn.Module):\n",
    "    def __init__(self, num_outputs, indices, A12, A10, num_blocks):\n",
    "\n",
    "        super(UnrollingFinal, self).__init__()\n",
    "        torch.manual_seed(42)\n",
    "        self.indices = indices\n",
    "        self.num_heads = indices[0]\n",
    "        self.num_flows = indices[2]-indices[1]\n",
    "        self.num_blocks = num_blocks\n",
    "\n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        self.A12 = torch.from_numpy(A12).to(self.device)\n",
    "        self.A10 = torch.from_numpy(A10).to(self.device)\n",
    "\n",
    "        self.hidD = nn.ModuleList()\n",
    "        self.hidQ = nn.ModuleList()\n",
    "        self.hidF = nn.ModuleList()\n",
    "\n",
    "        for i in range(self.num_blocks):\n",
    "            self.hidD.append(Sequential(Linear(4,1),nn.ReLU()))\n",
    "            self.hidQ.append(Sequential(Linear(4,1)))\n",
    "            self.hidF.append(Sequential(Linear(self.num_heads, self.num_heads), nn.ReLU()))\n",
    "\n",
    "        self.out = Linear(self.num_heads, num_outputs)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        A12 = self.A12.repeat(x.shape[0],1,1).double()\n",
    "        A21 = torch.transpose(A12,1,2)\n",
    "        A10 = self.A10.repeat(x.shape[0],1,1).double()\n",
    "\n",
    "        q, A10H0, Q, S = torch.unsqueeze(x[:,:self.indices[0]],dim=2), \\\n",
    "                       torch.bmm(A10,torch.unsqueeze(x[:,self.indices[0]:self.indices[1]],dim=2)), \\\n",
    "                       torch.unsqueeze(x[:,self.indices[1]:self.indices[2]],dim=2), \\\n",
    "                       x[:,self.indices[1]:].double().view(-1,self.num_flows,3)\n",
    "\n",
    "        for i in range(self.num_blocks):\n",
    "            D = self.hidD[i](torch.cat((S,Q),2)) + 1e-3\n",
    "            A11_Q = self.hidQ[i](torch.cat((S,Q),2))\n",
    "            F = torch.sum(torch.stack([torch.bmm(A21,Q),q,-torch.bmm(A21,torch.mul(D,A11_Q)),-torch.bmm(A21,torch.mul(D,A10H0))]),dim=0)\n",
    "            H = self.hidF[i](torch.flatten(F,start_dim=1)).view(-1,self.num_heads,1)\n",
    "            hid_Q = torch.mul(D,torch.sum(torch.stack([A11_Q,torch.bmm(A12,H),A10H0]),dim=0))\n",
    "            Q = torch.sub(Q,hid_Q)\n",
    "\n",
    "        return self.out(torch.flatten(H,start_dim=1))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Running experiments"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Working with FOS, network 1 of 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alber\\AppData\\Local\\Temp\\ipykernel_18832\\941387810.py:96: FutureWarning: incidence_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n",
      "  A12 = nx.incidence_matrix(to_networkx(graphs[0]), oriented=True).toarray().transpose()\n",
      "C:\\Users\\alber\\AppData\\Local\\Temp\\ipykernel_18832\\941387810.py:96: FutureWarning: incidence_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n",
      "  A12 = nx.incidence_matrix(to_networkx(graphs[0]), oriented=True).toarray().transpose()\n",
      "C:\\Users\\alber\\AppData\\Local\\Temp\\ipykernel_18832\\941387810.py:96: FutureWarning: incidence_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n",
      "  A12 = nx.incidence_matrix(to_networkx(graphs[0]), oriented=True).toarray().transpose()\n",
      "C:\\Users\\alber\\AppData\\Local\\Temp\\ipykernel_18832\\941387810.py:96: FutureWarning: incidence_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n",
      "  A12 = nx.incidence_matrix(to_networkx(graphs[0]), oriented=True).toarray().transpose()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UnrollingFinal: training combination 1 of 7\t\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 51/211 [00:28<01:28,  1.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UnrollingFinal: training combination 2 of 7\t\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▎      | 71/211 [00:50<01:40,  1.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UnrollingFinal: training combination 3 of 7\t\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 79/211 [01:08<01:54,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UnrollingFinal: training combination 4 of 7\t\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 100/211 [01:32<01:39,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate is divided by  2 to: 0.0005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 141/211 [02:11<01:05,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UnrollingFinal: training combination 5 of 7\t\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 32/211 [00:33<03:10,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UnrollingFinal: training combination 6 of 7\t\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 12/211 [00:15<04:13,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UnrollingFinal: training combination 7 of 7\t\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 100/211 [02:13<02:23,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate is divided by  2 to: 0.0005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▍| 200/211 [04:28<00:14,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate is divided by  2 to: 0.00025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 211/211 [04:42<00:00,  1.34s/it]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Axis limits cannot be NaN or Inf",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [17], line 79\u001B[0m\n\u001B[0;32m     74\u001B[0m model, tra_losses, val_losses, elapsed_time, epochs \u001B[38;5;241m=\u001B[39m training(model, optimizer, tra_loader, val_loader,\n\u001B[0;32m     75\u001B[0m                                                         patience\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m, report_freq\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m, n_epochs\u001B[38;5;241m=\u001B[39mn_epochs,\n\u001B[0;32m     76\u001B[0m                                                        alpha\u001B[38;5;241m=\u001B[39malpha, lr_rate\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m, lr_epoch\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m100\u001B[39m,\n\u001B[0;32m     77\u001B[0m                                                        normalization\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresults_folder\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mwdn\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00malgorithm\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     78\u001B[0m plot_loss(tra_losses,val_losses,\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresults_folder\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mwdn\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00malgorithm\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/loss/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m---> 79\u001B[0m \u001B[43mplot_R2\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43mval_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mresults_folder\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m/\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mwdn\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m/\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43malgorithm\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m/R2/\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mi\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     80\u001B[0m \u001B[38;5;66;03m# store training history and model\u001B[39;00m\n\u001B[0;32m     81\u001B[0m pd\u001B[38;5;241m.\u001B[39mDataFrame(data \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray([tra_losses, val_losses])\u001B[38;5;241m.\u001B[39mT)\u001B[38;5;241m.\u001B[39mto_csv(\n\u001B[0;32m     82\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresults_folder\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mwdn\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00malgorithm\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/hist/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.csv\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[1;32mC:\\Code\\main_unrolling\\utils\\visualization.py:78\u001B[0m, in \u001B[0;36mplot_R2\u001B[1;34m(model, loader, name, show, normalization)\u001B[0m\n\u001B[0;32m     76\u001B[0m plt\u001B[38;5;241m.\u001B[39mscatter(pred, real, alpha\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.01\u001B[39m)\n\u001B[0;32m     77\u001B[0m plt\u001B[38;5;241m.\u001B[39mplot([MIN, MAX], [MIN, MAX], \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mk-\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m---> 78\u001B[0m \u001B[43mplt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mxlim\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mMAX\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     79\u001B[0m plt\u001B[38;5;241m.\u001B[39mylim([\u001B[38;5;241m0\u001B[39m, MAX])\n\u001B[0;32m     80\u001B[0m plt\u001B[38;5;241m.\u001B[39mtitle(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPrediction vs Real\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Thesis\\lib\\site-packages\\matplotlib\\pyplot.py:1718\u001B[0m, in \u001B[0;36mxlim\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m   1716\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m args \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m kwargs:\n\u001B[0;32m   1717\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m ax\u001B[38;5;241m.\u001B[39mget_xlim()\n\u001B[1;32m-> 1718\u001B[0m ret \u001B[38;5;241m=\u001B[39m ax\u001B[38;5;241m.\u001B[39mset_xlim(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1719\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m ret\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Thesis\\lib\\site-packages\\matplotlib\\axes\\_base.py:3698\u001B[0m, in \u001B[0;36m_AxesBase.set_xlim\u001B[1;34m(self, left, right, emit, auto, xmin, xmax)\u001B[0m\n\u001B[0;32m   3696\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_process_unit_info([(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mx\u001B[39m\u001B[38;5;124m\"\u001B[39m, (left, right))], convert\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m   3697\u001B[0m left \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_validate_converted_limits(left, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconvert_xunits)\n\u001B[1;32m-> 3698\u001B[0m right \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_validate_converted_limits\u001B[49m\u001B[43m(\u001B[49m\u001B[43mright\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconvert_xunits\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3700\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m left \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m right \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   3701\u001B[0m     \u001B[38;5;66;03m# Axes init calls set_xlim(0, 1) before get_xlim() can be called,\u001B[39;00m\n\u001B[0;32m   3702\u001B[0m     \u001B[38;5;66;03m# so only grab the limits if we really need them.\u001B[39;00m\n\u001B[0;32m   3703\u001B[0m     old_left, old_right \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_xlim()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Thesis\\lib\\site-packages\\matplotlib\\axes\\_base.py:3614\u001B[0m, in \u001B[0;36m_AxesBase._validate_converted_limits\u001B[1;34m(self, limit, convert)\u001B[0m\n\u001B[0;32m   3611\u001B[0m converted_limit \u001B[38;5;241m=\u001B[39m convert(limit)\n\u001B[0;32m   3612\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\u001B[38;5;28misinstance\u001B[39m(converted_limit, Real)\n\u001B[0;32m   3613\u001B[0m         \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m np\u001B[38;5;241m.\u001B[39misfinite(converted_limit)):\n\u001B[1;32m-> 3614\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAxis limits cannot be NaN or Inf\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   3615\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m converted_limit\n",
      "\u001B[1;31mValueError\u001B[0m: Axis limits cannot be NaN or Inf"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for ix_wdn, wdn in enumerate(all_wdn_names):\n",
    "    print(f'\\nWorking with {wdn}, network {ix_wdn+1} of {len(all_wdn_names)}')\n",
    "\n",
    "    # retrieve wntr data\n",
    "    tra_database, val_database, tst_database = load_raw_dataset(wdn, data_folder)\n",
    "    # reduce training data\n",
    "    # tra_database = tra_database[:int(len(tra_database)*cfg['tra_prc'])]\n",
    "    if cfg['tra_num'] < len(tra_database):\n",
    "        tra_database = tra_database[:cfg['tra_num']]\n",
    "\n",
    "    # remove PES anomaly\n",
    "    if wdn == 'PES':\n",
    "        if len(tra_database)>4468:\n",
    "            del tra_database[4468]\n",
    "            print('Removed PES anomaly')\n",
    "            print('Check',tra_database[4468].pressure.mean())\n",
    "\n",
    "    # get GRAPH datasets    # later on we should change this and use normal scalers from scikit\n",
    "    tra_dataset, A12_bar = create_dataset(tra_database)\n",
    "    gn = GraphNormalizer()\n",
    "    gn = gn.fit(tra_dataset)\n",
    "    tra_dataset, _ = create_dataset(tra_database,normalizer=gn)\n",
    "    val_dataset,_ = create_dataset(val_database,normalizer=gn)\n",
    "    tst_dataset,_ = create_dataset(tst_database,normalizer=gn)\n",
    "    node_size, edge_size = tra_dataset[0].x.size(-1), tra_dataset[0].edge_attr.size(-1)\n",
    "    # number of nodes\n",
    "    # n_nodes=tra_dataset[0].x.shape[0]\n",
    "    n_nodes=(1-tra_database[0].type_1H).numpy().sum() # remove reservoirs\n",
    "    # dataloader\n",
    "    # transform dataset for MLP\n",
    "    # We begin with the MLP versions, when I want to add GNNs, check Riccardo's code\n",
    "    A10,A12 = create_incidence_matrices(tra_dataset, A12_bar)\n",
    "    tra_dataset_MLP, num_inputs, indices = create_dataset_MLP_from_graphs(tra_dataset)\n",
    "    val_dataset_MLP = create_dataset_MLP_from_graphs(val_dataset)[0]\n",
    "    tst_dataset_MLP = create_dataset_MLP_from_graphs(tst_dataset)[0]\n",
    "    tra_loader = torch.utils.data.DataLoader(tra_dataset_MLP,\n",
    "                                             batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset_MLP,\n",
    "                                             batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "    tst_loader = torch.utils.data.DataLoader(tst_dataset_MLP,\n",
    "                                             batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "    # loop through different algorithms\n",
    "    n_epochs = num_inputs\n",
    "    for algorithm in cfg['algorithms']:\n",
    "\n",
    "        hyperParams = cfg['hyperParams'][algorithm]\n",
    "        all_combinations = ParameterGrid(hyperParams)\n",
    "\n",
    "        # create results dataframe\n",
    "        results_df = pd.DataFrame(list(all_combinations))\n",
    "        results_df = pd.concat([results_df,\n",
    "                                pd.DataFrame(index=np.arange(len(all_combinations)),\n",
    "                                          columns=list(res_columns))],axis=1)\n",
    "\n",
    "        for i, combination in enumerate(all_combinations):\n",
    "            print(f'{algorithm}: training combination {i+1} of {len(all_combinations)}\\t',end='\\r',)\n",
    "            combination['indices'] = indices\n",
    "            combination['num_outputs'] = n_nodes\n",
    "            if algorithm == \"UnrollingFinal\":\n",
    "                combination['A12'] = A12\n",
    "                combination['A10'] = A10\n",
    "\n",
    "            wandb.config = combination\n",
    "\n",
    "            # model creation\n",
    "            model = getattr(sys.modules[__name__], algorithm)(**combination).double().to(device)\n",
    "            # print(model)\n",
    "            total_parameters = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "            # model optimizer\n",
    "            optimizer = optim.Adam(params=model.parameters(), **cfg['adamParams'])\n",
    "\n",
    "            # training\n",
    "            model, tra_losses, val_losses, elapsed_time, epochs = training(model, optimizer, tra_loader, val_loader,\n",
    "                                                                    patience=10, report_freq=0, n_epochs=n_epochs,\n",
    "                                                                   alpha=alpha, lr_rate=2, lr_epoch=100,\n",
    "                                                                   normalization=None, path = f'{results_folder}/{wdn}/{algorithm}/')\n",
    "            plot_loss(tra_losses,val_losses,f'{results_folder}/{wdn}/{algorithm}/loss/{i}')\n",
    "            plot_R2(model,val_loader,f'{results_folder}/{wdn}/{algorithm}/R2/{i}')\n",
    "            # store training history and model\n",
    "            pd.DataFrame(data = np.array([tra_losses, val_losses]).T).to_csv(\n",
    "                f'{results_folder}/{wdn}/{algorithm}/hist/{i}.csv')\n",
    "            torch.save(model, f'{results_folder}/{wdn}/{algorithm}/models/{i}.csv')\n",
    "\n",
    "            # compute and store predictions, compute r2 scores\n",
    "            losses = {}\n",
    "            r2_scores = {}\n",
    "            for split, loader in zip(['training','validation','testing'],[tra_loader,val_loader,tst_loader]):\n",
    "                losses[split], pred, real, test_time = testing(model, loader)\n",
    "                r2_scores[split] = r2_score(real, pred)\n",
    "                if i == 0:\n",
    "                    pd.DataFrame(data=real.reshape(-1,n_nodes)).to_csv(\n",
    "                        f'{results_folder}/{wdn}/{algorithm}/pred/{split}/real.csv') # save real obs\n",
    "                pd.DataFrame(data=pred.reshape(-1,n_nodes)).to_csv(\n",
    "                    f'{results_folder}/{wdn}/{algorithm}/pred/{split}/{i}.csv')\n",
    "\n",
    "            # store results\n",
    "            results_df.loc[i,res_columns] = (losses['training'], losses['validation'], losses['testing'],\n",
    "                                             r2_scores['training'], r2_scores['validation'], r2_scores['testing'],\n",
    "                                             total_parameters, elapsed_time,test_time, epochs)\n",
    "        # # save graph normalizer\n",
    "        # with open(f'{results_folder}/{wdn}/{algorithm}/gn.pickle', 'wb') as handle:\n",
    "        #     pickle.dump(gn, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        results_df.to_csv(f'{results_folder}/{wdn}/{algorithm}/results_{algorithm}.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
