{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "import wandb\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch_geometric\n",
    "from torch_geometric.utils import to_networkx\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as f\n",
    "from torch_geometric.nn import GCNConv, Sequential, TAGConv\n",
    "from torch_geometric.utils import subgraph\n",
    "import networkx as nx\n",
    "\n",
    "from utils.miscellaneous import read_config\n",
    "from utils.miscellaneous import create_folder_structure_MLPvsGNN\n",
    "from utils.miscellaneous import initalize_random_generators\n",
    "\n",
    "from training.train import training\n",
    "from training.test import testing\n",
    "\n",
    "from utils.visualization import plot_R2, plot_loss\n",
    "\n",
    "#wandb.init(project=\"Unrolling WDNs\", entity=\"albert-sola9\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Parse configuration file + initializations\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating folder: ./experiments/unrolling_GNN_WDN0017\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# read config files\n",
    "cfg = read_config(\"config_unrolling_GNN.yaml\")\n",
    "# create folder for results\n",
    "exp_name = cfg['exp_name']\n",
    "data_folder = cfg['data_folder']\n",
    "results_folder = create_folder_structure_MLPvsGNN(cfg, parent_folder='./experiments')\n",
    "\n",
    "\n",
    "all_wdn_names = cfg['networks']\n",
    "initalize_random_generators(cfg, count=0)\n",
    "\n",
    "# initialize pytorch device\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "#torch.set_num_threads(12)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# TO DO: at the moment I am not using the parsed values for batch size and num_epochs ;\n",
    "# I am not using alpha as well because the loss has no \"smoothness\" penalty (yet)\n",
    "batch_size = cfg['trainParams']['batch_size']\n",
    "alpha = cfg['lossParams']['alpha']\n",
    "res_columns = ['train_loss', 'valid_loss','test_loss','max_train_loss', 'max_valid_loss','max_test_loss', 'min_train_loss', 'min_valid_loss','min_test_loss','r2_train', 'r2_valid',\n",
    "\t\t\t   'r2_test','total_params','total_time','test_time','num_epochs']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Functions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.base import BaseEstimator,TransformerMixin\n",
    "\n",
    "class PowerLogTransformer(BaseEstimator,TransformerMixin):\n",
    "\tdef __init__(self,log_transform=False,power=4,reverse=True):\n",
    "\t\tif log_transform == True:\n",
    "\t\t\tself.log_transform = log_transform\n",
    "\t\t\tself.power = None\n",
    "\t\telse:\n",
    "\t\t\tself.power = power\n",
    "\t\t\tself.log_transform = None\n",
    "\t\tself.reverse=reverse\n",
    "\t\tself.max_ = None\n",
    "\t\tself.min_ = None\n",
    "\n",
    "\tdef fit(self,X,y=None):\n",
    "\t\tself.max_ = np.max(X)\n",
    "\t\tself.min_ = np.min(X)\n",
    "\t\treturn self\n",
    "\n",
    "\tdef transform(self,X):\n",
    "\t\tif self.log_transform==True:\n",
    "\t\t\tif self.reverse == True:\n",
    "\t\t\t\treturn np.log1p(self.max_-X)\n",
    "\t\t\telse:\n",
    "\t\t\t\treturn np.log1p(X-self.min_)\n",
    "\t\telse:\n",
    "\t\t\tif self.reverse == True:\n",
    "\t\t\t\treturn (self.max_-X)**(1/self.power )\n",
    "\t\t\telse:\n",
    "\t\t\t\treturn (X-self.min_)**(1/self.power )\n",
    "\n",
    "\tdef inverse_transform(self,X):\n",
    "\t\tif self.log_transform==True:\n",
    "\t\t\tif self.reverse == True:\n",
    "\t\t\t\treturn (self.max_ - np.exp(X))\n",
    "\t\t\telse:\n",
    "\t\t\t\treturn (np.exp(X) + self.min_)\n",
    "\t\telse:\n",
    "\t\t\tif self.reverse == True:\n",
    "\t\t\t\treturn (self.max_ - X**self.power )\n",
    "\t\t\telse:\n",
    "\t\t\t\treturn (X**self.power + self.min_)\n",
    "\n",
    "class GraphNormalizer:\n",
    "\tdef __init__(self, x_feat_names=['elevation','base_demand','base_head'],\n",
    "\t\t\t\t ea_feat_names=['diameter','length','roughness'], output='pressure'):\n",
    "\t\t# store\n",
    "\t\tself.x_feat_names = x_feat_names\n",
    "\t\tself.ea_feat_names = ea_feat_names\n",
    "\t\tself.output = output\n",
    "\n",
    "\t\t# create separate scaler for each feature (can be improved, e.g., you can fit a scaler for multiple columns)\n",
    "\t\tself.scalers = {}\n",
    "\t\tfor feat in self.x_feat_names:\n",
    "\t\t\tif feat == 'elevation':\n",
    "\t\t\t\tself.scalers[feat] = PowerLogTransformer(log_transform=True,reverse=False)\n",
    "\t\t\telse:\n",
    "\t\t\t\tself.scalers[feat] = MinMaxScaler()\n",
    "\t\tself.scalers[output] = PowerLogTransformer(log_transform=True,reverse=True)\n",
    "\t\tfor feat in self.ea_feat_names:\n",
    "\t\t\tif feat == 'length':\n",
    "\t\t\t\tself.scalers[feat] = PowerLogTransformer(log_transform=True,reverse=False)\n",
    "\t\t\telse:\n",
    "\t\t\t\tself.scalers[feat] = MinMaxScaler()\n",
    "\n",
    "\tdef fit(self, graphs):\n",
    "\t\t''' Fit the scalers on an array of x and ea features\n",
    "\t\t'''\n",
    "\t\tx, y, ea = from_graphs_to_pandas(graphs)\n",
    "\t\tfor ix, feat in enumerate(self.x_feat_names):\n",
    "\t\t\tself.scalers[feat] = self.scalers[feat].fit(x[:,ix].reshape(-1,1))\n",
    "\t\tself.scalers[self.output] = self.scalers[self.output].fit(y.reshape(-1,1))\n",
    "\t\tfor ix, feat in enumerate(self.ea_feat_names):\n",
    "\t\t\tself.scalers[feat] = self.scalers[feat].fit(ea[:,ix].reshape(-1,1))\n",
    "\t\treturn self\n",
    "\n",
    "\tdef transform(self, graph):\n",
    "\t\t''' Transform graph based on normalizer\n",
    "\t\t'''\n",
    "\t\tgraph = graph.clone()\n",
    "\t\tfor ix, feat in enumerate(self.x_feat_names):\n",
    "\t\t\ttemp = graph.x[:,ix].numpy().reshape(-1,1)\n",
    "\t\t\tgraph.x[:,ix] = torch.tensor(self.scalers[feat].transform(temp).reshape(-1))\n",
    "\t\tfor ix, feat in enumerate(self.ea_feat_names):\n",
    "\t\t\ttemp = graph.edge_attr[:,ix].numpy().reshape(-1,1)\n",
    "\t\t\tgraph.edge_attr[:,ix] = torch.tensor(self.scalers[feat].transform(temp).reshape(-1))\n",
    "\t\tgraph.y = torch.tensor(self.scalers[self.output].transform(graph.y.numpy().reshape(-1,1)).reshape(-1))\n",
    "\t\treturn graph\n",
    "\n",
    "\tdef inverse_transform(self, graph):\n",
    "\t\t''' Perform inverse transformation to return original features\n",
    "\t\t'''\n",
    "\t\tgraph = graph.clone()\n",
    "\t\tfor ix, feat in enumerate(self.x_feat_names):\n",
    "\t\t\ttemp = graph.x[:,ix].numpy().reshape(-1,1)\n",
    "\t\t\tgraph.x[:,ix] = torch.tensor(self.scalers[feat].inverse_transform(temp).reshape(-1))\n",
    "\t\tfor ix, feat in enumerate(self.ea_feat_names):\n",
    "\t\t\ttemp = graph.edge_attr[:,ix].numpy().reshape(-1,1)\n",
    "\t\t\tgraph.edge_attr[:,ix] = torch.tensor(self.scalers[feat].inverse_transform(temp).reshape(-1))\n",
    "\t\tgraph.y = torch.tensor(self.scalers[self.output].inverse_transform(graph.y.numpy().reshape(-1,1)).reshape(-1))\n",
    "\t\treturn graph\n",
    "\n",
    "\tdef transform_array(self,z,feat_name):\n",
    "\t\t'''\n",
    "\t\t\tThis is for MLP dataset; it can be done better (the entire thing, from raw data to datasets)\n",
    "\t\t'''\n",
    "\t\treturn self.scalers[feat_name].transform(z).reshape(-1)\n",
    "\n",
    "\tdef inverse_transform_array(self,z,feat_name):\n",
    "\t\t'''\n",
    "\t\t\tThis is for MLP dataset; it can be done better (the entire thing, from raw data to datasets)\n",
    "\t\t'''\n",
    "\t\treturn self.scalers[feat_name].inverse_transform(z)\n",
    "\n",
    "def from_graphs_to_pandas(graphs, l_x=3, l_ea=3):\n",
    "\tx = []\n",
    "\ty = []\n",
    "\tea = []\n",
    "\tfor i, graph in enumerate(graphs):\n",
    "\t\tx.append(graph.x.numpy())\n",
    "\t\ty.append(graph.y.reshape(-1,1).numpy())\n",
    "\t\tea.append(graph.edge_attr.numpy())\n",
    "\treturn np.concatenate(x,axis=0),np.concatenate(y,axis=0),np.concatenate(ea,axis=0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# constant indexes for node and edge features\n",
    "HEAD_INDEX = 0\n",
    "BASEDEMAND_INDEX = 1\n",
    "TYPE_INDEX = 2\n",
    "ELEVATION_INDEX = 3\n",
    "DIAMETER_INDEX = 0\n",
    "LENGTH_INDEX = 1\n",
    "ROUGHNESS_INDEX = 2\n",
    "FLOW_INDEX = 3\n",
    "\n",
    "def load_raw_dataset(wdn_name, data_folder):\n",
    "\t'''\n",
    "\tLoad tra/val/data for a water distribution network datasets\n",
    "\t-------\n",
    "\twdn_name : string\n",
    "\t\tprefix of pickle files to open\n",
    "\tdata_folder : string\n",
    "\t\tpath to datasets\n",
    "\t'''\n",
    "\n",
    "\tdata_tra = pickle.load(open(f'{data_folder}/train/{wdn_name}.p', \"rb\"))\n",
    "\tdata_val = pickle.load(open(f'{data_folder}/valid/{wdn_name}.p', \"rb\"))\n",
    "\tdata_tst = pickle.load(open(f'{data_folder}/test/{wdn_name}.p', \"rb\"))\n",
    "\n",
    "\treturn data_tra, data_val, data_tst\n",
    "\n",
    "def create_dataset(database, normalizer=None, HW_rough_minmax=[60, 150],add_virtual_reservoirs=False, output='pressure'):\n",
    "\t'''\n",
    "\tCreates working datasets dataset from the pickle databases\n",
    "\t------\n",
    "\tdatabase : list\n",
    "\t\teach element in the list is a pickle file containing Data objects\n",
    "\tnormalization: dict\n",
    "\t\tnormalize the dataset using mean and std\n",
    "\t'''\n",
    "\t# Roughness info (Hazen-Williams) / TODO: remove the hard_coding\n",
    "\tminR = HW_rough_minmax[0]\n",
    "\tmaxR = HW_rough_minmax[1]\n",
    "\n",
    "\tgraphs = []\n",
    "\n",
    "\tfor i in database:\n",
    "\t\tgraph = torch_geometric.data.Data()\n",
    "\n",
    "\t\t# Node attributes\n",
    "\t\t# elevation_head = i.elevation + i.base_head\n",
    "\t\t# elevation_head = i.elevation.clone()\n",
    "\t\t# elevation_head[elevation_head == 0] = elevation_head.mean()\n",
    "\n",
    "\t\tmin_elevation = min(i.elevation[i.type_1H == 0])\n",
    "\t\thead = i.pressure + i.base_head + i.elevation\n",
    "\t\t# elevation_head[i.type_1H == 1] = head[i.type_1H == 1]\n",
    "\t\t# elevation = elevation_head - min_elevation\n",
    "\n",
    "\t\t# base_demand = i.base_demand * 1000  # convert to l/s\n",
    "\t\t# graph.x = torch.stack((i.elevation, i.base_demand, i.type_1H*i.base_head), dim=1).float()\n",
    "\t\tgraph.x = torch.stack((i.elevation+i.base_head, i.base_demand, i.type_1H, i.elevation), dim=1).float()\n",
    "\t\t# graph.x = torch.stack((i.elevation+i.base_head, i.base_demand, i.type_1H), dim=1).float()\n",
    "\n",
    "\t\t# Position and ID\n",
    "\t\tgraph.pos = i.pos\n",
    "\t\tgraph.ID = i.ID\n",
    "\t\tedge_index_mask = [True if i.edge_index[:,j][0].item() < i.edge_index[:,j][1].item() else False for j in range(len(i.edge_index[0]))]\n",
    "\t\t# Edge index (Adjacency matrix)\n",
    "\t\tgraph.edge_index = i.edge_index[:,edge_index_mask]\n",
    "\t\t# Edge attributes\n",
    "\t\tdiameter = i.diameter[edge_index_mask]\n",
    "\t\tlength = i.length[edge_index_mask]\n",
    "\t\troughness = i.roughness[edge_index_mask]\n",
    "\t\tgraph.edge_attr = torch.stack((diameter, length, roughness), dim=1).float()\n",
    "\n",
    "\t\t# pressure = i.pressure\n",
    "\t\t# graph.y = pressure.reshape(-1,1)\n",
    "\n",
    "\t\t# Graph output (head)\n",
    "\t\tif output == 'head':\n",
    "\t\t\tgraph.y  = head[i.type_1H == 0].reshape(-1, 1)\n",
    "\t\telse:\n",
    "\t\t\tgraph.y = i.pressure[i.type_1H == 0].reshape(-1, 1)\n",
    "\t\t\t# pressure[i.type_1H == 1] = 0 # THIS HAS TO BE DONE BETTER\n",
    "\t\t\t# graph.y = pressure\n",
    "\n",
    "\t\t# normalization\n",
    "\t\tif normalizer is not None:\n",
    "\t\t\tgraph = normalizer.transform(graph)\n",
    "\n",
    "\t\tgraphs.append(graph)\n",
    "\treturn graphs, nx.incidence_matrix(to_networkx(graphs[0]), oriented=True).toarray().transpose()\n",
    "\n",
    "def create_incidence_matrices(graphs,incidence_matrix):\n",
    "\n",
    "\t# position of reservoirs\n",
    "\n",
    "\tix_res = graphs[0].x[:,TYPE_INDEX].numpy()>0\n",
    "\tA10 = incidence_matrix[:, ix_res]\n",
    "\tA12 = incidence_matrix[:, ~ix_res]\n",
    "\tA12[np.where(A10 == 1)[0],:] *= -1\n",
    "\tA10[np.where(A10 == 1)[0],:] *= -1\n",
    "\treturn A10, A12"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def create_dataset_MLP_from_graphs(graphs, features=['nodal_demands', 'base_heads','diameter','roughness','length'],no_res_out=True):\n",
    "\n",
    "\t# index edges to avoid duplicates: this considers all graphs to be UNDIRECTED!\n",
    "\tix_edge = graphs[0].edge_index.numpy().T\n",
    "\tix_edge = (ix_edge[:, 0] < ix_edge[:, 1])\n",
    "\n",
    "\t# position of reservoirs\n",
    "\tix_res = graphs[0].x[:,TYPE_INDEX].numpy()>0\n",
    "\tindices = []\n",
    "\tfor ix_feat, feature in enumerate(features):\n",
    "\t\tfor ix_item, item in enumerate(graphs):\n",
    "\t\t\tif feature == 'diameter':\n",
    "\t\t\t\tx_ = item.edge_attr[ix_edge,DIAMETER_INDEX]\n",
    "\t\t\telif feature == 'roughness':\n",
    "\t\t\t\t# remove reservoirs\n",
    "\t\t\t\tx_ = item.edge_attr[ix_edge,ROUGHNESS_INDEX]\n",
    "\t\t\telif feature == 'length':\n",
    "\t\t\t\t# remove reservoirs\n",
    "\t\t\t\tx_ = item.edge_attr[ix_edge,LENGTH_INDEX]\n",
    "\t\t\telif feature == 'nodal_demands':\n",
    "\t\t\t\t# remove reservoirs\n",
    "\t\t\t\tx_ = item.x[~ix_res,BASEDEMAND_INDEX]\n",
    "\t\t\telif feature == 'base_heads':\n",
    "\t\t\t\tx_ = item.x[ix_res,HEAD_INDEX]\n",
    "\t\t\telse:\n",
    "\t\t\t\traise ValueError(f'Feature {feature} not supported.')\n",
    "\t\t\tif ix_item == 0:\n",
    "\t\t\t\tx = x_\n",
    "\t\t\telse:\n",
    "\t\t\t\tx = torch.cat((x, x_), dim=0)\n",
    "\t\tif ix_feat == 0:\n",
    "\t\t\tX = x.reshape(len(graphs), -1)\n",
    "\t\telse:\n",
    "\t\t\tX = torch.cat((X, x.reshape(len(graphs), -1)), dim=1)\n",
    "\t\tindices.append(X.shape[1])\n",
    "\tfor ix_item, item in enumerate(graphs):\n",
    "\t\t# remove reservoirs from y as well\n",
    "\t\tif ix_item == 0:\n",
    "\t\t\tif no_res_out == True:\n",
    "\t\t\t\ty = item.y\n",
    "\t\t\telse:\n",
    "\t\t\t\ty = item.y[~ix_res]\n",
    "\t\telse:\n",
    "\t\t\tif no_res_out == True:\n",
    "\t\t\t\ty = torch.cat((y, item.y), dim=0)\n",
    "\t\t\telse:\n",
    "\t\t\t\ty = torch.cat((y, item.y[~ix_res]), dim=0)\n",
    "\ty = y.reshape(len(graphs), -1)\n",
    "\n",
    "\treturn torch.utils.data.TensorDataset(X, y), X.shape[1], indices"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "class BaselineEPANET(nn.Module):\n",
    "\tdef __init__(self, A12, A10, num_blocks):\n",
    "\n",
    "\t\tsuper(BaselineEPANET, self).__init__()\n",
    "\t\ttorch.manual_seed(42)\n",
    "\t\tself.num_blocks = num_blocks\n",
    "\t\tself.n = 1.852\n",
    "\n",
    "\t\tself.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\t\tself.A12 = torch.from_numpy(A12).to(self.device)\n",
    "\t\tself.num_heads = self.A12.shape[1]\n",
    "\t\tself.num_flows = self.A12.shape[0]\n",
    "\t\tself.A10 = torch.from_numpy(A10).to(self.device)\n",
    "\n",
    "\tdef compute_A11(self,r,q):\n",
    "\t\treturn torch.diag_embed(torch.mul(r,torch.pow(torch.abs(q),self.n-1)).flatten(start_dim=1))\n",
    "\n",
    "\tdef compute_D_inverse(self,r,q):\n",
    "\t\treturn torch.diag_embed(torch.div(1,torch.mul(self.n,torch.mul(r,torch.pow(torch.abs(q),self.n-1))).flatten(start_dim=1)))\n",
    "\n",
    "\tdef forward(self, data):\n",
    "\n",
    "\t\tix_res = data.cpu().x[:,TYPE_INDEX].numpy()>0 #obtain node indices for the reservoirs\n",
    "\t\tx = data.x[~ix_res] #get nodal information at junctions\n",
    "\n",
    "\t\tA12 = self.A12.repeat(data.num_graphs,1,1)\n",
    "\t\tA21 = torch.transpose(A12,1,2)\n",
    "\t\tA10 = self.A10.repeat(data.num_graphs,1,1)\n",
    "\n",
    "\t\ts, h0, l,d,c= torch.unsqueeze(x[:,1],dim=1).view(-1,self.num_heads,1).to(self.device), \\\n",
    "\t\t\t\t\t   torch.unsqueeze(torch.unsqueeze(data.x[data.cpu().x[:,TYPE_INDEX].numpy()>0,HEAD_INDEX],dim=1),dim=2).to(self.device), \\\n",
    "\t\t\t\t\t   data.edge_attr[:,LENGTH_INDEX].view(-1,self.num_flows,1).to(self.device), \\\n",
    "\t\t\t\t\t   data.edge_attr[:,DIAMETER_INDEX].view(-1,self.num_flows,1).to(self.device), \\\n",
    "\t\t\t\t\t   data.edge_attr[:,ROUGHNESS_INDEX].view(-1,self.num_flows,1).to(self.device)\n",
    "\n",
    "\t\tq =  torch.mul(math.pi/4, torch.pow(d,2))\n",
    "\t\tA10h0 = torch.bmm(A10,h0.double())\n",
    "\t\tr = torch.div(torch.mul(10.67,l),torch.mul(torch.pow(c,self.n),torch.pow(d,4.8704)))\n",
    "\t\tA11_0 = self.compute_A11(r,q).double()\n",
    "\t\tA_0 = torch.bmm(A21,torch.bmm(torch.linalg.inv(A11_0),A12))\n",
    "\t\th_0 = torch.bmm(torch.linalg.inv(A_0),-s - torch.bmm(A21,torch.bmm(torch.linalg.inv(A11_0),A10h0)))\n",
    "\t\tq = -torch.bmm(torch.linalg.inv(A11_0),torch.bmm(A12,h_0) + A10h0)\n",
    "\n",
    "\t\tfor i in range(self.num_blocks):\n",
    "\t\t\tD_inv = self.compute_D_inverse(r,q).double()\n",
    "\t\t\tA11 = self.compute_A11(r,q).double()\n",
    "\t\t\tF = torch.bmm(A21,q) - s - torch.bmm(torch.bmm(torch.bmm(A21,D_inv),A11),q) - torch.bmm(torch.bmm(A21,D_inv),A10h0)\n",
    "\t\t\tA = torch.bmm(torch.bmm(A21,D_inv),A12)\n",
    "\t\t\th = torch.bmm(torch.linalg.inv(A),F)\n",
    "\t\t\thid_q = torch.bmm(D_inv, torch.bmm(A11,q) + torch.bmm(A12,h) + A10h0)\n",
    "\t\t\tq = q-hid_q\n",
    "\n",
    "\t\treturn h.view(-1,1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "class NeumannEPANET(nn.Module):\n",
    "\tdef __init__(self, A12, A10, num_blocks, K):\n",
    "\n",
    "\t\tsuper(NeumannEPANET, self).__init__()\n",
    "\t\ttorch.manual_seed(42)\n",
    "\t\tself.num_blocks = num_blocks\n",
    "\t\tself.n = 1.852\n",
    "\t\tself.K = K\n",
    "\n",
    "\t\tself.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\t\tself.A12 = torch.from_numpy(A12).to(self.device)\n",
    "\t\tself.num_heads = self.A12.shape[1]\n",
    "\t\tself.num_flows = self.A12.shape[0]\n",
    "\t\tself.A10 = torch.from_numpy(A10).to(self.device)\n",
    "\n",
    "\t\tself.layers = nn.ModuleList()\n",
    "\n",
    "\t\tfor i in range(K):\n",
    "\t\t\tself.layers.append(Linear(self.num_heads,self.num_heads))\n",
    "\n",
    "\tdef compute_A11(self,r,q):\n",
    "\t\treturn torch.diag_embed(torch.mul(r,torch.pow(torch.abs(q),self.n-1)).flatten(start_dim=1))\n",
    "\n",
    "\tdef compute_D_inverse(self,r,q):\n",
    "\t\treturn torch.diag_embed(torch.div(1,torch.mul(self.n,torch.mul(r,torch.pow(torch.abs(q),self.n-1))).flatten(start_dim=1)))\n",
    "\n",
    "\tdef forward(self, data):\n",
    "\n",
    "\t\tix_res = data.cpu().x[:,TYPE_INDEX].numpy()>0 #obtain node indices for the reservoirs\n",
    "\t\tx = data.x[~ix_res] #get nodal information at junctions\n",
    "\n",
    "\t\tA12 = self.A12.repeat(data.num_graphs,1,1)\n",
    "\t\tA21 = torch.transpose(A12,1,2)\n",
    "\t\tA10 = self.A10.repeat(data.num_graphs,1,1)\n",
    "\n",
    "\t\ts, h0, l,d,c= torch.unsqueeze(x[:,1],dim=1).view(-1,self.num_heads,1).to(self.device), \\\n",
    "\t\t\t\t\t   torch.unsqueeze(torch.unsqueeze(data.x[data.cpu().x[:,TYPE_INDEX].numpy()>0,HEAD_INDEX],dim=1),dim=2).to(self.device), \\\n",
    "\t\t\t\t\t   data.edge_attr[:,LENGTH_INDEX].view(-1,self.num_flows,1).to(self.device), \\\n",
    "\t\t\t\t\t   data.edge_attr[:,DIAMETER_INDEX].view(-1,self.num_flows,1).to(self.device), \\\n",
    "\t\t\t\t\t   data.edge_attr[:,ROUGHNESS_INDEX].view(-1,self.num_flows,1).to(self.device)\n",
    "\n",
    "\t\tq =  torch.mul(math.pi/4, torch.pow(d,2))\n",
    "\t\tA10h0 = torch.bmm(A10,h0.double())\n",
    "\t\tr = torch.div(torch.mul(10.67,l),torch.mul(torch.pow(c,self.n),torch.pow(d,4.8704)))\n",
    "\t\tA11_0 = self.compute_A11(r,q).double()\n",
    "\t\tA_0 = torch.bmm(A21,torch.bmm(torch.linalg.inv(A11_0),A12))\n",
    "\t\th_0 = torch.bmm(torch.linalg.inv(A_0),-s - torch.bmm(A21,torch.bmm(torch.linalg.inv(A11_0),A10h0)))\n",
    "\t\tq = -torch.bmm(torch.linalg.inv(A11_0),torch.bmm(A12,h_0) + A10h0)\n",
    "\n",
    "\t\tfor i in range(self.num_blocks):\n",
    "\t\t\tD_inv = self.compute_D_inverse(r,q).double()\n",
    "\t\t\tA11 = self.compute_A11(r,q).double()\n",
    "\t\t\tF = torch.bmm(A21,q) - s - torch.bmm(torch.bmm(torch.bmm(A21,D_inv),A11),q) - torch.bmm(torch.bmm(A21,D_inv),A10h0)\n",
    "\t\t\tA = torch.bmm(torch.bmm(A21,D_inv),A12)\n",
    "\t\t\tnorm_A = torch.max(torch.abs(torch.linalg.eigvals(torch.bmm(A,torch.transpose(A,dim0=1,dim1=2)))))\n",
    "\t\t\tA_normalized = torch.div(A,norm_A.repeat(1,self.num_heads,self.num_heads))\n",
    "\t\t\tA_inv = 0\n",
    "\t\t\teta = torch.div(1,norm_A)-1e-6\n",
    "\t\t\tId = torch.eye(self.num_heads, dtype = torch.float64,device=self.device).reshape((1, self.num_heads, self.num_heads)).repeat(data.num_graphs, 1, 1)\n",
    "\t\t\tfor i in range(100):\n",
    "\t\t\t\tA_inv += torch.linalg.matrix_power(Id - A_normalized,i)\n",
    "\t\t\th = torch.bmm(A_inv,torch.mul(torch.div(1,norm_A),torch.mul(eta,F)))\n",
    "\t\t\thid_q = torch.bmm(D_inv, torch.bmm(A11,q) + torch.bmm(A12,h) + A10h0)\n",
    "\t\t\tq = q-hid_q\n",
    "\n",
    "\t\treturn h.view(-1,1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "\tdef __init__(self, hid_channels, indices, num_layers=6):\n",
    "\t\tsuper(MLP, self).__init__()\n",
    "\t\ttorch.manual_seed(42)\n",
    "\t\tself.hid_channels = hid_channels\n",
    "\t\tself.indices = indices\n",
    "\t\tself.num_heads = indices[0]\n",
    "\n",
    "\t\tlayers = [nn.Linear(indices[4], hid_channels),\n",
    "\t\t\t\t  nn.ReLU()]\n",
    "\n",
    "\t\tfor l in range(num_layers-1):\n",
    "\t\t\tlayers += [nn.Linear(hid_channels, hid_channels),\n",
    "\t\t\t\t\t   nn.ReLU()]\n",
    "\n",
    "\t\tlayers += [nn.Linear(hid_channels, self.num_heads)]\n",
    "\n",
    "\t\tself.main = nn.Sequential(*layers)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\n",
    "\t\tx = self.main(x)\n",
    "\n",
    "\t\treturn x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "class Unrolling(nn.Module):\n",
    "\tdef __init__(self, indices, num_blocks):\n",
    "\t\tsuper(Unrolling, self).__init__()\n",
    "\t\ttorch.manual_seed(42)\n",
    "\t\tself.indices = indices\n",
    "\t\tself.num_heads = indices[0]\n",
    "\t\tself.num_flows = indices[2]-indices[1]\n",
    "\t\tself.num_blocks = num_blocks\n",
    "\n",
    "\t\tself.hidq0_H = Linear(indices[2]-indices[1], self.num_heads)\n",
    "\t\tself.hidh0_q = Linear(indices[1]-indices[0], self.num_flows)\n",
    "\t\tself.hidh0_H = Linear(indices[1]-indices[0], self.num_heads)\n",
    "\t\tself.hids_q =  Linear(indices[0], self.num_flows)\n",
    "\t\tself.hid_S = nn.Sequential(Linear(indices[4] - indices[1], self.num_flows),\n",
    "\t\t\t\t\t\t   nn.ReLU())\n",
    "\n",
    "\t\tself.hid_HF = nn.ModuleList()\n",
    "\t\tself.hid_FH = nn.ModuleList()\n",
    "\t\tself.resq = nn.ModuleList()\n",
    "\t\tself.hidD_q = nn.ModuleList()\n",
    "\t\tself.hidD_H = nn.ModuleList()\n",
    "\n",
    "\t\tfor i in range(num_blocks):\n",
    "\t\t\tself.hid_HF.append(nn.Sequential(Linear(self.num_heads,self.num_flows), nn.ReLU()))\n",
    "\t\t\tself.hid_FH.append(nn.Sequential(Linear(self.num_flows, self.num_heads),\n",
    "\t\t\t\t\t\t   nn.ReLU()))\n",
    "\t\t\tself.resq.append(nn.Sequential(Linear(self.num_flows,self.num_heads),\n",
    "\t\t\t\t\t\t   nn.ReLU()))\n",
    "\t\t\tself.hidD_q.append(nn.Sequential(Linear(self.num_flows,self.num_flows),\n",
    "\t\t\t\t\t\t   nn.ReLU()))\n",
    "\t\t\tself.hidD_H.append(Linear(self.num_flows,self.num_heads))\n",
    "\n",
    "\t\tself.out = Linear(self.num_flows, self.num_heads)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\n",
    "\t\ts, h0, d, hid_S = x[:,:self.indices[0]], x[:,self.indices[0]:self.indices[1]], x[:,self.indices[1]:self.indices[2]] + 1e-5, x[:,self.indices[1]:]\n",
    "\t\tres_h0_q, res_s_q,  res_h0_H, res_S_q = self.hidh0_q(h0), self.hids_q(s),  self.hidh0_H(h0), self.hid_S(hid_S)\n",
    "\n",
    "\t\tq = torch.mul(math.pi/4, torch.pow(d,2))\n",
    "\t\tres_q_H = self.hidq0_H(q)\n",
    "\t\tfor i in range(self.num_blocks):\n",
    "\n",
    "\t\t\tD_q = self.hidD_q[i](torch.mul(q, res_S_q))\n",
    "\t\t\tD_H = self.hidD_H[i](D_q)\n",
    "\t\t\thid_x = torch.mul(D_q,torch.sum(torch.stack([q, res_s_q, res_h0_q]),dim=0))\n",
    "\t\t\tH = self.hid_FH[i](hid_x)\n",
    "\t\t\thid_x = self.hid_HF[i](torch.mul(torch.sum(torch.stack([H,res_h0_H,res_q_H]),dim=0), D_H))\n",
    "\t\t\tq = torch.sub(q,hid_x)\n",
    "\t\t\tres_q_H = self.resq[i](q)\n",
    "\t\t\tif torch.any(H > 100):\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\t\treturn self.out(q)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch_geometric\n",
    "import torch_geometric.nn.models\n",
    "from torch import Tensor\n",
    "from torch.nn import Linear, Tanh, Sequential, LayerNorm, ReLU, Sigmoid, Dropout, PReLU, LeakyReLU\n",
    "from torch_geometric.nn import GCNConv, ChebConv, MessagePassing\n",
    "from torch_geometric.typing import OptPairTensor, Adj, OptTensor, Size\n",
    "from torch_geometric.nn.inits import reset, glorot, uniform\n",
    "from typing import Union, Optional, Tuple, List\n",
    "\n",
    "class NNConvEmbed(MessagePassing):\n",
    "    def __init__(self, x_num, ea_num, emb_channels, aggr, dropout_rate=0):\n",
    "        super(NNConvEmbed, self).__init__(aggr=aggr)\n",
    "\n",
    "        self.x_num = x_num\n",
    "        self.ea_num = ea_num\n",
    "        self.emb_channels = emb_channels\n",
    "        self.nn = Sequential(Linear(2 * x_num + ea_num, emb_channels), Dropout(p=dropout_rate), ReLU())\n",
    "        self.aggr = aggr\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        reset(self.nn)\n",
    "\n",
    "    def forward(self, x: Union[Tensor, OptPairTensor], edge_index: Adj,\n",
    "                edge_attr: OptTensor = None, size: Size = None) -> Tensor:\n",
    "        out = self.propagate(edge_index, x=x, edge_attr=edge_attr, size=size)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def message(self, x_i, x_j, edge_attr):\n",
    "        z = torch.cat([x_i, x_j, edge_attr], dim=-1)\n",
    "        return self.nn(z)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}(aggr=\"{}\", nn={})'.format(self.__class__.__name__, self.aggr, self.nn)\n",
    "\n",
    "\n",
    "class GNN(nn.Module):\n",
    "    def __init__(self, hid_channels, edge_channels=32, dropout_rate=0, CC_K=2,\n",
    "                 emb_aggr='max', depth=2, normalize=True):\n",
    "        super(GNN, self).__init__()\n",
    "        self.hid_channels = hid_channels\n",
    "        self.dropout = dropout_rate\n",
    "        self.normalize = normalize\n",
    "\n",
    "        # embedding of node/edge features with NN\n",
    "        self.embedding = NNConvEmbed(2, 3, edge_channels, aggr=emb_aggr)\n",
    "\n",
    "        # CB convolutions (with normalization)\n",
    "        self.convs = nn.ModuleList()\n",
    "        for i in range(depth):\n",
    "            # if normalize == True:\n",
    "            # self.convs.append(LayerNorm(edge_channels, elementwise_affine=True))\n",
    "            if i == 0:\n",
    "                self.convs.append(ChebConv(edge_channels, hid_channels, CC_K, normalization='sym'))\n",
    "            else:\n",
    "                self.convs.append(ChebConv(hid_channels, hid_channels, CC_K, normalization='sym'))\n",
    "\n",
    "        # output layer (so far only a 1 layer MLP, make more?)\n",
    "        if depth == 0:\n",
    "            self.lin = Linear(edge_channels, 1)\n",
    "        else:\n",
    "            self.lin = Linear(hid_channels, 1)\n",
    "        # self.out = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, data):\n",
    "\n",
    "        # retrieve model device (for LayerNorm to work)\n",
    "        device = next(self.parameters()).device\n",
    "        # data = data.to(device)\n",
    "\n",
    "        x = data.x\n",
    "        edge_index = data.edge_index\n",
    "        edge_attr = data.edge_attr\n",
    "\n",
    "        # 1. Pre-process data (nodes and edges) with MLP\n",
    "        x = self.embedding(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "\n",
    "        # 2. Do convolutions\n",
    "        for i in range(len(self.convs)):\n",
    "            x = self.convs[i](x=x, edge_index=edge_index)\n",
    "            if self.normalize:\n",
    "                x = nn.LayerNorm(self.hid_channels, eps=1e-5, device=device)(x)\n",
    "            x = F.dropout(x, self.dropout, training=self.training)\n",
    "            x = nn.ReLU()(x)\n",
    "\n",
    "        # 3. Output\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.lin(x)\n",
    "        # x = self.out(x)\n",
    "\n",
    "        # Mask over storage nodes (which have pressure=0)\n",
    "        # x = (x[:, 0] * (1 - data.x[:, 2])).unsqueeze(-1)\n",
    "        x = x[data.x[:,2]<1,0]\n",
    "\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "class ModelBased(nn.Module):\n",
    "\tdef __init__(self, A12, A10, num_blocks, K=2):\n",
    "\n",
    "\t\tsuper(ModelBased, self).__init__()\n",
    "\t\tself.num_blocks = num_blocks\n",
    "\t\tself.n = 1.852\n",
    "\t\tself.K = K\n",
    "\n",
    "\t\tself.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\t\tself.A12 = torch.from_numpy(A12).to(self.device)\n",
    "\t\tself.num_heads = self.A12.shape[1]\n",
    "\t\tself.num_flows = self.A12.shape[0]\n",
    "\t\tself.A10 = torch.from_numpy(A10).to(self.device)\n",
    "\n",
    "\t\tself.layers = nn.ModuleList()\n",
    "\n",
    "\t\tfor i in range(K):\n",
    "\t\t\tself.layers.append(Linear(self.num_heads,self.num_heads))\n",
    "\n",
    "\t\tself.double()\n",
    "\t\tself.to(self.device)\n",
    "\n",
    "\tdef compute_A11(self,r,q):\n",
    "\t\treturn torch.diag_embed(torch.mul(r,torch.pow(torch.abs(q),self.n-1)).flatten(start_dim=1))\n",
    "\n",
    "\tdef compute_D_inverse(self,r,q):\n",
    "\t\treturn torch.diag_embed(torch.div(1,torch.mul(self.n,torch.mul(r,torch.pow(torch.abs(q),self.n-1))).flatten(start_dim=1)))\n",
    "\n",
    "\tdef forward(self, data):\n",
    "\n",
    "\t\tix_res = data.cpu().x[:,TYPE_INDEX].numpy()>0 #obtain node indices for the reservoirs\n",
    "\t\tix_nodes = data.cpu().x[:,TYPE_INDEX].numpy()==0 #obtain node indices for the reservoirs\n",
    "\t\tedge_index, edge_attr = subgraph(np.where(ix_nodes),data.edge_index,data.edge_attr, relabel_nodes=True)\n",
    "\t\tx = data.x[~ix_res] #get nodal information at junctions\n",
    "\n",
    "\t\tA12 = self.A12.repeat(data.num_graphs,1,1)\n",
    "\t\tA21 = torch.transpose(A12,1,2)\n",
    "\t\tA10 = self.A10.repeat(data.num_graphs,1,1)\n",
    "\n",
    "\t\ts, h0, l,d,c= torch.unsqueeze(x[:,1],dim=1).view(-1,self.num_heads,1).to(self.device), \\\n",
    "\t\t\t\t\t   data.x[data.cpu().x[:,TYPE_INDEX].numpy()>0,HEAD_INDEX].view(data.num_graphs,-1,1).to(self.device), \\\n",
    "\t\t\t\t\t   data.edge_attr[:,LENGTH_INDEX].view(-1,self.num_flows,1).to(self.device), \\\n",
    "\t\t\t\t\t   data.edge_attr[:,DIAMETER_INDEX].view(-1,self.num_flows,1).to(self.device) + 1e-5, \\\n",
    "\t\t\t\t\t   data.edge_attr[:,ROUGHNESS_INDEX].view(-1,self.num_flows,1).to(self.device) + 1e-5\n",
    "\n",
    "\t\tq =  torch.mul(math.pi/4, torch.pow(d,2))\n",
    "\t\tA10h0 = torch.bmm(A10,h0.double())\n",
    "\t\tr = torch.div(torch.mul(10.67,l),torch.mul(torch.pow(c,self.n),torch.pow(d,4.8704)))\n",
    "\t\tA11_0 = self.compute_A11(r,q).double()\n",
    "\t\tA_0 = torch.bmm(A21,torch.bmm(torch.linalg.inv(A11_0),A12))\n",
    "\t\th_0 = torch.bmm(torch.linalg.inv(A_0),s - torch.bmm(A21,torch.bmm(torch.linalg.inv(A11_0),A10h0)))\n",
    "\t\tq = -torch.bmm(torch.linalg.inv(A11_0),torch.bmm(A12,h_0) + A10h0)\n",
    "\n",
    "\t\tfor i in range(self.num_blocks):\n",
    "\t\t\tD_inv = self.compute_D_inverse(r,q).double()\n",
    "\t\t\tA11 = self.compute_A11(r,q).double()\n",
    "\t\t\tA = torch.bmm(torch.bmm(A21,D_inv), A12)\n",
    "\t\t\tF = torch.bmm(A21,q) - s - torch.bmm(torch.bmm(torch.bmm(A21,D_inv),A11),q) - torch.bmm(torch.bmm(A21,D_inv),A10h0)\n",
    "\t\t\t#edge_index, edge_attr = torch_geometric.utils.dense_to_sparse(f.normalize(A,dim=2))\n",
    "\t\t\tA = torch.div(A,torch.max(torch.linalg.eigvals(A).real,1)[0].view(-1,1,1))\n",
    "\t\t\th = torch.stack([self.layers[i](torch.bmm(torch.pow(A,i),f.normalize(F)).view(-1,self.num_heads)) for i in range(self.K)], dim=0).sum(dim=0).view(-1,self.num_heads,1)\n",
    "\t\t\thid_q = torch.bmm(D_inv, torch.bmm(A11,q) + torch.bmm(A12,h) + A10h0)\n",
    "\t\t\tq = q-hid_q\n",
    "\n",
    "\t\treturn h.view(-1,1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Running experiments"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Working with temp5, network 1 of 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alber\\AppData\\Local\\Temp\\ipykernel_27112\\2838048612.py:95: FutureWarning: incidence_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n",
      "  return graphs, nx.incidence_matrix(to_networkx(graphs[0]), oriented=True).toarray().transpose()\n",
      "C:\\Users\\alber\\AppData\\Local\\Temp\\ipykernel_27112\\2838048612.py:95: FutureWarning: incidence_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n",
      "  return graphs, nx.incidence_matrix(to_networkx(graphs[0]), oriented=True).toarray().transpose()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeumannEPANET: training combination 1 of 8\t\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [03:37<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [36], line 80\u001B[0m\n\u001B[0;32m     77\u001B[0m optimizer \u001B[38;5;241m=\u001B[39m optim\u001B[38;5;241m.\u001B[39mAdam(params\u001B[38;5;241m=\u001B[39mmodel\u001B[38;5;241m.\u001B[39mparameters(), \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mcfg[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124madamParams\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[0;32m     79\u001B[0m \u001B[38;5;66;03m# training\u001B[39;00m\n\u001B[1;32m---> 80\u001B[0m model, tra_losses, val_losses, elapsed_time, epochs \u001B[38;5;241m=\u001B[39m \u001B[43mtraining\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtra_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_loader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     81\u001B[0m \u001B[43m\t\t\t\t\t\t\t\t\t\t\t\t\t\t\u001B[49m\u001B[43mpatience\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreport_freq\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_epochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_epochs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     82\u001B[0m \u001B[43m\t\t\t\t\t\t\t\t\t\t\t\t\t   \u001B[49m\u001B[43malpha\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43malpha\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlr_rate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlr_epoch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m200\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     83\u001B[0m \u001B[43m\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   \u001B[49m\u001B[43mpath\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mresults_folder\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m/\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mwdn\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m/\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43malgorithm\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m/\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     84\u001B[0m plot_loss(tra_losses,val_losses,\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresults_folder\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mwdn\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00malgorithm\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/loss/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     85\u001B[0m plot_R2(model,val_loader,\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresults_folder\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mwdn\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00malgorithm\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/R2/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[1;32mC:\\Code\\main_unrolling\\training\\train.py:162\u001B[0m, in \u001B[0;36mtraining\u001B[1;34m(model, optimizer, train_loader, val_loader, n_epochs, patience, report_freq, alpha, lr_rate, lr_epoch, normalization, device, path)\u001B[0m\n\u001B[0;32m    159\u001B[0m \u001B[38;5;66;03m# torch.autograd.set_detect_anomaly(True)\u001B[39;00m\n\u001B[0;32m    160\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m tqdm(\u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1\u001B[39m, n_epochs \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m)):\n\u001B[0;32m    161\u001B[0m     \u001B[38;5;66;03m# Model training\u001B[39;00m\n\u001B[1;32m--> 162\u001B[0m     train_loss \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43malpha\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43malpha\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnormalization\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnormalization\u001B[49m\u001B[43m,\u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    164\u001B[0m     \u001B[38;5;66;03m# Model validation\u001B[39;00m\n\u001B[0;32m    165\u001B[0m     val_loss, _, _, _, _, _ \u001B[38;5;241m=\u001B[39m testing(model, val_loader, alpha\u001B[38;5;241m=\u001B[39malpha, normalization\u001B[38;5;241m=\u001B[39mnormalization)\n",
      "File \u001B[1;32mC:\\Code\\main_unrolling\\training\\train.py:99\u001B[0m, in \u001B[0;36mtrain_epoch\u001B[1;34m(model, loader, optimizer, alpha, normalization, device)\u001B[0m\n\u001B[0;32m     96\u001B[0m batch \u001B[38;5;241m=\u001B[39m batch\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m     98\u001B[0m \u001B[38;5;66;03m# Model prediction\u001B[39;00m\n\u001B[1;32m---> 99\u001B[0m preds \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    101\u001B[0m \u001B[38;5;66;03m# loss function = MSE if alpha=0\u001B[39;00m\n\u001B[0;32m    102\u001B[0m loss \u001B[38;5;241m=\u001B[39m smooth_loss(preds, batch\u001B[38;5;241m.\u001B[39my, alpha\u001B[38;5;241m=\u001B[39malpha, device\u001B[38;5;241m=\u001B[39mdevice)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Thesis\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Cell \u001B[1;32mIn [35], line 63\u001B[0m, in \u001B[0;36mNeumannEPANET.forward\u001B[1;34m(self, data)\u001B[0m\n\u001B[0;32m     61\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m100\u001B[39m):\n\u001B[0;32m     62\u001B[0m \tA_inv \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mlinalg\u001B[38;5;241m.\u001B[39mmatrix_power(Id \u001B[38;5;241m-\u001B[39m A_normalized,i)\n\u001B[1;32m---> 63\u001B[0m h \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241m.\u001B[39mbmm(A_inv,torch\u001B[38;5;241m.\u001B[39mmul(torch\u001B[38;5;241m.\u001B[39mdiv(\u001B[38;5;241m1\u001B[39m,norm_A),torch\u001B[38;5;241m.\u001B[39mmul(eta,F)))\n\u001B[0;32m     64\u001B[0m hid_q \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mbmm(D_inv, torch\u001B[38;5;241m.\u001B[39mbmm(A11,q) \u001B[38;5;241m+\u001B[39m torch\u001B[38;5;241m.\u001B[39mbmm(A12,h) \u001B[38;5;241m+\u001B[39m A10h0)\n\u001B[0;32m     65\u001B[0m q \u001B[38;5;241m=\u001B[39m q\u001B[38;5;241m-\u001B[39mhid_q\n",
      "Cell \u001B[1;32mIn [35], line 63\u001B[0m, in \u001B[0;36mNeumannEPANET.forward\u001B[1;34m(self, data)\u001B[0m\n\u001B[0;32m     61\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m100\u001B[39m):\n\u001B[0;32m     62\u001B[0m \tA_inv \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mlinalg\u001B[38;5;241m.\u001B[39mmatrix_power(Id \u001B[38;5;241m-\u001B[39m A_normalized,i)\n\u001B[1;32m---> 63\u001B[0m h \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241m.\u001B[39mbmm(A_inv,torch\u001B[38;5;241m.\u001B[39mmul(torch\u001B[38;5;241m.\u001B[39mdiv(\u001B[38;5;241m1\u001B[39m,norm_A),torch\u001B[38;5;241m.\u001B[39mmul(eta,F)))\n\u001B[0;32m     64\u001B[0m hid_q \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mbmm(D_inv, torch\u001B[38;5;241m.\u001B[39mbmm(A11,q) \u001B[38;5;241m+\u001B[39m torch\u001B[38;5;241m.\u001B[39mbmm(A12,h) \u001B[38;5;241m+\u001B[39m A10h0)\n\u001B[0;32m     65\u001B[0m q \u001B[38;5;241m=\u001B[39m q\u001B[38;5;241m-\u001B[39mhid_q\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_39_64.pyx:1179\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_39_64.SafeCallWrapper.__call__\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_39_64.pyx:620\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_39_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_39_64.pyx:929\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_39_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_39_64.pyx:920\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_39_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_39_64.pyx:317\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_39_64.PyDBFrame.do_wait_suspend\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mC:\\Program Files\\JetBrains\\PyCharm 2022.2.4\\plugins\\python\\helpers\\pydev\\pydevd.py:1160\u001B[0m, in \u001B[0;36mPyDB.do_wait_suspend\u001B[1;34m(self, thread, frame, event, arg, send_suspend_message, is_unhandled_exception)\u001B[0m\n\u001B[0;32m   1157\u001B[0m         from_this_thread\u001B[38;5;241m.\u001B[39mappend(frame_id)\n\u001B[0;32m   1159\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_threads_suspended_single_notification\u001B[38;5;241m.\u001B[39mnotify_thread_suspended(thread_id, stop_reason):\n\u001B[1;32m-> 1160\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_do_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mthread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msuspend_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrom_this_thread\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\Program Files\\JetBrains\\PyCharm 2022.2.4\\plugins\\python\\helpers\\pydev\\pydevd.py:1175\u001B[0m, in \u001B[0;36mPyDB._do_wait_suspend\u001B[1;34m(self, thread, frame, event, arg, suspend_type, from_this_thread)\u001B[0m\n\u001B[0;32m   1172\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_mpl_hook()\n\u001B[0;32m   1174\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocess_internal_commands()\n\u001B[1;32m-> 1175\u001B[0m         \u001B[43mtime\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0.01\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1177\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcancel_async_evaluation(get_current_thread_id(thread), \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mid\u001B[39m(frame)))\n\u001B[0;32m   1179\u001B[0m \u001B[38;5;66;03m# process any stepping instructions\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "for ix_wdn, wdn in enumerate(all_wdn_names):\n",
    "\tprint(f'\\nWorking with {wdn}, network {ix_wdn+1} of {len(all_wdn_names)}')\n",
    "\n",
    "\t# retrieve wntr data\n",
    "\ttra_database, val_database, tst_database = load_raw_dataset(wdn, data_folder)\n",
    "\t# reduce training data\n",
    "\t# tra_database = tra_database[:int(len(tra_database)*cfg['tra_prc'])]\n",
    "\tif cfg['tra_num'] < len(tra_database):\n",
    "\t\ttra_database = tra_database[:cfg['tra_num']]\n",
    "\n",
    "\t# remove PES anomaly\n",
    "\tif wdn == 'PES':\n",
    "\t\tif len(tra_database)>4468:\n",
    "\t\t\tdel tra_database[4468]\n",
    "\t\t\tprint('Removed PES anomaly')\n",
    "\t\t\tprint('Check',tra_database[4468].pressure.mean())\n",
    "\n",
    "\t# get GRAPH datasets    # later on we should change this and use normal scalers from scikit\n",
    "\ttra_dataset, A12_bar = create_dataset(tra_database)\n",
    "\tgn = GraphNormalizer()\n",
    "\tgn = gn.fit(tra_dataset)\n",
    "\ttra_dataset, _ = create_dataset(tra_database)\n",
    "\tval_dataset,_ = create_dataset(val_database)\n",
    "\ttst_dataset,_ = create_dataset(tst_database)\n",
    "\t# number of nodes\n",
    "\t# n_nodes=tra_dataset[0].x.shape[0]\n",
    "\tn_nodes=(1-tra_database[0].type_1H).numpy().sum() # remove reservoirs\n",
    "\t# dataloader\n",
    "\t# transform dataset for MLP\n",
    "\t# We begin with the MLP versions, when I want to add GNNs, check Riccardo's code\n",
    "\tA10,A12 = create_incidence_matrices(tra_dataset, A12_bar)\n",
    "\ttra_loader = torch_geometric.loader.DataLoader(tra_dataset, batch_size=batch_size,shuffle=True, pin_memory=True)\n",
    "\tval_loader = torch_geometric.loader.DataLoader(val_dataset, batch_size=batch_size,shuffle=False, pin_memory=True)\n",
    "\ttst_loader = torch_geometric.loader.DataLoader(tst_dataset, batch_size=batch_size,shuffle=False, pin_memory=True)\n",
    "\ttra_dataset_MLP, num_outputs, indices = create_dataset_MLP_from_graphs(tra_dataset)\n",
    "\t# loop through different algorithms\n",
    "\tnode_size, edge_size = tra_dataset[0].x.size(-1), tra_dataset[0].edge_attr.size(-1)\n",
    "    # number of nodes\n",
    "    # n_nodes=tra_dataset[0].x.shape[0]\n",
    "\tn_nodes=(1-tra_database[0].type_1H).numpy().sum() # remove reservoirs\n",
    "\tn_epochs = cfg['trainParams']['num_epochs']\n",
    "\tfor algorithm in cfg['algorithms']:\n",
    "\n",
    "\t\tif algorithm == 'MLP' or algorithm == 'Unrolling':\n",
    "\t\t\ttra_loader = torch.utils.data.DataLoader(tra_dataset_MLP,batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "\t\t\tval_loader = torch.utils.data.DataLoader(create_dataset_MLP_from_graphs(val_dataset)[0],batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "\t\t\ttst_loader = torch.utils.data.DataLoader(create_dataset_MLP_from_graphs(tst_dataset)[0],batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "\t\telse:\n",
    "\t\t\ttra_loader = torch_geometric.loader.DataLoader(tra_dataset,batch_size=batch_size,shuffle=True,pin_memory=True)\n",
    "\t\t\tval_loader = torch_geometric.loader.DataLoader(val_dataset,batch_size=batch_size,shuffle=False,pin_memory=True)\n",
    "\t\t\ttst_loader = torch_geometric.loader.DataLoader(tst_dataset,batch_size=batch_size,shuffle=False,pin_memory=True)\n",
    "\n",
    "\t\thyperParams = cfg['hyperParams'][algorithm]\n",
    "\t\tall_combinations = ParameterGrid(hyperParams)\n",
    "\n",
    "\t\t# create results dataframe\n",
    "\t\tresults_df = pd.DataFrame(list(all_combinations))\n",
    "\t\tresults_df = pd.concat([results_df,\n",
    "\t\t\t\t\t\t\t\tpd.DataFrame(index=np.arange(len(all_combinations)),\n",
    "\t\t\t\t\t\t\t\t\t\t  columns=list(res_columns))],axis=1)\n",
    "\n",
    "\t\tfor i, combination in enumerate(all_combinations):\n",
    "\t\t\tprint(f'{algorithm}: training combination {i+1} of {len(all_combinations)}\\t',end='\\r',)\n",
    "\t\t\tif algorithm == 'ModelBased' or 'NeumannEPANET':\n",
    "\t\t\t\tcombination['A12'] = A12\n",
    "\t\t\t\tcombination['A10'] = A10\n",
    "\t\t\tif algorithm == 'MLP' or algorithm == 'Unrolling':\n",
    "\t\t\t\tcombination['indices'] = indices\n",
    "\t\t\twandb.config = combination\n",
    "\n",
    "\t\t\t# model creation\n",
    "\t\t\tmodel = getattr(sys.modules[__name__], algorithm)(**combination).double().to(device)\n",
    "\t\t\t# print(model)\n",
    "\t\t\ttotal_parameters = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "\t\t\t# model optimizer\n",
    "\t\t\toptimizer = optim.Adam(params=model.parameters(), **cfg['adamParams'])\n",
    "\n",
    "\t\t\t# training\n",
    "\t\t\tmodel, tra_losses, val_losses, elapsed_time, epochs = training(model, None, tra_loader, val_loader,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tpatience=10, report_freq=0, n_epochs=n_epochs,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   alpha=alpha, lr_rate=2, lr_epoch=200,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   path = f'{results_folder}/{wdn}/{algorithm}/')\n",
    "\t\t\tplot_loss(tra_losses,val_losses,f'{results_folder}/{wdn}/{algorithm}/loss/{i}')\n",
    "\t\t\tplot_R2(model,val_loader,f'{results_folder}/{wdn}/{algorithm}/R2/{i}')\n",
    "\t\t\t# store training history and model\n",
    "\t\t\tpd.DataFrame(data = np.array([tra_losses, val_losses]).T).to_csv(\n",
    "\t\t\t\tf'{results_folder}/{wdn}/{algorithm}/hist/{i}.csv')\n",
    "\t\t\ttorch.save(model, f'{results_folder}/{wdn}/{algorithm}/models/{i}.csv')\n",
    "\n",
    "\t\t\t# compute and store predictions, compute r2 scores\n",
    "\t\t\tlosses = {}\n",
    "\t\t\tmax_losses = {}\n",
    "\t\t\tmin_losses = {}\n",
    "\t\t\tr2_scores = {}\n",
    "\t\t\tfor split, loader in zip(['training','validation','testing'],[tra_loader,val_loader,tst_loader]):\n",
    "\t\t\t\tlosses[split], max_losses[split], min_losses[split], pred, real, test_time = testing(model, loader)\n",
    "\t\t\t\tr2_scores[split] = r2_score(real, pred)\n",
    "\t\t\t\tif i == 0:\n",
    "\t\t\t\t\tpd.DataFrame(data=real.reshape(-1,n_nodes)).to_csv(\n",
    "\t\t\t\t\t\tf'{results_folder}/{wdn}/{algorithm}/pred/{split}/real.csv') # save real obs\n",
    "\t\t\t\tpd.DataFrame(data=pred.reshape(-1,n_nodes)).to_csv(\n",
    "\t\t\t\t\tf'{results_folder}/{wdn}/{algorithm}/pred/{split}/{i}.csv')\n",
    "\n",
    "\t\t\t# store results\n",
    "\t\t\tresults_df.loc[i,res_columns] = (losses['training'], losses['validation'], losses['testing'],\n",
    "\t\t\t\t\t\t\t\t\t\t\t max_losses['training'], max_losses['validation'], max_losses['testing'],\n",
    "\t\t\t\t\t\t\t\t\t\t\t min_losses['training'], min_losses['validation'], min_losses['testing'],\n",
    "\t\t\t\t\t\t\t\t\t\t\t r2_scores['training'], r2_scores['validation'], r2_scores['testing'],\n",
    "\t\t\t\t\t\t\t\t\t\t\t total_parameters, elapsed_time,test_time, epochs)\n",
    "\t\t# # save graph normalizer\n",
    "\t\t# with open(f'{results_folder}/{wdn}/{algorithm}/gn.pickle', 'wb') as handle:\n",
    "\t\t#     pickle.dump(gn, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\t\tresults_df.to_csv(f'{results_folder}/{wdn}/{algorithm}/results_{algorithm}.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from utils.Dashboard import Dashboard\n",
    "from IPython.display import display\n",
    "\n",
    "_,_,_, pred, real, time = testing(model, val_loader)\n",
    "d = Dashboard(pd.DataFrame(real.reshape(-1,n_nodes)),pd.DataFrame(pred.reshape(-1,n_nodes)),to_networkx(val_dataset[0],node_attrs=['pos']))\n",
    "f = d.display_results()\n",
    "display(f)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "real = pd.read_csv(f'./experiments/unrolling_WDN0020/PES/MLP/pred/testing/real.csv').drop(columns=['Unnamed: 0'])\n",
    "mlp_pred = pd.read_csv(f'./experiments/unrolling_WDN0020/PES/MLP/pred/testing/6.csv').drop(columns=['Unnamed: 0'])\n",
    "unrolling_pred =  pd.read_csv(f'./experiments/unrolling_WDN0020/PES/UnrollingModel/pred/testing/1.csv').drop(columns=['Unnamed: 0'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "res = real.sub(mlp_pred).pow(2).sum(axis=0)\n",
    "tot = real.sub(mlp_pred.mean(axis=0)).pow(2).sum(axis=0)\n",
    "r2_mlp = 1 - res/tot\n",
    "res = real.sub(unrolling_pred).pow(2).sum(axis=0)\n",
    "tot = real.sub(unrolling_pred.mean(axis=0)).pow(2).sum(axis=0)\n",
    "r2_unrolling = 1 - res/tot\n",
    "r2s = pd.concat([r2_mlp,r2_unrolling],axis=1).rename(columns={0:'MLP',1:'AU-MLP'})\n",
    "fig, ax = plt.subplots()\n",
    "r2s.plot.box(ax=ax)\n",
    "ax.set_title(\"$R^2$ Scores Comparison for PES\")\n",
    "ax.set_ylabel('$R^2$ Score')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
